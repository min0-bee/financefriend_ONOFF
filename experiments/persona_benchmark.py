from __future__ import annotations

import argparse
import json
import sys
import time
from pathlib import Path
from statistics import mean
from typing import Dict, List, Optional, Tuple

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì˜ ë¶€ëª¨ ë””ë ‰í„°ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
script_dir = Path(__file__).parent.parent
if str(script_dir) not in sys.path:
    sys.path.insert(0, str(script_dir))

from persona.persona import generate_structured_persona_reply
from persona.persona_optimized import (
    PERSONA_LOGGER,
    generate_structured_persona_reply_optimized,
)


def _time_call_original(prompt: str, term: Optional[str]) -> Tuple[str, float]:
    start = time.perf_counter()
    response = generate_structured_persona_reply(prompt, term=term)
    latency = time.perf_counter() - start
    return response, latency


def _time_call_optimized(prompt: str, term: Optional[str]) -> Tuple[str, Dict[str, float]]:
    formatted, metadata = generate_structured_persona_reply_optimized(prompt, term=term)
    latency = metadata.get("latency_seconds", 0.0)
    tokens = metadata.get("tokens", {}) or {}
    return formatted, {
        "latency": latency,
        "input_tokens": tokens.get("input"),
        "output_tokens": tokens.get("output"),
        "total_tokens": tokens.get("total"),
    }


def benchmark(
    prompt: str,
    term: Optional[str],
    runs: int = 10,
) -> Dict[str, Dict[str, float]]:
    original_latencies: List[float] = []
    optimized_latencies: List[float] = []
    optimized_input_tokens: List[float] = []
    optimized_output_tokens: List[float] = []
    optimized_total_tokens: List[float] = []

    print(f"\nğŸ”„ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘: {runs}íšŒ ì‹¤í–‰ (ì›ë³¸ {runs}íšŒ + ìµœì í™” {runs}íšŒ = ì´ {runs * 2}íšŒ API í˜¸ì¶œ)\n")

    for idx in range(runs):
        print(f"[{idx + 1}/{runs}] ì›ë³¸ ë²„ì „ ì‹¤í–‰ ì¤‘...", end=" ", flush=True)
        _, latency_orig = _time_call_original(prompt, term)
        original_latencies.append(latency_orig)
        print(f"ì™„ë£Œ ({latency_orig:.2f}ì´ˆ)")

        print(f"[{idx + 1}/{runs}] ìµœì í™” ë²„ì „ ì‹¤í–‰ ì¤‘...", end=" ", flush=True)
        _, meta_opt = _time_call_optimized(prompt, term)
        optimized_latencies.append(meta_opt["latency"])
        print(f"ì™„ë£Œ ({meta_opt['latency']:.2f}ì´ˆ)")

        for collector, key in [
            (optimized_input_tokens, "input_tokens"),
            (optimized_output_tokens, "output_tokens"),
            (optimized_total_tokens, "total_tokens"),
        ]:
            value = meta_opt.get(key)
            if value is not None:
                collector.append(value)

        PERSONA_LOGGER.info("benchmark_run=%s | original=%.3fs | optimized=%.3fs", idx, latency_orig, meta_opt["latency"])
        print()  # ë¹ˆ ì¤„ ì¶”ê°€

    return {
        "original": {
            "average_latency": mean(original_latencies),
            "min_latency": min(original_latencies),
            "max_latency": max(original_latencies),
        },
        "optimized": {
            "average_latency": mean(optimized_latencies),
            "min_latency": min(optimized_latencies),
            "max_latency": max(optimized_latencies),
            "average_input_tokens": mean(optimized_input_tokens) if optimized_input_tokens else 0.0,
            "average_output_tokens": mean(optimized_output_tokens) if optimized_output_tokens else 0.0,
            "average_total_tokens": mean(optimized_total_tokens) if optimized_total_tokens else 0.0,
        },
    }


def cli() -> None:
    parser = argparse.ArgumentParser(description="Compare original vs optimized persona latency.")
    parser.add_argument("--prompt", required=True, help="User question to send.")
    parser.add_argument("--term", default=None, help="Optional focus term.")
    parser.add_argument("--runs", type=int, default=10, help="Number of repeats per persona.")
    parser.add_argument("--output", default=None, help="Optional path to save JSON results.")
    args = parser.parse_args()

    results = benchmark(args.prompt, args.term, args.runs)
    
    print("\n" + "=" * 60)
    print("ğŸ“Š ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    orig = results["original"]
    opt = results["optimized"]
    print(f"\nì›ë³¸ ë²„ì „:")
    print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {orig['average_latency']:.2f}ì´ˆ")
    print(f"  ìµœì†Œ: {orig['min_latency']:.2f}ì´ˆ | ìµœëŒ€: {orig['max_latency']:.2f}ì´ˆ")
    print(f"\nìµœì í™” ë²„ì „:")
    print(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {opt['average_latency']:.2f}ì´ˆ")
    print(f"  ìµœì†Œ: {opt['min_latency']:.2f}ì´ˆ | ìµœëŒ€: {opt['max_latency']:.2f}ì´ˆ")
    if opt.get("average_input_tokens"):
        print(f"  í‰ê·  ì…ë ¥ í† í°: {opt['average_input_tokens']:.0f}")
        print(f"  í‰ê·  ì¶œë ¥ í† í°: {opt['average_output_tokens']:.0f}")
        print(f"  í‰ê·  ì´ í† í°: {opt['average_total_tokens']:.0f}")
    
    improvement = ((orig['average_latency'] - opt['average_latency']) / orig['average_latency']) * 100
    print(f"\nâš¡ ì„±ëŠ¥ ê°œì„ : {improvement:+.1f}% ({orig['average_latency']:.2f}ì´ˆ â†’ {opt['average_latency']:.2f}ì´ˆ)")
    print("=" * 60 + "\n")
    
    print("ì „ì²´ ê²°ê³¼ (JSON):")
    print(json.dumps(results, indent=2, ensure_ascii=False))

    if args.output:
        with open(args.output, "w", encoding="utf-8") as fp:
            json.dump(results, fp, indent=2, ensure_ascii=False)
        print(f"\nâœ… ê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    cli()


