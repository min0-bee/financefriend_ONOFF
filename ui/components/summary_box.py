
import streamlit as st
from core.utils import llm_chat
from core.config import DEFAULT_OPENAI_MODEL


# π“° μ¤λμ κΈμµ λ‰΄μ¤ μ”μ•½ λ°•μ¤ λ λ”λ§ ν•¨μ
def render(articles, use_openai : bool = False):
    st.markdown('<div class="summary-box">', unsafe_allow_html=True)
    st.subheader("π“ μ¤λμ κΈμµ λ‰΄μ¤ μ”μ•½")

    if use_openai and articles:
        # κΈ°μ‚¬ νƒ€μ΄ν‹€/ μ”μ•½ λ‡ κ°λ§ LLMμ— μ „λ‹¬ (ν† ν° λ³΄νΈ)
        tops = articles[:5]
        news_bullets = "\n".join([f"- {a['title']} :: {a.get('summary','')}" for a in tops])
        sys = {"role": "system", "content": "λ„λ” μ΄λ³΄μμ—κ² κΈμµ λ‰΄μ¤λ¥Ό 3~4λ¬Έμ¥μΌλ΅ μ‰½κ³  μ •ν™•ν•κ² μ”μ•½ν•λ” λ„μ°λ―Έμ•Ό."}
        usr = {"role": "user", "content": f"λ‹¤μ ν•­λ©μ„ μ¤λμ ν¬μΈνΈλ΅ 3~4λ¬Έμ¥ μ”μ•½ν•΄μ¤.\n{news_bullets}\n\nμΌλ° ν¬μκ¶μ λ” ν•μ§€ λ§κ³ , μ¤‘λ¦½μ μΌλ΅ ν•µμ‹¬λ§."}
        try:
            summary = llm_chat([sys,usr], max_tokens= 280)
        except Exception as e:
            summary = f"μ”μ•½ μƒμ„± μ¤‘ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤: {e}\n(μ„μ‹ Mock μ”μ•½μ„ ν‘μ‹ν•©λ‹λ‹¤.)"
    else:
        # Mock κ³ μ • ν…μ¤νΈ
        summary = (
            "μ¤λ κΈμµ μ‹μ¥μ€ ν•κµ­μ€ν–‰μ κΈ°μ¤€κΈλ¦¬ λ™κ²° κ²°μ •κ³Ό μ‚Όμ„±μ „μμ λ°°λ‹Ή μ¦μ•΅ λ°ν‘κ°€ μ£Όλ©λ°›μ•μµλ‹λ‹¤. "
            "μ›λ‹¬λ¬ ν™μ¨μ΄ 1,300μ›μ„ λνν•λ©° μ™Έν™μ‹μ¥μ λ³€λ™μ„±λ„ μ»¤μ§€κ³  μμµλ‹λ‹¤. "
            "μ „λ¬Έκ°€λ“¤μ€ ν–¥ν›„ ν†µν™”μ •μ±… λ°©ν–¥κ³Ό ν™μ¨ μ¶”μ΄λ¥Ό μ£Όμ‹ν•  ν•„μ”κ°€ μλ‹¤κ³  μ΅°μ–Έν•©λ‹λ‹¤."
        )


    st.write(summary)
    st.caption(f"π”§ model: {DEFAULT_OPENAI_MODEL} | use_openai={use_openai}")
    st.markdown('</div>', unsafe_allow_html=True)