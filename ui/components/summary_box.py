
import streamlit as st
from core.utils import llm_chat
from core.config import DEFAULT_OPENAI_MODEL, DEFAULT_NEWS_SUMMARY_PROMPT


# ğŸ“° ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ ë°•ìŠ¤ ë Œë”ë§ í•¨ìˆ˜
def _format_articles_for_prompt(articles):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif not isinstance(articles, (list, tuple)):
        articles = list(articles) if articles else []

    lines = []
    for item in articles:
        title = item.get("title") or "ì œëª© ì—†ìŒ"
        summary = item.get("summary") or item.get("content", "")
        date = item.get("date")
        if date:
            lines.append(f"- [{date}] {title} :: {summary}")
        else:
            lines.append(f"- {title} :: {summary}")
    return "\n".join(lines)


def _build_fallback_summary(articles):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif not isinstance(articles, (list, tuple)):
        articles = list(articles) if articles else []

    if not articles:
        return (
            "ì˜¤ëŠ˜ì˜ ì£¼ìš” ê¸ˆìœµ ë‰´ìŠ¤ëŠ” ì¤€ë¹„ëœ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ ì•ˆë‚´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. "
            "ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ê°€ëŠ¥í•´ì§€ë©´ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )

    bullets = []
    for item in articles[:3]:
        if isinstance(item, dict):
            title = item.get("title") or "ì œëª© ì—†ìŒ"
            summary = item.get("summary") or item.get("content", "ë‚´ìš© ì—†ìŒ")
            date = item.get("date")
            prefix = f"[{date}] " if date else ""
            bullets.append(f"â€¢ {prefix}{title}: {summary}")
        else:
            bullets.append(f"â€¢ {str(item)}")
    return "\n".join(bullets)


def render(articles, use_openai: bool = False):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif articles is None:
        articles = []

    st.markdown('<div class="summary-box">', unsafe_allow_html=True)
    st.subheader("ğŸ“Š ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½")

    if use_openai and articles:
        tops = articles[:5]
        articles_context = _format_articles_for_prompt(tops)
        prompt_template = st.session_state.get("news_summary_prompt", DEFAULT_NEWS_SUMMARY_PROMPT)
        user_prompt = prompt_template.format(articles=articles_context)

        sys = {
            "role": "system",
            "content": "ë„ˆëŠ” ì´ˆë³´ìì—ê²Œ ê¸ˆìœµ ì‹œì¥ ì´ìŠˆë¥¼ ì •í™•í•˜ê³  ì‰½ê²Œ ìš”ì•½í•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ ê¸°ìì•¼."
        }
        usr = {"role": "user", "content": user_prompt}
        try:
            summary = llm_chat([sys, usr], max_tokens=280, temperature=0.4)
        except Exception as e:
            summary = (
                f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\n"
                "ì•„ë˜ëŠ” ìµœê·¼ ê¸°ì‚¬ ëª©ë¡ì„ ê°„ë‹¨íˆ ë‚˜ì—´í•œ ì •ë³´ì…ë‹ˆë‹¤.\n"
                + _build_fallback_summary(tops)
            )
    else:
        summary = _build_fallback_summary(articles[:5])


    st.write(summary)
    if use_openai:
        st.caption(f"ğŸ”§ ì‚¬ìš© ëª¨ë¸: {DEFAULT_OPENAI_MODEL} (LLM ìš”ì•½ í™œì„±)")
    else:
        st.caption("â„¹ï¸ LLM ìš”ì•½ì´ ë¹„í™œì„±í™”ë˜ì–´ ê¸°ë³¸ ìš”ì•½ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
    st.markdown('</div>', unsafe_allow_html=True)