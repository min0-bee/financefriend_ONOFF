
import streamlit as st
import hashlib
import json
from core.utils import llm_chat
from core.config import DEFAULT_OPENAI_MODEL, DEFAULT_NEWS_SUMMARY_PROMPT


# ğŸ“° ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ ë°•ìŠ¤ ë Œë”ë§ í•¨ìˆ˜
def _format_articles_for_prompt(articles):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif not isinstance(articles, (list, tuple)):
        articles = list(articles) if articles else []

    lines = []
    for item in articles:
        title = item.get("title") or "ì œëª© ì—†ìŒ"
        summary = item.get("summary") or item.get("content", "")
        date = item.get("date")
        if date:
            lines.append(f"- [{date}] {title} :: {summary}")
        else:
            lines.append(f"- {title} :: {summary}")
    return "\n".join(lines)


def _build_fallback_summary(articles):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif not isinstance(articles, (list, tuple)):
        articles = list(articles) if articles else []

    if not articles:
        return (
            "ì˜¤ëŠ˜ì˜ ì£¼ìš” ê¸ˆìœµ ë‰´ìŠ¤ëŠ” ì¤€ë¹„ëœ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ ì•ˆë‚´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. "
            "ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ê°€ëŠ¥í•´ì§€ë©´ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )

    bullets = []
    for item in articles[:3]:
        if isinstance(item, dict):
            title = item.get("title") or "ì œëª© ì—†ìŒ"
            summary = item.get("summary") or item.get("content", "ë‚´ìš© ì—†ìŒ")
            date = item.get("date")
            prefix = f"[{date}] " if date else ""
            bullets.append(f"â€¢ {prefix}{title}: {summary}")
        else:
            bullets.append(f"â€¢ {str(item)}")
    return "\n".join(bullets)


def _get_articles_hash(articles):
    """ë‰´ìŠ¤ ëª©ë¡ì˜ í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸ (ìµœì í™”)"""
    if not articles:
        return ""
    
    # âœ… ì„±ëŠ¥ ê°œì„ : ìƒìœ„ 5ê°œ ê¸°ì‚¬ì˜ IDë§Œìœ¼ë¡œ í•´ì‹œ ìƒì„± (ë” ë¹ ë¦„)
    # IDê°€ ë³€ê²½ë˜ë©´ ë‰´ìŠ¤ê°€ ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
    tops = articles[:5]
    article_ids = []
    for item in tops:
        if isinstance(item, dict):
            article_id = item.get("id")
            if article_id:
                article_ids.append(str(article_id))
    
    # ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ í•´ì‹œ ê³„ì‚° (JSON ì§ë ¬í™”ë³´ë‹¤ ë¹ ë¦„)
    ids_string = ",".join(sorted(article_ids))
    return hashlib.md5(ids_string.encode('utf-8')).hexdigest()


def render(articles, use_openai: bool = False):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif articles is None:
        articles = []

    st.markdown('<div class="summary-box">', unsafe_allow_html=True)
    st.subheader("ğŸ“Š ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½")

    if use_openai and articles:
        tops = articles[:5]
        
        # âœ… ì„±ëŠ¥ ê°œì„ : ë‰´ìŠ¤ ëª©ë¡ í•´ì‹œ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸
        current_hash = _get_articles_hash(articles)
        cached_hash = st.session_state.get("news_summary_hash")
        cached_summary = st.session_state.get("news_summary_cached")
        
        # ìºì‹œê°€ ìˆê³  ë‰´ìŠ¤ê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìºì‹œëœ ìš”ì•½ ì‚¬ìš©
        if cached_summary and cached_hash == current_hash:
            summary = cached_summary
            st.caption("ğŸ’¾ ìºì‹œëœ ìš”ì•½ (ë‰´ìŠ¤ê°€ ë³€ê²½ë˜ì§€ ì•Šì•„ ì¬ìš”ì•½í•˜ì§€ ì•ŠìŒ)")
        else:
            # ë‰´ìŠ¤ê°€ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ìºì‹œê°€ ì—†ìœ¼ë©´ ì¬ìš”ì•½
            articles_context = _format_articles_for_prompt(tops)
            prompt_template = st.session_state.get("news_summary_prompt", DEFAULT_NEWS_SUMMARY_PROMPT)
            user_prompt = prompt_template.format(articles=articles_context)

            sys = {
                "role": "system",
                "content": "ë„ˆëŠ” ì´ˆë³´ìì—ê²Œ ê¸ˆìœµ ì‹œì¥ ì´ìŠˆë¥¼ ì •í™•í•˜ê³  ì‰½ê²Œ ìš”ì•½í•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ ê¸°ìì•¼."
            }
            usr = {"role": "user", "content": user_prompt}
            try:
                # âœ… ì„±ëŠ¥ ê°œì„ : OpenAI API í˜¸ì¶œ ì¤‘ ìŠ¤í”¼ë„ˆ í‘œì‹œ
                with st.spinner("ğŸ¤– AIê°€ ë‰´ìŠ¤ë¥¼ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    summary = llm_chat([sys, usr], max_tokens=280, temperature=0.4)
                
                # âœ… ì„±ëŠ¥ ê°œì„ : ìš”ì•½ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
                st.session_state["news_summary_hash"] = current_hash
                st.session_state["news_summary_cached"] = summary
            except Exception as e:
                summary = (
                    f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\n"
                    "ì•„ë˜ëŠ” ìµœê·¼ ê¸°ì‚¬ ëª©ë¡ì„ ê°„ë‹¨íˆ ë‚˜ì—´í•œ ì •ë³´ì…ë‹ˆë‹¤.\n"
                    + _build_fallback_summary(tops)
                )
    else:
        summary = _build_fallback_summary(articles[:5])


    st.write(summary)
    if use_openai:
        st.caption(f"ğŸ”§ ì‚¬ìš© ëª¨ë¸: {DEFAULT_OPENAI_MODEL} (LLM ìš”ì•½ í™œì„±)")
    else:
        st.caption("â„¹ï¸ LLM ìš”ì•½ì´ ë¹„í™œì„±í™”ë˜ì–´ ê¸°ë³¸ ìš”ì•½ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
    st.markdown('</div>', unsafe_allow_html=True)