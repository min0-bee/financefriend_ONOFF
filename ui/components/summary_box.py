
import streamlit as st
import hashlib
import json
from core.utils import llm_chat
from core.config import DEFAULT_OPENAI_MODEL, DEFAULT_NEWS_SUMMARY_PROMPT


# ğŸ“° ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½ ë°•ìŠ¤ ë Œë”ë§ í•¨ìˆ˜
def _format_articles_for_prompt(articles):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif not isinstance(articles, (list, tuple)):
        articles = list(articles) if articles else []

    lines = []
    for item in articles:
        title = item.get("title") or "ì œëª© ì—†ìŒ"
        summary = item.get("summary") or item.get("content", "")
        date = item.get("date")
        if date:
            lines.append(f"- [{date}] {title} :: {summary}")
        else:
            lines.append(f"- {title} :: {summary}")
    return "\n".join(lines)


def _build_fallback_summary(articles):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif not isinstance(articles, (list, tuple)):
        articles = list(articles) if articles else []

    if not articles:
        return (
            "ì˜¤ëŠ˜ì˜ ì£¼ìš” ê¸ˆìœµ ë‰´ìŠ¤ëŠ” ì¤€ë¹„ëœ ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ ì•ˆë‚´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. "
            "ë‰´ìŠ¤ ìˆ˜ì§‘ì´ ê°€ëŠ¥í•´ì§€ë©´ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        )

    bullets = []
    for item in articles[:3]:
        if isinstance(item, dict):
            title = item.get("title") or "ì œëª© ì—†ìŒ"
            summary = item.get("summary") or item.get("content", "ë‚´ìš© ì—†ìŒ")
            date = item.get("date")
            prefix = f"[{date}] " if date else ""
            bullets.append(f"â€¢ {prefix}{title}: {summary}")
        else:
            bullets.append(f"â€¢ {str(item)}")
    return "\n".join(bullets)


def _get_articles_hash(articles):
    """ë‰´ìŠ¤ ëª©ë¡ì˜ í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸ (ìµœì í™”)"""
    if not articles:
        return ""
    
    # âœ… ì„±ëŠ¥ ê°œì„ : ìƒìœ„ 5ê°œ ê¸°ì‚¬ì˜ IDë§Œìœ¼ë¡œ í•´ì‹œ ìƒì„± (ë” ë¹ ë¦„)
    # IDê°€ ë³€ê²½ë˜ë©´ ë‰´ìŠ¤ê°€ ë³€ê²½ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
    tops = articles[:5]
    article_ids = []
    for item in tops:
        if isinstance(item, dict):
            article_id = item.get("id")
            if article_id:
                article_ids.append(str(article_id))
    
    # ID ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ í•´ì‹œ ê³„ì‚° (JSON ì§ë ¬í™”ë³´ë‹¤ ë¹ ë¦„)
    ids_string = ",".join(sorted(article_ids))
    return hashlib.md5(ids_string.encode('utf-8')).hexdigest()


@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ (ë‰´ìŠ¤ê°€ ë°”ë€Œì§€ ì•ŠëŠ” í•œ ì¬ì‚¬ìš©)
def _generate_news_summary(articles_hash: str, articles_context: str, prompt_template: str) -> str:
    """
    ë‰´ìŠ¤ ìš”ì•½ ìƒì„± (st.cache_dataë¡œ ìºì‹±)
    - ë‰´ìŠ¤ í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±
    - ê°™ì€ ë‰´ìŠ¤ì— ëŒ€í•´ì„œëŠ” ì„¸ì…˜ ê°„ ê³µìœ 
    """
    from core.utils import llm_chat
    
    sys = {
        "role": "system",
        "content": "ë„ˆëŠ” ì´ˆë³´ìì—ê²Œ ê¸ˆìœµ ì‹œì¥ ì´ìŠˆë¥¼ ì •í™•í•˜ê³  ì‰½ê²Œ ìš”ì•½í•˜ëŠ” ê¸ˆìœµ ì „ë¬¸ ê¸°ìì•¼."
    }
    user_prompt = prompt_template.format(articles=articles_context)
    usr = {"role": "user", "content": user_prompt}
    
    try:
        summary = llm_chat([sys, usr], max_tokens=280, temperature=0.4)
        return summary
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜ (í˜¸ì¶œí•˜ëŠ” ìª½ì—ì„œ fallback ì²˜ë¦¬)
        raise e


def render(articles, use_openai: bool = False):
    if isinstance(articles, (str, bytes)):
        articles = [articles]
    elif articles is None:
        articles = []

    st.markdown('<div class="summary-box">', unsafe_allow_html=True)
    st.subheader("ğŸ“Š ì˜¤ëŠ˜ì˜ ê¸ˆìœµ ë‰´ìŠ¤ ìš”ì•½")

    if use_openai and articles:
        tops = articles[:5]
        
        # âœ… ì„±ëŠ¥ ê°œì„ : ë‰´ìŠ¤ ëª©ë¡ í•´ì‹œ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸
        current_hash = _get_articles_hash(articles)
        articles_context = _format_articles_for_prompt(tops)
        prompt_template = st.session_state.get("news_summary_prompt", DEFAULT_NEWS_SUMMARY_PROMPT)
        
        # âœ… ìµœì í™”: st.cache_dataë¡œ ì„¸ì…˜ ê°„ ê³µìœ  ìºì‹±
        # ë‰´ìŠ¤ í•´ì‹œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„± (ê°™ì€ ë‰´ìŠ¤ë©´ ì¬ì‚¬ìš©)
        # ìºì‹œ íˆíŠ¸ ì—¬ë¶€ ì¶”ì  (ì´ì „ í•´ì‹œì™€ ë¹„êµ)
        prev_hash = st.session_state.get("news_summary_hash")
        is_cache_hit = prev_hash == current_hash and prev_hash is not None
        
        try:
            # ìºì‹œ ë¯¸ìŠ¤ì¼ ë•Œë§Œ ìŠ¤í”¼ë„ˆ í‘œì‹œ (ìºì‹œ íˆíŠ¸ ì‹œì—ëŠ” ì¦‰ì‹œ ë°˜í™˜)
            if not is_cache_hit:
                with st.spinner("ğŸ¤– AIê°€ ë‰´ìŠ¤ë¥¼ ìš”ì•½í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    summary = _generate_news_summary(current_hash, articles_context, prompt_template)
                st.session_state["news_summary_hash"] = current_hash
                st.caption("âœ… ìƒˆë¡œìš´ ìš”ì•½ ìƒì„± ì™„ë£Œ")
            else:
                # ìºì‹œ íˆíŠ¸: ì¦‰ì‹œ ë°˜í™˜ (ìŠ¤í”¼ë„ˆ ì—†ìŒ)
                summary = _generate_news_summary(current_hash, articles_context, prompt_template)
                st.caption("ğŸ’¾ ìºì‹œëœ ìš”ì•½ (ë‰´ìŠ¤ê°€ ë³€ê²½ë˜ì§€ ì•Šì•„ ì¬ìš”ì•½í•˜ì§€ ì•ŠìŒ)")
        except Exception as e:
            # ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘ì´ê±°ë‚˜ ìƒì„± ì¤‘ ì—ëŸ¬ ë°œìƒ ì‹œ fallback
            summary = (
                f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}\n"
                "ì•„ë˜ëŠ” ìµœê·¼ ê¸°ì‚¬ ëª©ë¡ì„ ê°„ë‹¨íˆ ë‚˜ì—´í•œ ì •ë³´ì…ë‹ˆë‹¤.\n"
                + _build_fallback_summary(tops)
            )
    else:
        summary = _build_fallback_summary(articles[:5])


    st.write(summary)
    if use_openai:
        st.caption(f"ğŸ”§ ì‚¬ìš© ëª¨ë¸: {DEFAULT_OPENAI_MODEL} (LLM ìš”ì•½ í™œì„±)")
    else:
        st.caption("â„¹ï¸ LLM ìš”ì•½ì´ ë¹„í™œì„±í™”ë˜ì–´ ê¸°ë³¸ ìš”ì•½ì„ í‘œì‹œí•©ë‹ˆë‹¤.")
    st.markdown('</div>', unsafe_allow_html=True)