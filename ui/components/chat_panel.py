
import time, streamlit as st
from core.logger import log_event
from rag.glossary import explain_term
from core.utils import llm_chat  

def render(terms: dict[str, dict], use_openai: bool=False):
    st.markdown("### ğŸ’¬ ê¸ˆìœµ ìš©ì–´ ë„ìš°ë¯¸")
    st.markdown("---")

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ë Œë”(ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
    with st.container(height=400):
        for message in st.session_state.chat_history:
            role = message["role"]
            css = "user-message" if role == "user" else "bot-message"
            icon = "ğŸ‘¤" if role == "user" else "ğŸ¤–"
            st.markdown(
                f'<div class="chat-message {css}">{icon} {message["content"]}</div>',
                unsafe_allow_html=True
            )

    # ì…ë ¥ì°½(ê¸°ì¡´ ê·¸ëŒ€ë¡œ)
    user_input = st.chat_input("ê¶ê¸ˆí•œ ê¸ˆìœµ ìš©ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
    if user_input:
        t0 = time.time()
        log_event("chat_question", message=user_input, source="chat", surface="sidebar")
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        # 1) ì‚¬ì „ ë§¤ì¹­ ìš°ì„  (ë³€ê²½ ì—†ìŒ)
        found = next((t for t in terms.keys() if t in user_input), None)
        if found:
            explanation = explain_term(found, st.session_state.chat_history)
            log_event(
                "glossary_answer",
                term=found, source="chat", surface="sidebar",
                payload={"answer_len": len(explanation)}
            )
        else:
            # 2) LLM ë°±ì—…(ğŸ”Œ use_openai=Trueì¼ ë•Œë§Œ)
            if use_openai:
                sys = {
                    "role": "system",
                    "content": (
                        "ë„ˆëŠ” ì´ˆë³´ìë¥¼ ìœ„í•´ ê¸ˆìœµ ìš©ì–´/ì§ˆë¬¸ì„ ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì„¤ëª…í•˜ëŠ” ë„ìš°ë¯¸ì•¼. "
                        "ì •í™•í•˜ê³  ì¤‘ë¦½ì ìœ¼ë¡œ, ê³¼ì¥ ì—†ì´. ì¶œë ¥ í˜•ì‹: "
                        "1) ì •ì˜  2) í•µì‹¬ í¬ì¸íŠ¸ 2~3ê°œ  3) ì•„ì£¼ ì§§ì€ ì˜ˆì‹œ"
                    )
                }
                usr = {
                    "role": "user",
                    "content": f"ì´ ìš©ì–´/ì§ˆë¬¸ì„ ì‰½ê²Œ ì„¤ëª…í•´ì¤˜: {user_input}"
                }
                try:
                    # max_tokens/temperatureëŠ” í•„ìš”ì‹œ configë¡œ ëº„ ìˆ˜ ìˆìŒ
                    explanation = llm_chat([sys, usr], temperature=0.2, max_tokens=420)
                except Exception as e:
                    # LLM ì¥ì•  ì‹œ ê¸°ì¡´ MVP ë©”ì‹œì§€ë¡œ í´ë°±
                    explanation = (
                        f"(LLM ì—°ê²° ì˜¤ë¥˜: {e})\n"
                        "MVP ë‹¨ê³„ì—ì„œëŠ” ë“±ë¡ëœ ìš©ì–´ë§Œ ì•ˆì •ì ìœ¼ë¡œ ì§€ì›í•©ë‹ˆë‹¤. ë‹¤ìŒ ì¤‘ì—ì„œ ì„ íƒí•´ ì£¼ì„¸ìš”: "
                        + ", ".join(terms.keys())
                    )
            else:
                # ê¸°ì¡´ MVP ì•ˆë‚´ë¬¸ (ë³€ê²½ ì—†ìŒ)
                explanation = (
                    f"'{user_input}'ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš”! MVP ë‹¨ê³„ì—ì„œëŠ” ë“±ë¡ëœ ìš©ì–´("
                    + ", ".join(terms.keys())
                    + ")ë§Œ ì„¤ëª…í•©ë‹ˆë‹¤. ê¸°ì‚¬ í•˜ì´ë¼ì´íŠ¸ë¥¼ ëˆŒëŸ¬ë„ ì„¤ëª…ì´ ë– ìš” ğŸ˜Š"
                )

        # ë¡œê¹… + ì‘ë‹µ ì¶•ì  (ë³€ê²½ ì—†ìŒ)
        latency = int((time.time() - t0) * 1000)
        log_event(
            "chat_response",
            source="chat",
            surface="sidebar",
            message=explanation,          # âœ… ì±—ë´‡ ë‹µë³€ ë³¸ë¬¸
            answer_len=len(explanation),  # âœ… ì‘ë‹µ ê¸¸ì´
            latency_ms=latency            # âœ… ì‘ë‹µ ì§€ì—°(ms)
        )
        st.session_state.chat_history.append({"role": "assistant", "content": explanation})
        st.rerun()

    # ëŒ€í™” ì´ˆê¸°í™”(ë³€ê²½ ì—†ìŒ)
    if st.button("ğŸ”„ ëŒ€í™” ì´ˆê¸°í™”"):
        log_event("chat_reset", surface="sidebar")
        st.session_state.chat_history = []
        st.rerun()
