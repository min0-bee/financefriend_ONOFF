"""
ì„œë²„ ì¤‘ì‹¬ ë¡œê·¸ ë·°ì–´
ì„œë²„ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ í‘œì‹œí•©ë‹ˆë‹¤.
event_log ì¤‘ì‹¬ ëª¨ë“œì—ì„œëŠ” Supabaseì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

from core.config import API_BASE_URL, API_ENABLE, SUPABASE_ENABLE
from core.logger import _get_user_id, _get_backend_session_id, _ensure_backend_session, get_supabase_client
import streamlit as st
import pandas as pd
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import json
import importlib
import re
import os

# wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # GUI ë°±ì—”ë“œ ì—†ì´ ì‚¬ìš©
    WORDCLOUD_AVAILABLE = True
except ImportError:
    WORDCLOUD_AVAILABLE = False
    if SUPABASE_ENABLE:
        try:
            st.warning("âš ï¸ wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install wordcloud matplotlibë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        except:
            pass

px = None
try:
    if importlib.util.find_spec("plotly.express"):
        px = importlib.import_module("plotly.express")
        try:
            import plotly.graph_objects as go
            from plotly.subplots import make_subplots
        except Exception:
            go = None
            make_subplots = None
    else:
        go = None
        make_subplots = None
except Exception:
    px = None
    go = None
    make_subplots = None

# requests ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# ============================================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def _parse_payload(payload: Any) -> Dict[str, Any]:
    """payloadë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    if isinstance(payload, dict):
        return payload
    if isinstance(payload, str):
        try:
            return json.loads(payload) if payload else {}
        except:
            return {}
    return {}

def _extract_from_payload(payload: Any, key: str, default=None):
    """payloadì—ì„œ íŠ¹ì • í‚¤ ê°’ì„ ì¶”ì¶œ"""
    parsed = _parse_payload(payload)
    return parsed.get(key, default)

def _extract_perf_data(row: pd.Series) -> Optional[Dict[str, Any]]:
    """payloadì—ì„œ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ (event_name ê¸°ë°˜)"""
    try:
        payload_raw = row.get("payload")
        event_name = row.get("event_name")
        
        if not payload_raw:
            return None
        
        payload = _parse_payload(payload_raw)
        if not payload:
            return None
        
        # event_nameì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
        if event_name == "news_detail_open":
            perf_steps = payload.get("perf_steps", {})
            if isinstance(perf_steps, dict) and "highlight_ms" in perf_steps:
                return {
                    "highlight_ms": perf_steps.get("highlight_ms"),
                    "terms_filter_ms": perf_steps.get("terms_filter_ms"),
                    "total_ms": perf_steps.get("total_ms"),
                    "terms_count": perf_steps.get("terms_count"),
                    "content_length": perf_steps.get("content_length"),
                    "cache_hit": payload.get("cache_hit", False),
                    "highlight_cache_hit": payload.get("highlight_cache_hit", False),
                    "terms_cache_hit": payload.get("terms_cache_hit", False),
                }
        
        elif event_name == "news_click":
            click_process_ms = payload.get("click_process_ms")
            if click_process_ms is not None:
                return {
                    "click_process_ms": click_process_ms,
                    "content_length": payload.get("content_length"),
                }
        
        elif event_name in ("glossary_click", "glossary_answer"):
            perf_steps = payload.get("perf_steps", {})
            if isinstance(perf_steps, dict) and "explanation_ms" in perf_steps:
                return {
                    "explanation_ms": perf_steps.get("explanation_ms"),
                    "total_ms": perf_steps.get("total_ms"),
                    "answer_length": perf_steps.get("answer_length"),
                }
        
        # RAG ì‘ë‹µ ì‹œê°„ (latency_ms ì§ì ‘ ì‚¬ìš©)
        if event_name in ("chat_response", "glossary_answer"):
            latency_ms = payload.get("latency_ms") or row.get("latency_ms")
            if latency_ms is not None:
                return {
                    "latency_ms": latency_ms,
                    "answer_length": payload.get("answer_len") or payload.get("answer_length"),
                }
        
        return None
    except:
        return None

def _get_news_id_from_row(row: pd.Series) -> Optional[str]:
    """rowì—ì„œ news_id ì¶”ì¶œ (ì—¬ëŸ¬ ì†ŒìŠ¤ í™•ì¸)"""
    news_id = row.get("news_id")
    if news_id and not pd.isna(news_id):
        return str(news_id)
    
    # payloadì—ì„œ ì¶”ì¶œ ì‹œë„
    payload = _parse_payload(row.get("payload"))
    if payload:
        news_id = payload.get("news_id") or payload.get("article_id")
        if news_id:
            return str(news_id)
    
    # ref_id í™•ì¸
    ref_id = row.get("ref_id")
    if ref_id and not pd.isna(ref_id):
        return str(ref_id)
    
    return None

def _format_news_id_display(news_id: Optional[str]) -> str:
    """news_idë¥¼ í‘œì‹œìš©ìœ¼ë¡œ í¬ë§·íŒ… (ìŒìˆ˜ë©´ ì„ì‹œ ë‰´ìŠ¤ë¡œ í‘œì‹œ)"""
    if not news_id:
        return "N/A"
    
    try:
        news_id_num = float(news_id)
        if news_id_num < 0:
            return f"ì„ì‹œ ë‰´ìŠ¤ ({news_id})"
        else:
            return str(int(news_id_num))
    except (ValueError, TypeError):
        return str(news_id)

def _get_term_from_row(row: pd.Series) -> Optional[str]:
    """rowì—ì„œ term ì¶”ì¶œ (ì—¬ëŸ¬ ì†ŒìŠ¤ í™•ì¸)"""
    term = row.get("term")
    if term and not pd.isna(term) and term != "":
        return str(term)
    
    # payloadì—ì„œ ì¶”ì¶œ
    payload = _parse_payload(row.get("payload"))
    if payload:
        term = payload.get("term")
        if term:
            return str(term)
    
    # term_from_payload ì»¬ëŸ¼ í™•ì¸
    term_from_payload = row.get("term_from_payload")
    if term_from_payload and not pd.isna(term_from_payload):
        return str(term_from_payload)
    
    return None

# ============================================================================
# ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ë“¤
# ============================================================================

def _fetch_news_from_supabase(limit: int = 1000) -> pd.DataFrame:
    """Supabaseì—ì„œ news í…Œì´ë¸” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if not SUPABASE_ENABLE:
        return pd.DataFrame()
    
    supabase = get_supabase_client()
    if not supabase:
        return pd.DataFrame()
    
    try:
        # deleted_atì´ NULLì¸ ë‰´ìŠ¤ë§Œ ê°€ì ¸ì˜¤ê¸° (ì‚­ì œë˜ì§€ ì•Šì€ ë‰´ìŠ¤)
        query = (
            supabase.table("news")
            .select("*")
            .is_("deleted_at", "null")
            .order("published_at", desc=True)
            .limit(limit)
        )
        
        response = query.execute()
        
        if response.data:
            df = pd.DataFrame(response.data)
            
            # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
            date_columns = ["published_at", "created_at", "updated_at", "deleted_at"]
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], errors="coerce")
            
            return df
        return pd.DataFrame()
    except Exception as e:
        st.warning(f"âš ï¸ Supabaseì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def _fetch_event_logs_from_supabase(user_id: Optional[str] = None, limit: int = 1000) -> pd.DataFrame:
    """Supabaseì—ì„œ event_logs ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if not SUPABASE_ENABLE:
        return pd.DataFrame()
    
    supabase = get_supabase_client()
    if not supabase:
        return pd.DataFrame()
    
    try:
        query = supabase.table("event_logs").select("*")
        
        if user_id:
            query = query.eq("user_id", user_id)
        
        query = query.order("event_time", desc=True).limit(limit)
        
        response = query.execute()
        
        if response.data:
            df = pd.DataFrame(response.data)
            if "event_time" in df.columns:
                df["event_time"] = pd.to_datetime(df["event_time"], errors="coerce")
            if "payload" in df.columns:
                def _extract_from_payload(payload, key):
                    """payloadì—ì„œ íŠ¹ì • í‚¤ ê°’ì„ ì¶”ì¶œ"""
                    parsed = _parse_payload(payload)
                    return parsed.get(key)
                
                # term ì¶”ì¶œ
                df["term_from_payload"] = df["payload"].apply(lambda p: _extract_from_payload(p, "term"))
                
                # news_id ì¶”ì¶œ
                if "news_id" not in df.columns or df["news_id"].isna().all():
                    df["news_id_from_payload"] = df["payload"].apply(
                        lambda p: _extract_from_payload(p, "news_id") or _extract_from_payload(p, "article_id")
                    )
                    if "news_id" not in df.columns:
                        df["news_id"] = df["news_id_from_payload"]
                    else:
                        df["news_id"] = df["news_id"].fillna(df["news_id_from_payload"])
                
                # latency_ms ì¶”ì¶œ
                if "latency_ms" not in df.columns or df["latency_ms"].isna().all():
                    df["latency_ms_from_payload"] = df["payload"].apply(lambda p: _extract_from_payload(p, "latency_ms"))
                    if "latency_ms" not in df.columns:
                        df["latency_ms"] = df["latency_ms_from_payload"]
                    else:
                        df["latency_ms"] = df["latency_ms"].fillna(df["latency_ms_from_payload"])
            return df
        return pd.DataFrame()
    except Exception as e:
        st.warning(f"âš ï¸ Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def _to_kst(series):
    """UTC ì‹œê°„ì„ KSTë¡œ ë³€í™˜"""
    dt = pd.to_datetime(series, errors="coerce", utc=True)
    return dt.dt.tz_convert("Asia/Seoul")

def _fill_sessions_from_time(
    df: pd.DataFrame,
    *,
    threshold_minutes: int = 30,
    time_column: str = "event_time",
    user_column: str = "user_id",
) -> pd.DataFrame:
    """event_time ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ IDë¥¼ ì¶”ì‚°í•©ë‹ˆë‹¤."""
    if df.empty or time_column not in df.columns:
        result = df.copy()
        if "session_id" in result.columns:
            result["session_id_resolved"] = result["session_id"]
        return result

    work = df.copy()
    work[time_column] = pd.to_datetime(work[time_column], errors="coerce")

    has_user_column = user_column in work.columns
    if has_user_column:
        session_users = work[user_column].fillna("anonymous").astype(str)
        session_users = session_users.where(session_users.str.len() > 0, "anonymous")
    else:
        session_users = pd.Series(["anonymous"] * len(work), index=work.index)

    threshold = pd.Timedelta(minutes=threshold_minutes)
    order = work.index.to_series(name="_session_order")
    work = work.assign(_session_user=session_users, _session_order=order)
    work = work.sort_values(["_session_user", time_column, "_session_order"])

    gaps = work.groupby("_session_user")[time_column].diff()
    new_session_flags = gaps.isna() | (gaps > threshold) | work[time_column].isna()
    session_sequence = new_session_flags.astype(int).groupby(work["_session_user"]).cumsum()

    inferred_ids = work["_session_user"].astype(str) + "-" + session_sequence.astype(str)
    work["session_id_inferred"] = inferred_ids

    resolved = work.sort_values("_session_order")["session_id_inferred"]
    result = df.copy()
    result["session_id_inferred"] = resolved

    if "session_id" in result.columns:
        session_series = result["session_id"]
        session_series = session_series.where(session_series.notna(), None)
        if session_series.dtype != object:
            session_series = session_series.astype("object")
        missing_mask = session_series.isna() | (session_series.astype(str).str.len() == 0)
        session_series = session_series.astype("object")
        session_series.loc[missing_mask] = result.loc[missing_mask, "session_id_inferred"]
        result["session_id"] = session_series
    else:
        result["session_id"] = result["session_id_inferred"]

    result["session_id_resolved"] = result["session_id"]
    return result

# ============================================================================
# ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜
# ============================================================================

def render():
    """
    ì„œë²„ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ë¡œê·¸ ë·°ì–´ ë Œë”ë§
    
    ì„¸ ê°œì˜ íƒ­ìœ¼ë¡œ êµ¬ì„±:
    1. ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (Service Health)
    2. ğŸŸ¡ ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (Content Quality)
    3. ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (User Behavior)
    """
    from core.logger import _get_user_id
    
    st.markdown("## ğŸ“Š ë¡œê·¸ ë·°ì–´")

    with st.spinner("ğŸ”„ Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        df = _fetch_event_logs_from_supabase(user_id=None, limit=2000)

        if df.empty:
            st.info("ğŸ“­ ì•„ì§ ì´ë²¤íŠ¸ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì•±ì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ê°€ ìˆ˜ì§‘ë©ë‹ˆë‹¤.")
            return

        df["event_time"] = _to_kst(df["event_time"])
        df = df.sort_values("event_time")

        # ì„¸ì…˜ ê³„ì‚° (ëª¨ë“  íƒ­ì—ì„œ ì‚¬ìš©)
        session_gap_minutes = st.session_state.get("log_viewer_session_gap_supabase", 30)
        df = _fill_sessions_from_time(df, threshold_minutes=session_gap_minutes)
        session_column = "session_id_resolved" if "session_id_resolved" in df.columns else "session_id"

        # ì„¸ ê°œì˜ íƒ­ìœ¼ë¡œ ë¶„ë¦¬
        tab1, tab2, tab3 = st.tabs([
            "ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ (Service Health)",
            "ğŸŸ¡ ì½˜í…ì¸  í’ˆì§ˆ (Content Quality)", 
            "ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ (User Behavior)"
        ])
        
        # íƒ­ 1: ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (ì „ì²´ ë°ì´í„° ì‚¬ìš©)
        with tab1:
            _render_service_health_tab(df, session_column)
        
        # íƒ­ 2: ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (ì „ì²´ ë°ì´í„° ì‚¬ìš©)
        with tab2:
            _render_content_quality_tab(df)
        
        # íƒ­ 3: ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (í•„í„° ì ìš©)
        with tab3:
            _render_user_behavior_tab(df, session_column)

# ============================================================================
# íƒ­ 1: ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (Service Health)
# ============================================================================

def _render_service_health_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° íƒ­
    â†’ "MVPê°€ ë©ˆì¶”ì§€ ì•Šê³  ë²„í‹¸ ìˆ˜ ìˆëŠ”ê°€?"
    """
    st.markdown("### ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (Service Health)")
    st.markdown("**ëª©í‘œ**: ì„œë¹„ìŠ¤ì˜ ê¸°ìˆ ì  ì•ˆì •ì„± ì¸¡ì • - ëª¨ë“  ë¶„ì„ì˜ ê¸°ë°˜")
    
    # ì£¼ìš” ì„±ëŠ¥ ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        news_clicks = int((df_view["event_name"] == "news_click").sum())
        st.metric("ë‰´ìŠ¤ í´ë¦­", news_clicks)
    with col2:
        detail_opens = int((df_view["event_name"] == "news_detail_open").sum())
        st.metric("ìƒì„¸ ì§„ì…", detail_opens)
    with col3:
        chat_questions = int((df_view["event_name"] == "chat_question").sum())
        st.metric("ì±— ì§ˆë¬¸", chat_questions)
    with col4:
        url_errors = int((df_view["event_name"] == "news_url_add_error").sum())
        st.metric("URL íŒŒì‹± ì‹¤íŒ¨", url_errors)
    
    # ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ
    perf_events = df_view[df_view["event_name"].isin([
        "news_click", "news_detail_open", "glossary_click", "glossary_answer",
        "chat_response", "news_search_from_chat"
    ])].copy()
    
    if not perf_events.empty:
        perf_events["perf_data"] = perf_events.apply(_extract_perf_data, axis=1)
        perf_events_with_data = perf_events[perf_events["perf_data"].notna()]
        
        # 1. ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ë¡œë”© ì‹œê°„
        _render_detail_performance(perf_events_with_data)
        
        # 2. RAG ì‘ë‹µ ì‹œê°„
        _render_rag_performance(perf_events_with_data)
        
        # 3. ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„
        _render_search_performance(df_view)
        
        # 4. í•˜ì´ë¼ì´íŠ¸ ê³„ì‚° ì‹œê°„ (detail ì„±ëŠ¥ì— í¬í•¨)
        
        # 5. URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨
        _render_url_parsing_quality(df_view)
        
        # 6. Streamlit ì„¸ì…˜ ìˆ˜ / ë™ì‹œ ì ‘ì† ë¶€í•˜
        _render_session_load(df_view, session_column)
    else:
        st.info("ğŸ“Š ì„±ëŠ¥ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìµœê·¼ ì´ë²¤íŠ¸ ë¡œê·¸
    st.markdown("### ğŸ“„ ìµœê·¼ ì„±ëŠ¥ ì´ë²¤íŠ¸")
    perf_recent = df_view[df_view["event_name"].isin([
        "news_click", "news_detail_open", "glossary_click", "glossary_answer",
        "chat_question", "chat_response", "news_url_add_error"
    ])].sort_values("event_time", ascending=False).head(50)
    
    display_cols = ["event_time", "event_name", "user_id", "session_id", "latency_ms"]
    available_cols = [col for col in display_cols if col in perf_recent.columns]
    if available_cols:
        st.dataframe(perf_recent[available_cols], use_container_width=True, height=300)

def _render_detail_performance(perf_events_with_data: pd.DataFrame):
    """ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ì„±ëŠ¥ ë¶„ì„"""
    detail_events = perf_events_with_data[perf_events_with_data["event_name"] == "news_detail_open"]
    
    if detail_events.empty:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ë¡œë”© ì‹œê°„")
    
    perf_data_list = []
    for idx, row in detail_events.iterrows():
        perf = row["perf_data"]
        if perf and isinstance(perf, dict):
            news_id = _get_news_id_from_row(row)
            highlight_ms = perf.get("highlight_ms", 0)
            terms_filter_ms = perf.get("terms_filter_ms", 0)
            total_ms = perf.get("total_ms") or (highlight_ms + terms_filter_ms)
            
            cache_status = []
            if perf.get("highlight_cache_hit"):
                cache_status.append("í•˜ì´ë¼ì´íŠ¸âœ…")
            if perf.get("terms_cache_hit"):
                cache_status.append("ìš©ì–´âœ…")
            if not cache_status:
                cache_status.append("âŒ")
            
            perf_data_list.append({
                "event_time": row.get("event_time"),
                "news_id": _format_news_id_display(news_id),
                "í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)": highlight_ms,
                "ìš©ì–´ í•„í„°ë§ (ms)": terms_filter_ms,
                "ì „ì²´ ë Œë”ë§ (ms)": total_ms,
                "ë°œê²¬ëœ ìš©ì–´ ìˆ˜": perf.get("terms_count", 0),
                "ê¸°ì‚¬ ê¸¸ì´ (ì)": perf.get("content_length", 0),
                "ìºì‹œ íˆíŠ¸": " / ".join(cache_status),
            })
    
    if perf_data_list:
        perf_df = pd.DataFrame(perf_data_list)
        perf_df = perf_df.sort_values("event_time", ascending=False)
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### ğŸ“Š ì„±ëŠ¥ í†µê³„")
            avg_highlight = perf_df["í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)"].mean()
            avg_filter = perf_df["ìš©ì–´ í•„í„°ë§ (ms)"].mean()
            avg_total = perf_df["ì „ì²´ ë Œë”ë§ (ms)"].mean()
            cache_hit_rate = (perf_df["ìºì‹œ íˆíŠ¸"].str.contains("âœ…", na=False).sum() / len(perf_df) * 100) if len(perf_df) > 0 else 0
            
            st.metric("í‰ê·  í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬", f"{avg_highlight:.0f}ms")
            st.metric("í‰ê·  ìš©ì–´ í•„í„°ë§", f"{avg_filter:.0f}ms")
            st.metric("í‰ê·  ì „ì²´ ë Œë”ë§", f"{avg_total:.0f}ms")
            st.metric("ìºì‹œ íˆíŠ¸ìœ¨", f"{cache_hit_rate:.1f}%")
        
        with col2:
            st.markdown("##### ğŸ” ë³‘ëª© ì§€ì  ë¶„ì„")
            if len(perf_df) > 0:
                highlight_pct = (perf_df["í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)"] / perf_df["ì „ì²´ ë Œë”ë§ (ms)"] * 100).mean()
                filter_pct = (perf_df["ìš©ì–´ í•„í„°ë§ (ms)"] / perf_df["ì „ì²´ ë Œë”ë§ (ms)"] * 100).mean()
                
                st.write(f"**í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ ë¹„ìœ¨**: {highlight_pct:.1f}%")
                st.write(f"**ìš©ì–´ í•„í„°ë§ ë¹„ìœ¨**: {filter_pct:.1f}%")
                
                if highlight_pct > 50:
                    st.warning("âš ï¸ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ê°€ ì£¼ìš” ë³‘ëª©ì…ë‹ˆë‹¤!")
                elif filter_pct > 30:
                    st.warning("âš ï¸ ìš©ì–´ í•„í„°ë§ì´ ë³‘ëª©ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    st.success("âœ… ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        
        # ì‹œê°í™”
        if px is not None and len(perf_df) > 0:
            fig = px.scatter(
                perf_df.head(100),
                x="í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)",
                y="ìš©ì–´ í•„í„°ë§ (ms)",
                size="ì „ì²´ ë Œë”ë§ (ms)",
                color="ìºì‹œ íˆíŠ¸",
                hover_data=["news_id", "ë°œê²¬ëœ ìš©ì–´ ìˆ˜", "ê¸°ì‚¬ ê¸¸ì´ (ì)"],
                title="ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ì„±ëŠ¥ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(perf_df.head(20), use_container_width=True, height=300)

def _render_rag_performance(perf_events_with_data: pd.DataFrame):
    """RAG ì‘ë‹µ ì‹œê°„ ë¶„ì„"""
    rag_events = perf_events_with_data[perf_events_with_data["event_name"].isin(["glossary_answer", "chat_response"])]
    
    if rag_events.empty:
        return
    
    st.markdown("#### ğŸ¤– RAG ì‘ë‹µ ì‹œê°„")
    
    rag_data_list = []
    for idx, row in rag_events.iterrows():
        perf = row["perf_data"]
        if perf and isinstance(perf, dict):
            term = _get_term_from_row(row)
            latency_ms = perf.get("latency_ms") or perf.get("total_ms") or perf.get("explanation_ms")
            answer_length = perf.get("answer_length") or perf.get("answer_len", 0)
            
            if latency_ms is not None:
                rag_data_list.append({
                    "event_time": row.get("event_time"),
                    "event_name": row.get("event_name"),
                    "term": term or "",
                    "ì‘ë‹µ ì‹œê°„ (ms)": latency_ms,
                    "ë‹µë³€ ê¸¸ì´ (ì)": answer_length,
                })
    
    if rag_data_list:
        rag_df = pd.DataFrame(rag_data_list)
        rag_df = rag_df.sort_values("event_time", ascending=False)
        
        col1, col2 = st.columns(2)
        with col1:
            avg_latency = rag_df["ì‘ë‹µ ì‹œê°„ (ms)"].mean()
            st.metric("í‰ê·  RAG ì‘ë‹µ ì‹œê°„", f"{avg_latency:.0f}ms")
        with col2:
            avg_length = rag_df["ë‹µë³€ ê¸¸ì´ (ì)"].mean()
            st.metric("í‰ê·  ë‹µë³€ ê¸¸ì´", f"{avg_length:.0f}ì")
        
        # ì‹œê°í™”
        if px is not None and len(rag_df) > 0:
            fig = px.histogram(
                rag_df,
                x="ì‘ë‹µ ì‹œê°„ (ms)",
                nbins=30,
                title="RAG ì‘ë‹µ ì‹œê°„ ë¶„í¬",
                labels={"ì‘ë‹µ ì‹œê°„ (ms)": "ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)", "count": "ë¹ˆë„"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(rag_df.head(20), use_container_width=True, height=300)

def _render_search_performance(df_view: pd.DataFrame):
    """ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„"""
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"]
    
    if search_events.empty:
        return
    
    st.markdown("#### ğŸ” ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„")
    
    search_data_list = []
    for idx, row in search_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        latency_ms = payload.get("latency_ms") or row.get("latency_ms")
        message = payload.get("message") or row.get("message", "")
        
        # latency_msë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê³  ìœ íš¨í•œ ê°’ë§Œ ì‚¬ìš©
        try:
            if latency_ms is not None:
                latency_ms = pd.to_numeric(latency_ms, errors='coerce')
                if pd.notna(latency_ms) and latency_ms > 0:
                    search_data_list.append({
                        "event_time": row.get("event_time"),
                        "ê²€ìƒ‰ì–´": message[:50] if message else "",
                        "ì²˜ë¦¬ ì‹œê°„ (ms)": float(latency_ms),
                    })
        except (ValueError, TypeError):
            continue
    
    if search_data_list:
        search_df = pd.DataFrame(search_data_list)
        search_df = search_df.sort_values("event_time", ascending=False)
        
        # ìœ íš¨í•œ ê°’ë§Œìœ¼ë¡œ í‰ê·  ê³„ì‚°
        valid_latencies = search_df["ì²˜ë¦¬ ì‹œê°„ (ms)"].dropna()
        if len(valid_latencies) > 0:
            avg_latency = valid_latencies.mean()
            st.metric("í‰ê·  ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„", f"{avg_latency:.0f}ms")
        else:
            st.metric("í‰ê·  ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„", "N/A")
        
        if px is not None and len(search_df) > 0 and len(valid_latencies) > 0:
            fig = px.box(
                search_df,
                y="ì²˜ë¦¬ ì‹œê°„ (ms)",
                title="ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(search_df.head(20), use_container_width=True, height=200)
    else:
        st.info("ğŸ“Š ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _render_url_parsing_quality(df_view: pd.DataFrame):
    """URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨ ì´ë²¤íŠ¸"""
    url_events = df_view[df_view["event_name"].isin(["news_url_added_from_chat", "news_url_add_error"])]
    
    if url_events.empty:
        return
    
    st.markdown("#### ğŸ”— URL íŒŒì‹± í’ˆì§ˆ")
    
    success_count = int((url_events["event_name"] == "news_url_added_from_chat").sum())
    error_count = int((url_events["event_name"] == "news_url_add_error").sum())
    total_count = success_count + error_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("URL íŒŒì‹± ì„±ê³µ", success_count)
    with col2:
        st.metric("URL íŒŒì‹± ì‹¤íŒ¨", error_count)
    with col3:
        if total_count > 0:
            success_rate = (success_count / total_count) * 100
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        else:
            st.metric("ì„±ê³µë¥ ", "N/A")
    
    if total_count > 0 and px is not None:
        fig = px.pie(
            values=[success_count, error_count],
            names=["ì„±ê³µ", "ì‹¤íŒ¨"],
            title="URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_session_load(df_view: pd.DataFrame, session_column: str):
    """Streamlit ì„¸ì…˜ ìˆ˜ / ë™ì‹œ ì ‘ì† ë¶€í•˜"""
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### ğŸ‘¥ ì„¸ì…˜ ë¶€í•˜ ë¶„ì„")
    
    # ì‹œê°„ëŒ€ë³„ ì„¸ì…˜ ìˆ˜
    df_view_copy = df_view.copy()
    df_view_copy["hour"] = df_view_copy["event_time"].dt.floor("H")
    hourly_sessions = df_view_copy.groupby("hour")[session_column].nunique().reset_index()
    hourly_sessions.columns = ["ì‹œê°„", "ì„¸ì…˜ ìˆ˜"]
    
    col1, col2 = st.columns(2)
    with col1:
        max_sessions = hourly_sessions["ì„¸ì…˜ ìˆ˜"].max() if len(hourly_sessions) > 0 else 0
        st.metric("ìµœëŒ€ ë™ì‹œ ì„¸ì…˜ ìˆ˜", f"{max_sessions}ê°œ")
    with col2:
        avg_sessions = hourly_sessions["ì„¸ì…˜ ìˆ˜"].mean() if len(hourly_sessions) > 0 else 0
        st.metric("í‰ê·  ì„¸ì…˜ ìˆ˜", f"{avg_sessions:.1f}ê°œ")
    
    if px is not None and len(hourly_sessions) > 0:
        fig = px.line(
            hourly_sessions,
            x="ì‹œê°„",
            y="ì„¸ì…˜ ìˆ˜",
            title="ì‹œê°„ëŒ€ë³„ ì„¸ì…˜ ìˆ˜ ì¶”ì´"
        )
        st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# íƒ­ 2: ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (Content Quality)
# ============================================================================

def _render_content_quality_tab(df_view: pd.DataFrame):
    """
    ğŸŸ¡ ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° íƒ­
    â†’ "ìš°ë¦¬ ì œí’ˆì´ ì œê³µí•˜ëŠ” ë‰´ìŠ¤ ë°ì´í„° ìì²´ê°€ ì¢‹ì€ê°€?"
    """
    st.markdown("### ğŸŸ¡ ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (Content Quality)")
    st.markdown("**ëª©í‘œ**: ë‰´ìŠ¤ ì½˜í…ì¸ ì˜ í’ˆì§ˆ ì¸¡ì • - ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ìì‚°")
    
    # ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„ (DB ë‰´ìŠ¤ vs ì„ì‹œ ë‰´ìŠ¤) - ì´ë²¤íŠ¸ ë¡œê·¸ ê¸°ë°˜
    _render_news_source_analysis(df_view)
    
    # URL íŒŒì‹± í’ˆì§ˆ (ì´ë²¤íŠ¸ ë¡œê·¸ ê¸°ë°˜)
    _render_url_parsing_quality_for_content(df_view)
    
    # Supabase news í…Œì´ë¸” ì—°ë™ ë¶„ì„
    with st.spinner("ğŸ”„ Supabaseì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        news_df = _fetch_news_from_supabase(limit=5000)
        
        if news_df.empty:
            st.warning("âš ï¸ Supabase `news` í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ Supabase ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success(f"âœ… {len(news_df):,}ê°œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            
            # ì£¼ìš” ë¶„ì„ í•­ëª©ë“¤
            _render_news_source_distribution(news_df)
            _render_financial_news_ratio(news_df)
            _render_content_length_analysis(news_df)
            _render_content_missing_analysis(news_df)
            _render_title_content_duplication(news_df)
            _render_impact_score_distribution(news_df)
            _render_duplicate_news_analysis(news_df)
            _render_news_collection_trends(news_df)
            
            # ì›Œë“œí´ë¼ìš°ë“œ (ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥)
            st.markdown("---")
            _render_wordcloud(news_df)
            
            # ê¸°ì´ˆ ë‰´ìŠ¤ ì§€í‘œ ë¶„ì„ + ë¼ì´ë‹¤ ì°¨íŠ¸
            st.markdown("---")
            _render_news_radar_analysis(news_df)
            
            # í”„ë¡¬í”„íŠ¸ íŠœë‹ìš© ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë¹„êµ
            st.markdown("---")
            _render_prompt_tuning_comparison(news_df)

def _render_news_source_analysis(df_view: pd.DataFrame):
    """ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„ (DB ë‰´ìŠ¤ vs ì„ì‹œ ë‰´ìŠ¤)"""
    # news_idê°€ ìˆëŠ” ì´ë²¤íŠ¸ í•„í„°ë§
    news_events = df_view[df_view["news_id"].notna()].copy()
    
    if news_events.empty:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„")
    
    # news_idë¥¼ ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
    def is_temp_news(news_id):
        """news_idê°€ ì„ì‹œ ë‰´ìŠ¤(ìŒìˆ˜)ì¸ì§€ í™•ì¸"""
        try:
            news_id_num = float(news_id)
            return news_id_num < 0
        except (ValueError, TypeError):
            return False
    
    news_events["is_temp"] = news_events["news_id"].apply(is_temp_news)
    
    db_news_count = (~news_events["is_temp"]).sum()
    temp_news_count = news_events["is_temp"].sum()
    total_count = len(news_events)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("DB ë‰´ìŠ¤", f"{db_news_count:,}ê±´")
    with col2:
        st.metric("ì„ì‹œ ë‰´ìŠ¤ (URL ì§ì ‘ ì…ë ¥)", f"{temp_news_count:,}ê±´")
    with col3:
        if total_count > 0:
            temp_ratio = (temp_news_count / total_count) * 100
            st.metric("ì„ì‹œ ë‰´ìŠ¤ ë¹„ìœ¨", f"{temp_ratio:.1f}%")
        else:
            st.metric("ì„ì‹œ ë‰´ìŠ¤ ë¹„ìœ¨", "N/A")
    
    if total_count > 0 and px is not None:
        source_df = pd.DataFrame({
            "ì†ŒìŠ¤": ["DB ë‰´ìŠ¤", "ì„ì‹œ ë‰´ìŠ¤"],
            "ê±´ìˆ˜": [db_news_count, temp_news_count]
        })
        fig = px.pie(
            source_df,
            values="ê±´ìˆ˜",
            names="ì†ŒìŠ¤",
            title="ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„í¬"
        )
        st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# Supabase news í…Œì´ë¸” ê¸°ë°˜ ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜ë“¤
# ============================================================================

def _render_news_source_distribution(news_df: pd.DataFrame):
    """ë‰´ìŠ¤ ì¶œì²˜(ì–¸ë¡ ì‚¬) ë¶„í¬"""
    if "source" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ì¶œì²˜(ì–¸ë¡ ì‚¬) ë¶„í¬")
    
    # sourceê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_source = news_df[news_df["source"].notna() & (news_df["source"] != "")]
    
    if news_with_source.empty:
        st.info("ğŸ“Š ì¶œì²˜ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    source_counts = news_with_source["source"].value_counts()
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ê³ ìœ  ì¶œì²˜ ìˆ˜", len(source_counts))
    with col2:
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", len(news_with_source))
    
    # Top 10 ì¶œì²˜
    top_sources = source_counts.head(10).reset_index()
    top_sources.columns = ["ì¶œì²˜", "ê±´ìˆ˜"]
    
    if px is not None and len(top_sources) > 0:
        fig = px.bar(
            top_sources,
            x="ì¶œì²˜",
            y="ê±´ìˆ˜",
            title="ë‰´ìŠ¤ ì¶œì²˜ Top 10",
            labels={"ì¶œì²˜": "ì–¸ë¡ ì‚¬", "ê±´ìˆ˜": "ê¸°ì‚¬ ìˆ˜"}
        )
        fig.update_xaxes(tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
    
    st.dataframe(top_sources, use_container_width=True, height=300)

def _render_financial_news_ratio(news_df: pd.DataFrame):
    """ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘"""
    # is_finance_news ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if "is_finance_news" in news_df.columns:
        news_with_flag = news_df[news_df["is_finance_news"].notna()].copy()
        
        if not news_with_flag.empty:
            financial_count = (news_with_flag["is_finance_news"] == True).sum()
            non_financial_count = (news_with_flag["is_finance_news"] == False).sum()
            total_count = len(news_with_flag)
            
            st.markdown("#### ğŸ’° ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘ (is_finance_news ì»¬ëŸ¼ ê¸°ì¤€)")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ê¸ˆìœµ ê¸°ì‚¬", f"{financial_count:,}ê±´")
                if total_count > 0:
                    financial_ratio = (financial_count / total_count) * 100
                    st.caption(f"ë¹„ìœ¨: {financial_ratio:.1f}%")
            with col2:
                st.metric("ë¹„ê¸ˆìœµ ê¸°ì‚¬", f"{non_financial_count:,}ê±´")
                if total_count > 0:
                    non_financial_ratio = (non_financial_count / total_count) * 100
                    st.caption(f"ë¹„ìœ¨: {non_financial_ratio:.1f}%")
            with col3:
                st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_count:,}ê±´")
            
            if total_count > 0 and px is not None:
                ratio_df = pd.DataFrame({
                    "ë¶„ë¥˜": ["ê¸ˆìœµ ê¸°ì‚¬", "ë¹„ê¸ˆìœµ ê¸°ì‚¬"],
                    "ê±´ìˆ˜": [financial_count, non_financial_count]
                })
                # Donut chart (hole íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                fig = px.pie(
                    ratio_df,
                    values="ê±´ìˆ˜",
                    names="ë¶„ë¥˜",
                    title="ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘",
                    hole=0.4  # ë„ë„› ì°¨íŠ¸ë¡œ ë§Œë“¤ê¸°
                )
                st.plotly_chart(fig, use_container_width=True)
            return
    
    # is_finance_news ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
    if "title" not in news_df.columns and "content" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ’° ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘ (í‚¤ì›Œë“œ ê¸°ë°˜)")
    
    # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ (ì œëª©/ë³¸ë¬¸ì—ì„œ ê²€ìƒ‰)
    financial_keywords = [
        "ê¸ˆìœµ", "ì€í–‰", "ì¦ê¶Œ", "ì£¼ì‹", "ì±„ê¶Œ", "ë¶€ë™ì‚°", "ê²½ì œ", "ì‹œì¥", "íˆ¬ì", "ìì‚°",
        "ì´ì", "ê¸ˆë¦¬", "í™˜ìœ¨", "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "ê²½ê¸°", "ê²½ì œì„±ì¥", "GDP",
        "ê¸°ì—…", "ìƒì¥", "IPO", "ë°°ë‹¹", "ìˆ˜ìµ", "ì†ì‹¤", "ì¬ë¬´", "íšŒê³„", "ì„¸ê¸ˆ", "ì •ì±…",
        "í•œêµ­ì€í–‰", "ê¸ˆìœµê°ë…ì›", "ê¸ˆìœµìœ„ì›íšŒ", "ì¦ì„ ìœ„", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ë‚˜ìŠ¤ë‹¥",
        "ë¹„íŠ¸ì½”ì¸", "ì•”í˜¸í™”í", "ë¸”ë¡ì²´ì¸", "ê¸ˆìœµê¶Œ", "ê¸ˆìœµì‚¬", "ì€í–‰ê¶Œ"
    ]
    
    def is_financial(row):
        """ë‰´ìŠ¤ê°€ ê¸ˆìœµ ê´€ë ¨ì¸ì§€ íŒë‹¨"""
        title = str(row.get("title", "")).lower()
        content = str(row.get("content", "")).lower()
        
        text = title + " " + content[:500]  # ë³¸ë¬¸ ì•ë¶€ë¶„ 500ìë§Œ í™•ì¸
        
        for keyword in financial_keywords:
            if keyword in text:
                return True
        return False
    
    news_df_copy = news_df.copy()
    news_df_copy["is_financial"] = news_df_copy.apply(is_financial, axis=1)
    
    financial_count = news_df_copy["is_financial"].sum()
    non_financial_count = (~news_df_copy["is_financial"]).sum()
    total_count = len(news_df_copy)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê¸ˆìœµ ê¸°ì‚¬", f"{financial_count:,}ê±´")
        if total_count > 0:
            financial_ratio = (financial_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {financial_ratio:.1f}%")
    with col2:
        st.metric("ë¹„ê¸ˆìœµ ê¸°ì‚¬", f"{non_financial_count:,}ê±´")
        if total_count > 0:
            non_financial_ratio = (non_financial_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {non_financial_ratio:.1f}%")
    with col3:
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_count:,}ê±´")
    
    if total_count > 0 and px is not None:
        ratio_df = pd.DataFrame({
            "ë¶„ë¥˜": ["ê¸ˆìœµ ê¸°ì‚¬", "ë¹„ê¸ˆìœµ ê¸°ì‚¬"],
            "ê±´ìˆ˜": [financial_count, non_financial_count]
        })
        # Donut chart (hole íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        fig = px.pie(
            ratio_df,
            values="ê±´ìˆ˜",
            names="ë¶„ë¥˜",
            title="ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘",
            hole=0.4  # ë„ë„› ì°¨íŠ¸ë¡œ ë§Œë“¤ê¸°
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_content_length_analysis(news_df: pd.DataFrame):
    """ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬"""
    # content ë˜ëŠ” raw_content_length ì»¬ëŸ¼ í™•ì¸
    content_col = None
    if "raw_content_length" in news_df.columns:
        content_col = "raw_content_length"
    elif "content" in news_df.columns:
        content_col = "content"
    else:
        return
    
    st.markdown("#### ğŸ“ ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬")
    
    if content_col == "raw_content_length":
        # raw_content_lengthê°€ ìˆ«ì ì»¬ëŸ¼ì¸ ê²½ìš°
        news_with_content = news_df[news_df[content_col].notna()].copy()
        news_with_content["content_length"] = pd.to_numeric(news_with_content[content_col], errors='coerce')
    else:
        # content ì»¬ëŸ¼ì—ì„œ ê¸¸ì´ ê³„ì‚°
        news_with_content = news_df[news_df[content_col].notna() & (news_df[content_col] != "")].copy()
        news_with_content["content_length"] = news_with_content[content_col].astype(str).str.len()
    
    news_with_content = news_with_content[news_with_content["content_length"].notna() & (news_with_content["content_length"] > 0)]
    
    if news_with_content.empty:
        st.info("ğŸ“Š ë³¸ë¬¸ ê¸¸ì´ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        avg_length = news_with_content["content_length"].mean()
        st.metric("í‰ê·  ë³¸ë¬¸ ê¸¸ì´", f"{avg_length:.0f}ì")
    with col2:
        median_length = news_with_content["content_length"].median()
        st.metric("ì¤‘ê°„ê°’ ë³¸ë¬¸ ê¸¸ì´", f"{median_length:.0f}ì")
    with col3:
        min_length = news_with_content["content_length"].min()
        max_length = news_with_content["content_length"].max()
        st.metric("ìµœì†Œ/ìµœëŒ€ ê¸¸ì´", f"{min_length:.0f} / {max_length:.0f}ì")
    
    if px is not None and len(news_with_content) > 0:
        fig = px.histogram(
            news_with_content,
            x="content_length",
            nbins=50,
            title="ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬",
            labels={"content_length": "ë³¸ë¬¸ ê¸¸ì´ (ì)", "count": "ë¹ˆë„"}
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_content_missing_analysis(news_df: pd.DataFrame):
    """ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨"""
    # content ë˜ëŠ” raw_content_length ì»¬ëŸ¼ í™•ì¸
    content_col = None
    if "content" in news_df.columns:
        content_col = "content"
    elif "raw_content_length" in news_df.columns:
        content_col = "raw_content_length"
    else:
        return
    
    st.markdown("#### ğŸ“ ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨")
    
    total_count = len(news_df)
    
    if content_col == "raw_content_length":
        # raw_content_lengthê°€ ìˆ«ì ì»¬ëŸ¼ì¸ ê²½ìš°
        missing_content = news_df[news_df[content_col].isna() | (pd.to_numeric(news_df[content_col], errors='coerce') == 0)]
        missing_count = len(missing_content)
        
        short_content = news_df[
            news_df[content_col].notna() &
            (pd.to_numeric(news_df[content_col], errors='coerce') < 50)
        ]
        short_count = len(short_content)
    else:
        # content ì»¬ëŸ¼ì¸ ê²½ìš°
        missing_content = news_df[news_df[content_col].isna() | (news_df[content_col] == "")]
        missing_count = len(missing_content)
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš°ë„ ëˆ„ë½ìœ¼ë¡œ ê°„ì£¼ (50ì ë¯¸ë§Œ)
        short_content = news_df[
            news_df[content_col].notna() & 
            (news_df[content_col] != "") &
            (news_df[content_col].astype(str).str.len() < 50)
        ]
        short_count = len(short_content)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ë³¸ë¬¸ ì™„ì „ ëˆ„ë½", f"{missing_count:,}ê±´")
        if total_count > 0:
            missing_rate = (missing_count / total_count) * 100
            st.caption(f"ëˆ„ë½ë¥ : {missing_rate:.1f}%")
    with col2:
        st.metric("ë³¸ë¬¸ ì§§ìŒ (<50ì)", f"{short_count:,}ê±´")
        if total_count > 0:
            short_rate = (short_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {short_rate:.1f}%")
    with col3:
        total_issue = missing_count + short_count
        st.metric("ì´ ë¬¸ì œ ê¸°ì‚¬", f"{total_issue:,}ê±´")
        if total_count > 0:
            issue_rate = (total_issue / total_count) * 100
            st.caption(f"ë¬¸ì œ ë¹„ìœ¨: {issue_rate:.1f}%")
    
    # ì‹œê°„ëŒ€ë³„ ëˆ„ë½/ì§§ì€ ê¸°ì‚¬ ë¹„ìœ¨ ì¶”ì´ (Line chart)
    if "published_at" in news_df.columns and total_count > 0:
        news_with_date = news_df[news_df["published_at"].notna()].copy()
        if not news_with_date.empty:
            news_with_date["date"] = news_with_date["published_at"].dt.date
            
            if content_col == "raw_content_length":
                news_with_date["is_missing"] = news_with_date[content_col].isna() | (pd.to_numeric(news_with_date[content_col], errors='coerce') == 0)
                news_with_date["is_short"] = news_with_date[content_col].notna() & (pd.to_numeric(news_with_date[content_col], errors='coerce') < 50)
            else:
                news_with_date["is_missing"] = news_with_date[content_col].isna() | (news_with_date[content_col] == "")
                news_with_date["is_short"] = (
                    news_with_date[content_col].notna() & 
                    (news_with_date[content_col] != "") &
                    (news_with_date[content_col].astype(str).str.len() < 50)
                )
            
            daily_stats = news_with_date.groupby("date").agg({
                "is_missing": "sum",
                "is_short": "sum"
            }).reset_index()
            daily_stats["total"] = news_with_date.groupby("date").size().values
            daily_stats["missing_rate"] = (daily_stats["is_missing"] / daily_stats["total"] * 100).fillna(0)
            daily_stats["short_rate"] = (daily_stats["is_short"] / daily_stats["total"] * 100).fillna(0)
            daily_stats["issue_rate"] = ((daily_stats["is_missing"] + daily_stats["is_short"]) / daily_stats["total"] * 100).fillna(0)
            
            if px is not None and len(daily_stats) > 0:
                fig = px.line(
                    daily_stats,
                    x="date",
                    y=["missing_rate", "short_rate", "issue_rate"],
                    title="ì¼ë³„ ë³¸ë¬¸ í’ˆì§ˆ ë¬¸ì œ ë¹„ìœ¨ ì¶”ì´",
                    labels={"date": "ë‚ ì§œ", "value": "ë¹„ìœ¨ (%)", "variable": "ìœ í˜•"},
                    color_discrete_map={
                        "missing_rate": "#ef4444",
                        "short_rate": "#f59e0b",
                        "issue_rate": "#3b82f6"
                    }
                )
                fig.update_traces(mode='lines+markers')
                st.plotly_chart(fig, use_container_width=True)
    
    if total_count > 0 and px is not None:
        quality_df = pd.DataFrame({
            "ìƒíƒœ": ["ì •ìƒ", "ëˆ„ë½", "ì§§ìŒ"],
            "ê±´ìˆ˜": [total_count - total_issue, missing_count, short_count]
        })
        fig = px.pie(
            quality_df,
            values="ê±´ìˆ˜",
            names="ìƒíƒœ",
            title="ë³¸ë¬¸ í’ˆì§ˆ ìƒíƒœ"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_title_content_duplication(news_df: pd.DataFrame):
    """ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µë¥ """
    if "title" not in news_df.columns or "content" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ”„ ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µë¥ ")
    
    # titleê³¼ contentê°€ ëª¨ë‘ ìˆëŠ” ë‰´ìŠ¤ë§Œ ë¶„ì„
    valid_news = news_df[
        news_df["title"].notna() & 
        (news_df["title"] != "") &
        news_df["content"].notna() & 
        (news_df["content"] != "")
    ].copy()
    
    if valid_news.empty:
        st.info("ğŸ“Š ì œëª©ê³¼ ë³¸ë¬¸ì´ ëª¨ë‘ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì œëª©ì´ ë³¸ë¬¸ ì•ë¶€ë¶„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    def check_duplication(row):
        title = str(row["title"]).strip()
        content = str(row["content"]).strip()
        
        if not title or not content:
            return False
        
        # ë³¸ë¬¸ ì•ë¶€ë¶„(ì œëª© ê¸¸ì´ì˜ 2ë°°)ì—ì„œ ì œëª© í¬í•¨ ì—¬ë¶€ í™•ì¸
        content_preview = content[:len(title) * 2]
        return title in content_preview
    
    valid_news["has_duplication"] = valid_news.apply(check_duplication, axis=1)
    
    duplication_count = valid_news["has_duplication"].sum()
    total_count = len(valid_news)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì¤‘ë³µ ë°œê²¬", f"{duplication_count:,}ê±´")
    with col2:
        if total_count > 0:
            dup_rate = (duplication_count / total_count) * 100
            st.metric("ì¤‘ë³µë¥ ", f"{dup_rate:.1f}%")
        else:
            st.metric("ì¤‘ë³µë¥ ", "N/A")
    
    if total_count > 0 and px is not None:
        dup_df = pd.DataFrame({
            "ìƒíƒœ": ["ì¤‘ë³µ ì—†ìŒ", "ì¤‘ë³µ ìˆìŒ"],
            "ê±´ìˆ˜": [total_count - duplication_count, duplication_count]
        })
        fig = px.pie(
            dup_df,
            values="ê±´ìˆ˜",
            names="ìƒíƒœ",
            title="ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µ ë¶„í¬"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_impact_score_distribution(news_df: pd.DataFrame):
    """ì„íŒ©íŠ¸ ì ìˆ˜ ë¶„í¬"""
    if "impact_score" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“Š ì„íŒ©íŠ¸ ì ìˆ˜ ë¶„í¬")
    
    # impact_scoreê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_score = news_df[news_df["impact_score"].notna()].copy()
    
    if news_with_score.empty:
        st.info("ğŸ“Š ì„íŒ©íŠ¸ ì ìˆ˜ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        avg_score = news_with_score["impact_score"].mean()
        st.metric("í‰ê·  ì„íŒ©íŠ¸ ì ìˆ˜", f"{avg_score:.1f}")
    with col2:
        median_score = news_with_score["impact_score"].median()
        st.metric("ì¤‘ê°„ê°’ ì ìˆ˜", f"{median_score:.1f}")
    with col3:
        min_score = news_with_score["impact_score"].min()
        max_score = news_with_score["impact_score"].max()
        st.metric("ìµœì†Œ/ìµœëŒ€ ì ìˆ˜", f"{min_score:.0f} / {max_score:.0f}")
    
    if px is not None and len(news_with_score) > 0:
        fig = px.histogram(
            news_with_score,
            x="impact_score",
            nbins=30,
            title="ì„íŒ©íŠ¸ ì ìˆ˜ ë¶„í¬",
            labels={"impact_score": "ì„íŒ©íŠ¸ ì ìˆ˜", "count": "ë¹ˆë„"}
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
        score_bins = [0, 10, 20, 30, 40, 50, 100, float('inf')]
        score_labels = ["0-10", "10-20", "20-30", "30-40", "40-50", "50-100", "100+"]
        news_with_score["score_range"] = pd.cut(
            news_with_score["impact_score"], 
            bins=score_bins, 
            labels=score_labels, 
            right=False
        )
        range_counts = news_with_score["score_range"].value_counts().sort_index()
        
        if len(range_counts) > 0:
            range_df = range_counts.reset_index()
            range_df.columns = ["ì ìˆ˜ êµ¬ê°„", "ê±´ìˆ˜"]
            fig2 = px.bar(
                range_df,
                x="ì ìˆ˜ êµ¬ê°„",
                y="ê±´ìˆ˜",
                title="ì„íŒ©íŠ¸ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬"
            )
            st.plotly_chart(fig2, use_container_width=True)

def _render_duplicate_news_analysis(news_df: pd.DataFrame):
    """ì¤‘ë³µ ê¸°ì‚¬ ë¹„ìœ¨"""
    if "url" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ” ì¤‘ë³µ ê¸°ì‚¬ ë¹„ìœ¨")
    
    # urlì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_url = news_df[news_df["url"].notna() & (news_df["url"] != "")]
    
    if news_with_url.empty:
        st.info("ğŸ“Š URLì´ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸
    url_counts = news_with_url["url"].value_counts()
    duplicate_urls = url_counts[url_counts > 1]
    
    total_news = len(news_with_url)
    unique_urls = len(url_counts)
    duplicate_count = len(duplicate_urls)
    total_duplicate_instances = duplicate_urls.sum() - duplicate_count  # ì¤‘ë³µ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê³ ìœ  URL ìˆ˜", f"{unique_urls:,}ê°œ")
    with col2:
        st.metric("ì¤‘ë³µ URL ìˆ˜", f"{duplicate_count:,}ê°œ")
    with col3:
        if total_news > 0:
            dup_rate = (total_duplicate_instances / total_news) * 100
            st.metric("ì¤‘ë³µ ë¹„ìœ¨", f"{dup_rate:.1f}%")
        else:
            st.metric("ì¤‘ë³µ ë¹„ìœ¨", "N/A")
    
    # ì¤‘ë³µì´ ë§ì€ URL Top 10
    if len(duplicate_urls) > 0:
        top_duplicates = duplicate_urls.head(10).reset_index()
        top_duplicates.columns = ["URL", "ì¤‘ë³µ íšŸìˆ˜"]
        st.dataframe(top_duplicates, use_container_width=True, height=200)

def _render_news_collection_trends(news_df: pd.DataFrame):
    """RSS/í¬ë¡¤ë§ë³„ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸"""
    if "published_at" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸")
    
    # published_atì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_date = news_df[news_df["published_at"].notna()].copy()
    
    if news_with_date.empty:
        st.info("ğŸ“Š ë°œí–‰ì¼ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¼ë³„ ìˆ˜ì§‘ëŸ‰
    news_with_date["date"] = news_with_date["published_at"].dt.date
    daily_counts = news_with_date.groupby("date").size().reset_index(name="ê±´ìˆ˜")
    daily_counts = daily_counts.sort_values("date")
    
    col1, col2 = st.columns(2)
    with col1:
        total_news = len(news_with_date)
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_news:,}ê±´")
    with col2:
        if len(daily_counts) > 0:
            avg_daily = daily_counts["ê±´ìˆ˜"].mean()
            st.metric("ì¼í‰ê·  ìˆ˜ì§‘ëŸ‰", f"{avg_daily:.1f}ê±´")
    
    if px is not None and len(daily_counts) > 0:
        fig = px.line(
            daily_counts,
            x="date",
            y="ê±´ìˆ˜",
            title="ì¼ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì´",
            labels={"date": "ë‚ ì§œ", "ê±´ìˆ˜": "ìˆ˜ì§‘ ê±´ìˆ˜"}
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # sourceë³„ ìˆ˜ì§‘ëŸ‰ (sourceê°€ ìˆëŠ” ê²½ìš°)
    if "source" in news_df.columns:
        news_with_source = news_with_date[news_with_date["source"].notna() & (news_with_date["source"] != "")]
        if not news_with_source.empty:
            source_counts = news_with_source["source"].value_counts().head(10)
            
            if px is not None and len(source_counts) > 0:
                source_df = source_counts.reset_index()
                source_df.columns = ["ì¶œì²˜", "ê±´ìˆ˜"]
                fig2 = px.bar(
                    source_df,
                    x="ì¶œì²˜",
                    y="ê±´ìˆ˜",
                    title="ì¶œì²˜ë³„ ìˆ˜ì§‘ëŸ‰ Top 10",
                    labels={"ì¶œì²˜": "ì–¸ë¡ ì‚¬/ì†ŒìŠ¤", "ê±´ìˆ˜": "ìˆ˜ì§‘ ê±´ìˆ˜"}
                )
                fig2.update_xaxes(tickangle=-45)
                st.plotly_chart(fig2, use_container_width=True)

def _get_korean_font_path():
    """í•œê¸€ í°íŠ¸ ê²½ë¡œ ì°¾ê¸°"""
    # Windows í°íŠ¸ ê²½ë¡œë“¤
    windows_font_paths = [
        "C:/Windows/Fonts/NanumGothic.ttf",
        "C:/Windows/Fonts/NanumBarunGothic.ttf",
        "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
        "C:/Windows/Fonts/gulim.ttc",  # êµ´ë¦¼
    ]
    
    # Linux/Mac í°íŠ¸ ê²½ë¡œë“¤
    linux_font_paths = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/System/Library/Fonts/AppleGothic.ttf",  # Mac
    ]
    
    # Windows ê²½ë¡œ í™•ì¸
    for font_path in windows_font_paths:
        if os.path.exists(font_path):
            return font_path
    
    # Linux/Mac ê²½ë¡œ í™•ì¸
    for font_path in linux_font_paths:
        if os.path.exists(font_path):
            return font_path
    
    return None

def _extract_korean_words(text: str) -> List[str]:
    """í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ (ì¡°ì‚¬/ë¶ˆìš©ì–´ ì œê±°)"""
    if not text:
        return []
    
    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë° ì¡°ì‚¬ (ë‰´ìŠ¤ íŠ¹í™”)
    stopwords = {
        # ì¡°ì‚¬
        "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
        "ì—ì„œ", "ì—ê²Œ", "í•œí…Œ", "ê»˜", "ë¶€í„°", "ê¹Œì§€", "ë§Œ", "ì¡°ì°¨", "ë§ˆì €", "ë¿",
        
        # ëŒ€ëª…ì‚¬
        "ê·¸", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ", "ê·¸ë“¤", "ì´ë“¤", "ì €ë“¤", "ê·¸ë…€", "ê·¸ë¶„",
        
        # ì ‘ì†ì‚¬/ë¶€ì‚¬
        "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê·¸ëŸ°ë°", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ",
        "ë•Œë¬¸ì—", "ìœ„í•´", "ëŒ€í•´", "ê´€ë ¨", "ë”", "ë˜", "ë˜ëŠ”", "ë°",
        
        # ë‰´ìŠ¤ ê´€ë ¨ ìš©ì–´
        "ê¸°ì", "ì—°í•©ë‰´ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ì·¨ì¬", "ì „ë¬¸", "ì¸í„°ë·°",
        "ë°œí‘œ", "ë°œìƒ", "í™•ì¸", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "í–ˆë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
        "ì „ë§", "ì˜ˆìƒ", "ì˜ˆì¸¡", "ì¶”ì •", "ë¶„ì„", "ì¡°ì‚¬", "ê²°ê³¼", "ë°œê²¬",
        "ì´ë²ˆ", "ì´ë‚ ", "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼", "ì§€ë‚œ", "ì˜¬í•´", "ì‘ë…„", "ë‚´ë…„",
        "ì§€ë‚œí•´", "ì˜¬", "ì‘", "ë‚´",
        
        # ì‹œê°„ ê´€ë ¨
        "ì›”", "ì¼", "ë…„", "ì‹œ", "ë¶„", "ì´ˆ", "ìš”ì¼", "ì£¼", "ê°œì›”", "ë…„ë„",
        "ì˜¤ì „", "ì˜¤í›„", "ìƒˆë²½", "ë°¤", "ë‚®", "ì €ë…", "ë‚ ì§œ",
        
        # ìˆ˜ëŸ‰/ë‹¨ìœ„ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ë§Œ", "ì–µ", "ì¡°", "ì›", "ë‹¬ëŸ¬", "í¼ì„¼íŠ¸", "í”„ë¡œ",
        "ê°œ", "ëª…", "ê±´", "ê³³", "ì°¨ë¡€", "ë²ˆ", "íšŒ", "ì°¨",
        
        # ì¼ë°˜ ë™ì‚¬/í˜•ìš©ì‚¬ (ë„ˆë¬´ ë¹ˆë²ˆí•œ ê²ƒë“¤)
        "ìˆë‹¤", "ì—†ë‹¤", "ë˜ë‹¤", "í•˜ë‹¤", "ì´ë‹¤", "ì•Šë‹¤", "ì•„ë‹ˆë‹¤",
        "ê°™ë‹¤", "ë‹¤ë¥´ë‹¤", "í¬ë‹¤", "ì‘ë‹¤", "ë§ë‹¤", "ì ë‹¤", "ë†’ë‹¤", "ë‚®ë‹¤",
        "ì¢‹ë‹¤", "ë‚˜ì˜ë‹¤", "ìƒˆë¡­ë‹¤", "ì˜¤ë˜ë˜ë‹¤", "ìµœê·¼", "ìµœì‹ ",
        "ìˆëŠ”", "ì—†ëŠ”", "ë˜ëŠ”", "í•˜ëŠ”", "ê²ƒìœ¼ë¡œ", "ê²ƒì´ë‹¤", "ê²ƒì´", "ê²ƒì„", "ê²ƒì„",
        
        # ì¼ë°˜ì ì¸ í˜•ìš©ì‚¬/ë¶€ì‚¬
        "ë§¤ìš°", "ì•„ì£¼", "ë„ˆë¬´", "ì •ë§", "ì§„ì§œ", "ì™„ì „", "ì „í˜€", "ë³„ë¡œ",
        "ê°€ì¥", "ìµœê³ ", "ìµœëŒ€", "ìµœì†Œ",
        
        # ë‰´ìŠ¤ ë¬¸ì²´ íŠ¹í™”
        "ë”°ë¥´ë©´", "ì— ë”°ë¥´ë©´", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "ì „í–ˆë‹¤", "ì•Œë ¤ì¡Œë‹¤",
        "í™•ì¸ëë‹¤", "ë°œìƒí–ˆë‹¤", "ë‚˜íƒ€ë‚¬ë‹¤", "ì§€ì í–ˆë‹¤", "ê°•ì¡°í–ˆë‹¤",
        "ì£¼ì¥í–ˆë‹¤", "ì œê¸°í–ˆë‹¤", "ìš”êµ¬í–ˆë‹¤", "ì´‰êµ¬í–ˆë‹¤", "ì œì•ˆí–ˆë‹¤",
        "ë°œí‘œí–ˆë‹¤", "ê³µê°œí–ˆë‹¤", "ë°œí‘œëë‹¤", "ê³µê°œëë‹¤",
        
        # ê¸°ê´€/ì§ì±… (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ì •ë¶€", "êµ­ê°€", "ê¸°ê´€", "ë‹¨ì²´", "ì¡°ì§", "íšŒì‚¬", "ê¸°ì—…", "ë²•ì¸",
        "ëŒ€í‘œ", "ì‚¬ì¥", "íšŒì¥", "ì´ì‚¬", "ì§ì›", "ê´€ê³„ì", "ë‹¹êµ­",
        
        # ì¼ë°˜ì ì¸ ëª…ì‚¬ (ë‰´ìŠ¤ì—ì„œ ë„ˆë¬´ ìì£¼ ë‚˜ì˜¤ëŠ” ê²ƒë“¤)
        "ì‚¬ì‹¤", "ë‚´ìš©", "ìƒí™©", "ë¬¸ì œ", "ì´ìŠˆ", "ì‚¬ê±´", "ì‚¬ê³ ", "ì‚¬ë¡€",
        "ê²½ìš°", "ë•Œë¬¸", "ì´ìœ ", "ì›ì¸", "ê²°ê³¼", "ì˜í–¥", "íš¨ê³¼",
        "ë°©ë²•", "ë°©ì•ˆ", "ëŒ€ì±…", "ì •ì±…", "ì œë„", "ì‹œìŠ¤í…œ",
        "ê³¼ì •", "ì ˆì°¨", "ë‹¨ê³„", "ìˆ˜ì¤€", "ì •ë„", "ë²”ìœ„",
        "êµ­ë‚´", "ëŒ€ë¹„",
        
        # ìœ„ì¹˜/ë°©í–¥ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ìœ„", "ì•„ë˜", "ì•", "ë’¤", "ì™¼ìª½", "ì˜¤ë¥¸ìª½", "ì¤‘ì•™", "ì¤‘ì‹¬",
        "ë‚´ë¶€", "ì™¸ë¶€", "ì•ìª½", "ë’¤ìª½", "ì–‘ìª½",
        
        # ê¸°íƒ€ ë¹ˆë²ˆí•œ ë‹¨ì–´
        "ë“±", "ë°", "ë˜", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜",
        "ì´ë¯¸", "ì•„ì§", "ë²Œì¨", "ê³§", "ê³§ë°”ë¡œ", "ì¦‰ì‹œ", "ë°”ë¡œ",
    }
    
    # ì˜ë¯¸ì—†ëŠ” ìˆ«ì íŒ¨í„´ ì œê±° (1-31ì¼, 1-12ì›” ë“±)
    # ë‹¨ì–´ ì¶”ì¶œ í›„ ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±°
    
    # í•œê¸€ë§Œ ì¶”ì¶œ (í•œê¸€, ê³µë°±, ìˆ«ì)
    korean_text = re.sub(r'[^ê°€-í£\s0-9]', ' ', text)
    
    # ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
    words = korean_text.split()
    
    # í•„í„°ë§ í•¨ìˆ˜
    def should_keep_word(word):
        word = word.strip()
        
        # 2ê¸€ì ë¯¸ë§Œ ì œê±°
        if len(word) < 2:
            return False
        
        # ë¶ˆìš©ì–´ ì œê±°
        if word in stopwords:
            return False
        
        # ì˜ë¯¸ì—†ëŠ” ìˆ«ì íŒ¨í„´ ì œê±° (1-31ì¼, 1-12ì›” ë“±)
        # ìˆ«ìë§Œ ìˆê±°ë‚˜ ìˆ«ì+ì¼/ì›”/ë…„/ì‹œ/ë¶„ ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ì–´
        if re.match(r'^\d+[ì¼ì›”ë…„ì‹œë¶„ì´ˆ]?$', word):
            # 1-31ì¼, 1-12ì›” ê°™ì€ ì˜ë¯¸ì—†ëŠ” ìˆ«ìëŠ” ì œê±°
            num_match = re.match(r'^(\d+)([ì¼ì›”ë…„ì‹œë¶„ì´ˆ]?)$', word)
            if num_match:
                num = int(num_match.group(1))
                suffix = num_match.group(2)
                # 1-31ì¼, 1-12ì›” ê°™ì€ íŒ¨í„´ì€ ì œê±°
                if suffix in ['ì¼', 'ì›”'] and 1 <= num <= 31:
                    return False
                # 1-24ì‹œ ê°™ì€ íŒ¨í„´ë„ ì œê±°
                if suffix == 'ì‹œ' and 1 <= num <= 24:
                    return False
                # 1-60ë¶„ ê°™ì€ íŒ¨í„´ë„ ì œê±°
                if suffix == 'ë¶„' and 1 <= num <= 60:
                    return False
                # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° (ë„ˆë¬´ ì§§ì€ ìˆ«ì)
                if not suffix and num < 100:
                    return False
        
        return True
    
    # í•„í„°ë§ ì ìš©
    filtered_words = [
        word.strip() 
        for word in words 
        if should_keep_word(word.strip())
    ]
    
    return filtered_words

def _render_wordcloud(news_df: pd.DataFrame):
    """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if not WORDCLOUD_AVAILABLE:
        st.info("ğŸ’¡ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `pip install wordcloud matplotlib`ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    st.markdown("#### â˜ï¸ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
    
    # ë‚ ì§œ ì„ íƒ UI
    if "published_at" in news_df.columns:
        news_with_date = news_df[news_df["published_at"].notna()].copy()
        if not news_with_date.empty:
            news_with_date["date"] = pd.to_datetime(news_with_date["published_at"]).dt.date
            available_dates = sorted(news_with_date["date"].unique(), reverse=True)
            
            if len(available_dates) > 0:
                # ê¸°ë³¸ê°’: ê°€ì¥ ìµœê·¼ ë‚ ì§œ
                default_index = 0
                selected_date = st.selectbox(
                    "ë‚ ì§œ ì„ íƒ",
                    options=available_dates,
                    index=default_index,
                    format_func=lambda x: x.strftime("%Yë…„ %mì›” %dì¼") if hasattr(x, 'strftime') else str(x),
                    key="wordcloud_date"
                )
                
                # ì„ íƒí•œ ë‚ ì§œì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                selected_news = news_with_date[
                    news_with_date["date"] == selected_date
                ].copy()
                
                if selected_news.empty:
                    date_str = selected_date.strftime('%Yë…„ %mì›” %dì¼') if hasattr(selected_date, 'strftime') else str(selected_date)
                    st.info(f"ğŸ“Š {date_str}ì— ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return
            else:
                st.info("ğŸ“Š ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
        else:
            st.info("ğŸ“Š ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
    else:
        # published_atì´ ì—†ìœ¼ë©´ ì „ì²´ ë‰´ìŠ¤ ì‚¬ìš©
        selected_news = news_df.copy()
        st.info("âš ï¸ published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        selected_date = None
    
    if selected_news.empty:
        st.info("ğŸ“Š ì„ íƒí•œ ë‚ ì§œì— ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    recent_news = selected_news
    
    # titleê³¼ content í•©ì¹˜ê¸°
    text_list = []
    for idx, row in recent_news.iterrows():
        title = str(row.get("title", "")).strip()
        content = str(row.get("content", "")).strip()
        
        if title or content:
            combined_text = f"{title} {content}"
            text_list.append(combined_text)
    
    if not text_list:
        st.info("ğŸ“Š ì œëª©ê³¼ ë³¸ë¬¸ì´ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    all_text = " ".join(text_list)
    
    # í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ
    words = _extract_korean_words(all_text)
    
    if not words:
        st.info("ğŸ“Š ì¶”ì¶œëœ í•œêµ­ì–´ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
    word_freq = {}
    for word in words:
        word_freq[word] = word_freq.get(word, 0) + 1
    
    if not word_freq:
        st.info("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì°¾ê¸°
    font_path = _get_korean_font_path()
    
    if not font_path:
        st.warning("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›Œë“œí´ë¼ìš°ë“œê°€ ì œëŒ€ë¡œ í‘œì‹œë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ Windows: C:/Windows/Fonts/NanumGothic.ttf ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    try:
        wordcloud_params = {
            "width": 800,
            "height": 400,
            "background_color": "white",
            "max_words": 100,
            "relative_scaling": 0.5,
            "colormap": "viridis"
        }
        
        if font_path:
            wordcloud_params["font_path"] = font_path
        
        wordcloud = WordCloud(**wordcloud_params).generate_from_frequencies(word_freq)
        
        # ì´ë¯¸ì§€ í‘œì‹œ
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        if selected_date:
            date_str = selected_date.strftime('%Yë…„ %mì›” %dì¼') if hasattr(selected_date, 'strftime') else str(selected_date)
            title = f"{date_str} ë‰´ìŠ¤ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ"
        else:
            title = "ë‰´ìŠ¤ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ"
        ax.set_title(title, fontsize=16, pad=20)
        
        st.pyplot(fig)
        plt.close(fig)
        
        # ìƒìœ„ í‚¤ì›Œë“œ í‘œì‹œ
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]
        if top_keywords:
            st.markdown("##### ğŸ” ìƒìœ„ í‚¤ì›Œë“œ Top 20")
            keyword_df = pd.DataFrame(top_keywords, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
            st.dataframe(keyword_df, use_container_width=True, height=300)
    
    except Exception as e:
        st.error(f"âŒ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("ğŸ’¡ í•œê¸€ í°íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¬ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

def _render_news_radar_analysis(news_df: pd.DataFrame):
    """ê¸°ì´ˆ ë‰´ìŠ¤ ì§€í‘œ ë¶„ì„ + ë¼ì´ë‹¤ ì°¨íŠ¸"""
    st.markdown("### ğŸ“Š ê¸°ì´ˆ ë‰´ìŠ¤ ì§€í‘œ ë¶„ì„ + ë¼ì´ë‹¤ ì°¨íŠ¸")
    st.markdown("**ëª©ì **: ë‰´ìŠ¤ì˜ 5ê°€ì§€ ì§€í‘œë¥¼ ë¶„ì„í•˜ì—¬ ì‹œê°í™”")
    
    if news_df.empty:
        st.info("ğŸ“Š ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‰´ìŠ¤ ì„ íƒ UI
    st.markdown("#### ğŸ“° ë¶„ì„í•  ë‰´ìŠ¤ ì„ íƒ")
    
    if "title" in news_df.columns:
        # ì œëª©ìœ¼ë¡œ ì„ íƒ
        if "news_id" in news_df.columns:
            titles_with_id = [f"[{row['news_id']}] {row['title']}" if pd.notna(row['title']) else f"[{row['news_id']}] ì œëª© ì—†ìŒ" 
                             for _, row in news_df.iterrows()]
        else:
            titles_with_id = news_df["title"].fillna("ì œëª© ì—†ìŒ").tolist()
        
        selected_index = st.selectbox(
            "ë‰´ìŠ¤ ì„ íƒ",
            range(len(titles_with_id)),
            format_func=lambda x: titles_with_id[x][:100] + "..." if len(titles_with_id[x]) > 100 else titles_with_id[x],
            key="radar_news_selection"
        )
        
        selected_news = news_df.iloc[selected_index].to_dict()
    else:
        st.warning("âš ï¸ ì œëª© ì»¬ëŸ¼ì´ ì—†ì–´ ë‰´ìŠ¤ë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„ íƒí•œ ë‰´ìŠ¤ì˜ ì§€í‘œ ê³„ì‚°
    news_scores = _calculate_news_scores(selected_news)
    
    # ë¼ì´ë‹¤ ì°¨íŠ¸ ìƒì„±
    if go is not None:
        st.markdown("#### ğŸ“ˆ ì„ íƒí•œ ë‰´ìŠ¤ ë¼ì´ë‹¤ ì°¨íŠ¸")
        
        # ë¼ì´ë‹¤ ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
        categories = ["ì‹œì¥ ì˜í–¥ë„", "ì •ë³´ ë°€ë„", "ì´ˆë³´ì ë‚œì´ë„", "í•™ìŠµ ê°€ì¹˜", "ì‹¤í–‰ ê°€ì¹˜"]
        values = [
            news_scores["market_impact"],
            news_scores["info_density"],
            news_scores["beginner_friendly"],
            news_scores["learning_value"],
            news_scores["action_value"]
        ]
        
        # ë¼ì´ë‹¤ ì°¨íŠ¸ ìƒì„±
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name='ì„ íƒí•œ ë‰´ìŠ¤',
            line_color='rgb(59, 130, 246)'  # íŒŒë€ìƒ‰
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 100]
                )),
            showlegend=True,
            title="ë‰´ìŠ¤ ì§€í‘œ ë¶„ì„",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ì ìˆ˜ ìƒì„¸ ì •ë³´
        st.markdown("##### ğŸ“‹ ìƒì„¸ ì ìˆ˜")
        score_df = pd.DataFrame([
            {"ì§€í‘œ": "ì‹œì¥ ì˜í–¥ë„", "ì ìˆ˜": f"{news_scores['market_impact']:.1f}/100"},
            {"ì§€í‘œ": "ì •ë³´ ë°€ë„", "ì ìˆ˜": f"{news_scores['info_density']:.1f}/100"},
            {"ì§€í‘œ": "ì´ˆë³´ì ë‚œì´ë„", "ì ìˆ˜": f"{news_scores['beginner_friendly']:.1f}/100"},
            {"ì§€í‘œ": "í•™ìŠµ ê°€ì¹˜", "ì ìˆ˜": f"{news_scores['learning_value']:.1f}/100"},
            {"ì§€í‘œ": "ì‹¤í–‰ ê°€ì¹˜", "ì ìˆ˜": f"{news_scores['action_value']:.1f}/100"},
        ])
        st.dataframe(score_df, use_container_width=True)
        
        # ìµœê·¼ 7ì¼ í‰ê·  ë¼ì´ë‹¤ ì°¨íŠ¸
        st.markdown("#### ğŸ“Š ìµœê·¼ 7ì¼ ê¸°ì‚¬ í‰ê·  ë¼ì´ë‹¤ ì°¨íŠ¸")
        
        # ìµœê·¼ 7ì¼ ë‰´ìŠ¤ í•„í„°ë§
        if "published_at" in news_df.columns:
            cutoff_date = datetime.now() - timedelta(days=7)
            recent_news = news_df[
                news_df["published_at"].notna() & 
                (pd.to_datetime(news_df["published_at"]) >= cutoff_date)
            ].copy()
        else:
            recent_news = news_df.copy()
            st.info("âš ï¸ published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        if not recent_news.empty:
            # ìµœê·¼ 7ì¼ ë‰´ìŠ¤ë“¤ì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
            all_scores = []
            for idx, row in recent_news.iterrows():
                scores = _calculate_news_scores(row.to_dict())
                all_scores.append(scores)
            
            if all_scores:
                avg_scores = {
                    "market_impact": sum(s["market_impact"] for s in all_scores) / len(all_scores),
                    "info_density": sum(s["info_density"] for s in all_scores) / len(all_scores),
                    "beginner_friendly": sum(s["beginner_friendly"] for s in all_scores) / len(all_scores),
                    "learning_value": sum(s["learning_value"] for s in all_scores) / len(all_scores),
                    "action_value": sum(s["action_value"] for s in all_scores) / len(all_scores),
                }
                
                # í‰ê·  ë¼ì´ë‹¤ ì°¨íŠ¸ ìƒì„±
                avg_fig = go.Figure()
                
                avg_fig.add_trace(go.Scatterpolar(
                    r=[
                        avg_scores["market_impact"],
                        avg_scores["info_density"],
                        avg_scores["beginner_friendly"],
                        avg_scores["learning_value"],
                        avg_scores["action_value"]
                    ],
                    theta=categories,
                    fill='toself',
                    name='ìµœê·¼ 7ì¼ í‰ê· ',
                    line_color='rgb(34, 197, 94)'  # ì´ˆë¡ìƒ‰
                ))
                
                avg_fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 100]
                        )),
                    showlegend=True,
                    title="ìµœê·¼ 7ì¼ ê¸°ì‚¬ í‰ê·  ì§€í‘œ",
                    height=500
                )
                
                st.plotly_chart(avg_fig, use_container_width=True)
                
                # í‰ê·  ì ìˆ˜ ìƒì„¸ ì •ë³´
                st.markdown("##### ğŸ“‹ ìµœê·¼ 7ì¼ í‰ê·  ì ìˆ˜")
                avg_score_df = pd.DataFrame([
                    {"ì§€í‘œ": "ì‹œì¥ ì˜í–¥ë„", "í‰ê·  ì ìˆ˜": f"{avg_scores['market_impact']:.1f}/100"},
                    {"ì§€í‘œ": "ì •ë³´ ë°€ë„", "í‰ê·  ì ìˆ˜": f"{avg_scores['info_density']:.1f}/100"},
                    {"ì§€í‘œ": "ì´ˆë³´ì ë‚œì´ë„", "í‰ê·  ì ìˆ˜": f"{avg_scores['beginner_friendly']:.1f}/100"},
                    {"ì§€í‘œ": "í•™ìŠµ ê°€ì¹˜", "í‰ê·  ì ìˆ˜": f"{avg_scores['learning_value']:.1f}/100"},
                    {"ì§€í‘œ": "ì‹¤í–‰ ê°€ì¹˜", "í‰ê·  ì ìˆ˜": f"{avg_scores['action_value']:.1f}/100"},
                ])
                st.dataframe(avg_score_df, use_container_width=True)
                st.caption(f"ğŸ“Š ë¶„ì„ ê¸°ì‚¬ ìˆ˜: {len(recent_news):,}ê±´")
            else:
                st.info("ğŸ“Š ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ“Š ìµœê·¼ 7ì¼ê°„ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ Plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë¼ì´ë‹¤ ì°¨íŠ¸ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def _calculate_news_scores(news: Dict[str, Any]) -> Dict[str, float]:
    """
    ë‰´ìŠ¤ì˜ 5ê°€ì§€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ë¼ì´ë‹¤ ì°¨íŠ¸ì— ì‚¬ìš©í•  ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ê° ì§€í‘œëŠ” 0-100 ì ìˆ˜ë¡œ ê³„ì‚°ë˜ë©°, ë‰´ìŠ¤ì˜ ì œëª©ê³¼ ë³¸ë¬¸ì„ ë¶„ì„í•˜ì—¬ ì ìˆ˜ë¥¼ ì‚°ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        news: ë‰´ìŠ¤ ë°ì´í„° ë”•ì…”ë„ˆë¦¬. 'title'ê³¼ 'content' í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
    
    Returns:
        Dict[str, float]: 5ê°€ì§€ ì§€í‘œì˜ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
        - market_impact: ì‹œì¥ ì˜í–¥ë„ (0-100)
        - info_density: ì •ë³´ ë°€ë„ (0-100)
        - beginner_friendly: ì´ˆë³´ì ë‚œì´ë„ (0-100, ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)
        - learning_value: í•™ìŠµ ê°€ì¹˜ (0-100)
        - action_value: ì‹¤í–‰ ê°€ì¹˜ (0-100)
    
    ê³„ì‚° ë°©ë²•:
        ========== 1. ì‹œì¥ ì˜í–¥ë„ (Market Impact) ==========
        ê¸°ë³¸ ì ìˆ˜: 50ì 
        - ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ë§¤ì¹­: í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì  (ìµœëŒ€ 50ì  ì¶”ê°€)
          * í‚¤ì›Œë“œ: ê¸ˆë¦¬, ê¸ˆìœµ, ì¦ê¶Œ, ì£¼ì‹, ì‹œì¥, ê²½ì œ, ì •ì±…, í•œêµ­ì€í–‰, ì½”ìŠ¤í”¼, ì½”ìŠ¤ë‹¥,
                    ì¸í”Œë ˆì´ì…˜, ë””í”Œë ˆì´ì…˜, í™˜ìœ¨, ë¶€ë™ì‚°, íˆ¬ì, ìì‚°
        - ìˆ«ì í¬í•¨ ì—¬ë¶€: ìˆ«ì 1ê°œë‹¹ +2ì  (ìµœëŒ€ 20ì  ì¶”ê°€)
          * íŒ¨í„´: \d+[%ì›ì–µë§Œì¡°]? (ì˜ˆ: "3.5%", "1000ì›", "1ì–µ")
        ìµœì¢… ì ìˆ˜ ë²”ìœ„: 50-100ì 
        
        ========== 2. ì •ë³´ ë°€ë„ (Info Density) ==========
        ê¸°ë³¸ ì ìˆ˜: ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¼ ê²°ì •
        - ë³¸ë¬¸ ê¸¸ì´ ê¸°ì¤€:
          * 2000ì ì´ìƒ: 80ì 
          * 1000-1999ì: 70ì 
          * 500-999ì: 60ì 
          * 500ì ë¯¸ë§Œ: 40ì 
        - ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€: ìˆ«ì 1ê°œë‹¹ +1ì  (ìµœëŒ€ 20ì  ì¶”ê°€)
          * ë³¸ë¬¸ ë‚´ ìˆ«ì íŒ¨í„´ ë§¤ì¹­: \d+[%ì›ì–µë§Œì¡°]?
        ìµœì¢… ì ìˆ˜ ë²”ìœ„: 40-100ì 
        
        ========== 3. ì´ˆë³´ì ë‚œì´ë„ (Beginner Friendly) ==========
        ê¸°ë³¸ ì ìˆ˜: 100ì  (ì „ë¬¸ ìš©ì–´ê°€ ì—†ìœ¼ë©´ ìµœê³ ì )
        - ì „ë¬¸ ìš©ì–´ ê°ì : ì „ë¬¸ ìš©ì–´ 1ê°œë‹¹ -10ì 
          * ì „ë¬¸ ìš©ì–´: íŒŒìƒìƒí’ˆ, ì˜µì…˜, ì„ ë¬¼, ìŠ¤ì™‘, í—¤ì§€, ë ˆë²„ë¦¬ì§€, ë§ˆì§„ì½œ, ì¦ê±°ê¸ˆ,
                      M&A, IPO, ê³µëª¨ì£¼, ë°°ë‹¹ë½ì¼, ì•¡ë©´ë¶„í• , ìœ ìƒì¦ì
        - ë³¸ë¬¸ ê¸¸ì´ ê°ì : ë³¸ë¬¸ì´ 300ì ë¯¸ë§Œì´ë©´ -20ì  (ì •ë³´ ë¶€ì¡±)
        ìµœì¢… ì ìˆ˜ ë²”ìœ„: 0-100ì  (ë†’ì„ìˆ˜ë¡ ì´ˆë³´ìì—ê²Œ ì‰¬ì›€)
        
        ========== 4. í•™ìŠµ ê°€ì¹˜ (Learning Value) ==========
        ê¸°ë³¸ ì ìˆ˜: 50ì 
        - êµìœ¡ì  í‚¤ì›Œë“œ ë§¤ì¹­: í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
          * í‚¤ì›Œë“œ: ì„¤ëª…, ì´ìœ , ë°°ê²½, ê³¼ì •, ë°©ë²•, ì›ë¦¬, ê°œë…, ì˜ë¯¸,
                    ì˜í–¥, íš¨ê³¼, ê²°ê³¼, ë¶„ì„, ì „ë§, ì˜ˆìƒ
        - ë³¸ë¬¸ ê¸¸ì´ ë³´ë„ˆìŠ¤:
          * 1500ì ì´ìƒ: +20ì 
          * 800-1499ì: +10ì 
        ìµœì¢… ì ìˆ˜ ë²”ìœ„: 50-100ì 
        
        ========== 5. ì‹¤í–‰ ê°€ì¹˜ (Action Value) ==========
        ê¸°ë³¸ ì ìˆ˜: 50ì 
        - í–‰ë™ ì§€ì¹¨ í‚¤ì›Œë“œ ë§¤ì¹­: í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
          * í‚¤ì›Œë“œ: ê¶Œì¥, ì œì•ˆ, ì¡°ì–¸, ë°©ì•ˆ, ëŒ€ì±…, ì „ëµ, ê³„íš, ë°©ë²•,
                    í•´ì•¼, í•„ìš”, ì¤‘ìš”, ì£¼ì˜, ê²½ê³ , ì‹œì‚¬ì 
        - êµ¬ì²´ì  ìˆ«ì/ê¸°ê°„ ë³´ë„ˆìŠ¤: ìˆ«ì/ê¸°ê°„ì´ 3ê°œ ì´ìƒì´ë©´ +15ì 
          * íŒ¨í„´: \d+[%ì›ì–µë§Œì¡°ì¼ì›”ë…„] (ì˜ˆ: "3%", "2024ë…„", "1ì›”")
        ìµœì¢… ì ìˆ˜ ë²”ìœ„: 50-100ì 
        
    ì˜ˆì‹œ:
        >>> news = {
        ...     "title": "í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬ 3.5%ë¡œ ì¸ìƒ",
        ...     "content": "í•œêµ­ì€í–‰ì´ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ 0.25%p ì¸ìƒí•˜ì—¬ 3.5%ë¡œ ê²°ì •í–ˆë‹¤. ì´ëŠ” ì¸í”Œë ˆì´ì…˜ì„ ì–µì œí•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë¡œ..."
        ... }
        >>> scores = _calculate_news_scores(news)
        >>> print(scores)
        {
            'market_impact': 85.0,      # ê¸ˆë¦¬, í•œêµ­ì€í–‰, ì¸í”Œë ˆì´ì…˜ í‚¤ì›Œë“œ + ìˆ«ì í¬í•¨
            'info_density': 75.0,       # ë³¸ë¬¸ ê¸¸ì´ + ìˆ«ì í¬í•¨
            'beginner_friendly': 80.0,  # ì „ë¬¸ ìš©ì–´ ì—†ìŒ
            'learning_value': 70.0,     # ì„¤ëª…ì  ë‚´ìš© í¬í•¨
            'action_value': 60.0        # ì¡°ì¹˜, í•„ìš” ë“± í‚¤ì›Œë“œ í¬í•¨
        }
    """
    title = str(news.get("title", "")).strip()
    content = str(news.get("content", "")).strip()
    
    # ê¸°ë³¸ ì ìˆ˜: ëª¨ë“  ì§€í‘œëŠ” 50ì ì—ì„œ ì‹œì‘ (ì´ˆë³´ì ë‚œì´ë„ëŠ” 100ì ì—ì„œ ì‹œì‘)
    scores = {
        "market_impact": 50.0,      # ì‹œì¥ ì˜í–¥ë„: ê¸°ë³¸ 50ì 
        "info_density": 50.0,       # ì •ë³´ ë°€ë„: ê¸°ë³¸ 50ì  (ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¼ ì¬ì„¤ì •ë¨)
        "beginner_friendly": 100.0, # ì´ˆë³´ì ë‚œì´ë„: ê¸°ë³¸ 100ì  (ì „ë¬¸ ìš©ì–´ì— ë”°ë¼ ê°ì )
        "learning_value": 50.0,     # í•™ìŠµ ê°€ì¹˜: ê¸°ë³¸ 50ì 
        "action_value": 50.0        # ì‹¤í–‰ ê°€ì¹˜: ê¸°ë³¸ 50ì 
    }
    
    # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ì œëª© + ë³¸ë¬¸, ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ í‚¤ì›Œë“œ ë§¤ì¹­)
    text = f"{title} {content}".lower()
    content_len = len(content)
    title_len = len(title)
    
    # ========== 1. ì‹œì¥ ì˜í–¥ë„ (Market Impact) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ ê¸ˆìœµ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ í¬ê¸°ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ + ìˆ«ì í¬í•¨ ì—¬ë¶€
    
    # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡ (16ê°œ)
    market_keywords = [
        "ê¸ˆë¦¬", "ê¸ˆìœµ", "ì¦ê¶Œ", "ì£¼ì‹", "ì‹œì¥", "ê²½ì œ", "ì •ì±…", "í•œêµ­ì€í–‰",
        "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "í™˜ìœ¨", "ë¶€ë™ì‚°", "íˆ¬ì", "ìì‚°"
    ]
    market_count = sum(1 for keyword in market_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì  (ìµœëŒ€ 50ì  ì¶”ê°€ ê°€ëŠ¥)
    scores["market_impact"] = min(100, 50 + market_count * 5)
    
    # ìˆ«ì í¬í•¨ ì—¬ë¶€ (ê¸ˆë¦¬, í¼ì„¼íŠ¸, ê¸ˆì•¡ ë“±)
    # íŒ¨í„´: ìˆ«ì + ë‹¨ìœ„(%, ì›, ì–µ, ë§Œ, ì¡°)
    numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', text))
    # ìˆ«ì 1ê°œë‹¹ +2ì  (ìµœëŒ€ 20ì  ì¶”ê°€ ê°€ëŠ¥)
    scores["market_impact"] = min(100, scores["market_impact"] + min(numbers * 2, 20))
    
    # ========== 2. ì •ë³´ ë°€ë„ (Info Density) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ì— í¬í•¨ëœ ì •ë³´ì˜ ì–‘ê³¼ ì§ˆì„ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: ë³¸ë¬¸ ê¸¸ì´ + ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€
    
    if content_len > 0:
        # ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¥¸ ê¸°ë³¸ ì ìˆ˜ ì„¤ì •
        if content_len >= 2000:
            scores["info_density"] = 80  # ë§¤ìš° ê¸´ ê¸°ì‚¬: ì •ë³´ê°€ í’ë¶€í•¨
        elif content_len >= 1000:
            scores["info_density"] = 70  # ê¸´ ê¸°ì‚¬: ì •ë³´ê°€ ë§ìŒ
        elif content_len >= 500:
            scores["info_density"] = 60  # ì¤‘ê°„ ê¸¸ì´: ì ë‹¹í•œ ì •ë³´
        else:
            scores["info_density"] = 40  # ì§§ì€ ê¸°ì‚¬: ì •ë³´ê°€ ë¶€ì¡±í•¨
        
        # ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€ (ë³¸ë¬¸ ë‚´ì—ì„œë§Œ ê²€ìƒ‰)
        numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', content))
        # ìˆ«ì 1ê°œë‹¹ +1ì  (ìµœëŒ€ 20ì  ì¶”ê°€ ê°€ëŠ¥)
        scores["info_density"] = min(100, scores["info_density"] + min(numbers, 20))
    else:
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì •ë³´ ë°€ë„ëŠ” 0ì 
        scores["info_density"] = 0
    
    # ========== 3. ì´ˆë³´ì ë‚œì´ë„ (Beginner Friendly) ê³„ì‚° ==========
    # ëª©ì : ì´ˆë³´ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì •ë„ë¥¼ ì¸¡ì • (ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)
    # ê³„ì‚° ë°©ë²•: ì „ë¬¸ ìš©ì–´ ê°ì  + ë³¸ë¬¸ ê¸¸ì´ ê°ì 
    
    # ì „ë¬¸ ìš©ì–´ ëª©ë¡ (14ê°œ)
    expert_terms = [
        "íŒŒìƒìƒí’ˆ", "ì˜µì…˜", "ì„ ë¬¼", "ìŠ¤ì™‘", "í—¤ì§€", "ë ˆë²„ë¦¬ì§€", "ë§ˆì§„ì½œ", "ì¦ê±°ê¸ˆ",
        "M&A", "IPO", "ê³µëª¨ì£¼", "ë°°ë‹¹ë½ì¼", "ì•¡ë©´ë¶„í• ", "ìœ ìƒì¦ì"
    ]
    expert_count = sum(1 for term in expert_terms if term in text)
    # ì „ë¬¸ ìš©ì–´ 1ê°œë‹¹ -10ì  (ìµœì†Œ 0ì )
    scores["beginner_friendly"] = max(0, 100 - expert_count * 10)
    
    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì´í•´í•˜ê¸° ì–´ë ¤ì›€
    if content_len < 300:
        scores["beginner_friendly"] = max(0, scores["beginner_friendly"] - 20)
    
    # ========== 4. í•™ìŠµ ê°€ì¹˜ (Learning Value) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ êµìœ¡ì  ê°€ì¹˜ë¥¼ ì œê³µí•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: êµìœ¡ì  í‚¤ì›Œë“œ + ë³¸ë¬¸ ê¸¸ì´ ë³´ë„ˆìŠ¤
    
    # êµìœ¡ì  í‚¤ì›Œë“œ ëª©ë¡ (14ê°œ)
    learning_keywords = [
        "ì„¤ëª…", "ì´ìœ ", "ë°°ê²½", "ê³¼ì •", "ë°©ë²•", "ì›ë¦¬", "ê°œë…", "ì˜ë¯¸",
        "ì˜í–¥", "íš¨ê³¼", "ê²°ê³¼", "ë¶„ì„", "ì „ë§", "ì˜ˆìƒ"
    ]
    learning_count = sum(1 for keyword in learning_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
    scores["learning_value"] = min(100, 50 + learning_count * 5)
    
    # ë³¸ë¬¸ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ë” ë§ì€ ë°°ê²½ ì •ë³´ì™€ ì„¤ëª…ì„ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    if content_len >= 1500:
        scores["learning_value"] = min(100, scores["learning_value"] + 20)  # ë§¤ìš° ê¸´ ê¸°ì‚¬: +20ì 
    elif content_len >= 800:
        scores["learning_value"] = min(100, scores["learning_value"] + 10)   # ê¸´ ê¸°ì‚¬: +10ì 
    
    # ========== 5. ì‹¤í–‰ ê°€ì¹˜ (Action Value) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì´ë‚˜ í–‰ë™ ì§€ì¹¨ì„ ì œê³µí•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: í–‰ë™ ì§€ì¹¨ í‚¤ì›Œë“œ + êµ¬ì²´ì  ìˆ«ì/ê¸°ê°„ ë³´ë„ˆìŠ¤
    
    # í–‰ë™ ì§€ì¹¨ í‚¤ì›Œë“œ ëª©ë¡ (14ê°œ)
    action_keywords = [
        "ê¶Œì¥", "ì œì•ˆ", "ì¡°ì–¸", "ë°©ì•ˆ", "ëŒ€ì±…", "ì „ëµ", "ê³„íš", "ë°©ë²•",
        "í•´ì•¼", "í•„ìš”", "ì¤‘ìš”", "ì£¼ì˜", "ê²½ê³ ", "ì‹œì‚¬ì "
    ]
    action_count = sum(1 for keyword in action_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
    scores["action_value"] = min(100, 50 + action_count * 5)
    
    # êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ê¸°ê°„ì´ ìˆìœ¼ë©´ ì‹¤í–‰ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    # íŒ¨í„´: ìˆ«ì + ë‹¨ìœ„(%, ì›, ì–µ, ë§Œ, ì¡°, ì¼, ì›”, ë…„)
    specific_numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°ì¼ì›”ë…„]', text))
    if specific_numbers >= 3:
        scores["action_value"] = min(100, scores["action_value"] + 15)  # êµ¬ì²´ì  ì •ë³´ ë³´ë„ˆìŠ¤
    
    # ========== ìµœì¢… ì ìˆ˜ ì •ê·œí™” ==========
    # ëª¨ë“  ì ìˆ˜ë¥¼ 0-100 ë²”ìœ„ë¡œ ì œí•œ (ì•ˆì „ì¥ì¹˜)
    for key in scores:
        scores[key] = max(0, min(100, scores[key]))
    
    return scores

def _render_url_parsing_quality_for_content(df_view: pd.DataFrame):
    """ì½˜í…ì¸  í’ˆì§ˆ íƒ­ìš© URL íŒŒì‹± í’ˆì§ˆ"""
    url_events = df_view[df_view["event_name"].isin(["news_url_added_from_chat", "news_url_add_error"])]
    
    if url_events.empty:
        st.info("ğŸ“Š URL íŒŒì‹± ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.markdown("#### ğŸ“° URL íŒŒì‹± í’ˆì§ˆ (í¬ë¡¤ë§/íŒŒì‹± ì‹¤íŒ¨ìœ¨)")
    
    success_count = int((url_events["event_name"] == "news_url_added_from_chat").sum())
    error_count = int((url_events["event_name"] == "news_url_add_error").sum())
    total_count = success_count + error_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("íŒŒì‹± ì„±ê³µ", success_count)
    with col2:
        st.metric("íŒŒì‹± ì‹¤íŒ¨", error_count)
    with col3:
        if total_count > 0:
            failure_rate = (error_count / total_count) * 100
            st.metric("ì‹¤íŒ¨ìœ¨", f"{failure_rate:.1f}%")
        else:
            st.metric("ì‹¤íŒ¨ìœ¨", "N/A")
    
    # ì‹œê°„ëŒ€ë³„ ì‹¤íŒ¨ìœ¨ ì¶”ì´
    if total_count > 0:
        url_events_copy = url_events.copy()
        url_events_copy["hour"] = url_events_copy["event_time"].dt.floor("H")
        
        # ì„±ê³µ/ì‹¤íŒ¨ë³„ë¡œ ê·¸ë£¹í™”
        success_by_hour = url_events_copy[url_events_copy["event_name"] == "news_url_added_from_chat"].groupby("hour").size().reset_index(name="ì„±ê³µ")
        error_by_hour = url_events_copy[url_events_copy["event_name"] == "news_url_add_error"].groupby("hour").size().reset_index(name="ì‹¤íŒ¨")
        
        # ë³‘í•©
        hourly_stats = success_by_hour.merge(error_by_hour, on="hour", how="outer").fillna(0)
        
        if len(hourly_stats) > 0 and px is not None:
            hourly_stats["ì‹¤íŒ¨ìœ¨"] = (hourly_stats["ì‹¤íŒ¨"] / (hourly_stats["ì„±ê³µ"] + hourly_stats["ì‹¤íŒ¨"]) * 100).fillna(0)
            fig = px.line(
                hourly_stats,
                x="hour",
                y="ì‹¤íŒ¨ìœ¨",
                title="ì‹œê°„ëŒ€ë³„ URL íŒŒì‹± ì‹¤íŒ¨ìœ¨ ì¶”ì´"
            )
            st.plotly_chart(fig, use_container_width=True)

def _render_prompt_tuning_comparison(news_df: pd.DataFrame):
    """í”„ë¡¬í”„íŠ¸ íŠœë‹ìš© ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë¹„êµ ë¸”ë¡"""
    st.markdown("### 4ï¸âƒ£ ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë¹„êµ (í”„ë¡¬í”„íŠ¸ íŠœë‹ìš©)")
    st.markdown("**ëª©ì **: ë‰´ìŠ¤ ì›ë¬¸ + LLM ìš”ì•½ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„  í¬ì¸íŠ¸ ë°œê²¬")
    
    if news_df.empty:
        st.info("ğŸ“Š ë¹„êµí•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•„í„°ë§ UI
    st.markdown("#### ğŸ” ê¸°ì‚¬ í•„í„°ë§")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # ë‚ ì§œ í•„í„°
        if "published_at" in news_df.columns:
            news_with_date = news_df[news_df["published_at"].notna()].copy()
            if not news_with_date.empty:
                min_date = news_with_date["published_at"].min().date()
                max_date = news_with_date["published_at"].max().date()
                date_range = st.date_input(
                    "ë°œí–‰ì¼ ë²”ìœ„",
                    value=(min_date, max_date),
                    min_value=min_date,
                    max_value=max_date,
                    key="prompt_tuning_date_range"
                )
            else:
                date_range = None
        else:
            date_range = None
    
    with col2:
        # ì–¸ë¡ ì‚¬ í•„í„°
        if "source" in news_df.columns:
            sources = ["ì „ì²´"] + sorted(news_df[news_df["source"].notna()]["source"].unique().tolist())
            selected_source = st.selectbox("ì–¸ë¡ ì‚¬", sources, key="prompt_tuning_source")
        else:
            selected_source = "ì „ì²´"
    
    with col3:
        # ë³¸ë¬¸ ê¸¸ì´ í•„í„°
        if "content" in news_df.columns:
            news_with_content = news_df[news_df["content"].notna() & (news_df["content"] != "")].copy()
            if not news_with_content.empty:
                news_with_content["content_length"] = news_with_content["content"].astype(str).str.len()
                min_len = int(news_with_content["content_length"].min())
                max_len = int(news_with_content["content_length"].max())
                length_range = st.slider(
                    "ë³¸ë¬¸ ê¸¸ì´ (ì)",
                    min_value=min_len,
                    max_value=max_len,
                    value=(min_len, max_len),
                    key="prompt_tuning_length"
                )
            else:
                length_range = None
        else:
            length_range = None
    
    with col4:
        # ê¸ˆìœµ/ë¹„ê¸ˆìœµ í•„í„°
        if "is_finance_news" in news_df.columns:
            category_options = ["ì „ì²´", "ê¸ˆìœµ", "ë¹„ê¸ˆìœµ"]
            selected_category = st.selectbox("ì¹´í…Œê³ ë¦¬", category_options, key="prompt_tuning_category")
        else:
            selected_category = "ì „ì²´"
    
    # í•„í„°ë§ ì ìš©
    filtered_df = news_df.copy()
    
    # ë‚ ì§œ í•„í„°
    if date_range and len(date_range) == 2 and "published_at" in filtered_df.columns:
        start_date, end_date = date_range
        filtered_df = filtered_df[
            (pd.to_datetime(filtered_df["published_at"]).dt.date >= start_date) &
            (pd.to_datetime(filtered_df["published_at"]).dt.date <= end_date)
        ]
    
    # ì–¸ë¡ ì‚¬ í•„í„°
    if selected_source != "ì „ì²´" and "source" in filtered_df.columns:
        filtered_df = filtered_df[filtered_df["source"] == selected_source]
    
    # ê¸¸ì´ í•„í„°
    if length_range and "content" in filtered_df.columns:
        filtered_df = filtered_df[
            filtered_df["content"].notna() & (filtered_df["content"] != "")
        ].copy()
        filtered_df["content_length"] = filtered_df["content"].astype(str).str.len()
        filtered_df = filtered_df[
            (filtered_df["content_length"] >= length_range[0]) &
            (filtered_df["content_length"] <= length_range[1])
        ]
    
    # ì¹´í…Œê³ ë¦¬ í•„í„°
    if selected_category != "ì „ì²´" and "is_finance_news" in filtered_df.columns:
        if selected_category == "ê¸ˆìœµ":
            filtered_df = filtered_df[filtered_df["is_finance_news"] == True]
        else:
            filtered_df = filtered_df[filtered_df["is_finance_news"] == False]
    
    if filtered_df.empty:
        st.warning("âš ï¸ í•„í„° ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.info(f"ğŸ“Š í•„í„°ë§ëœ ê¸°ì‚¬: {len(filtered_df):,}ê±´")
    
    # ìš”ì•½ í’ˆì§ˆ í†µê³„ ë¶„ì„
    st.markdown("---")
    st.markdown("#### ğŸ“Š ìš”ì•½ í’ˆì§ˆ í†µê³„ ë¶„ì„")
    
    # í•„í„°ë§ëœ ê¸°ì‚¬ë“¤ì— ëŒ€í•´ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì‹¤í–‰
    quality_stats = {
        "ì œëª© ê·¸ëŒ€ë¡œ í¬í•¨": 0,
        "ìš”ì•½ ê³¼ë„í•˜ê²Œ ì¥í™©": 0,
        "ìš”ì•½ ë„ˆë¬´ ì§§ìŒ": 0,
        "ê¸ˆìœµ ë‰´ìŠ¤ ìˆ«ì ëˆ„ë½": 0,
        "ë³¸ë¬¸ ì•ë¶€ë¶„ ë³µì‚¬": 0,
        "í’ˆì§ˆ í†µê³¼": 0,
        "ìš”ì•½ ì—†ìŒ": 0
    }
    
    # ì²´í¬ë¦¬ìŠ¤íŠ¸ í•¨ìˆ˜
    def check_summary_quality(row, selected_category):
        """ê¸°ì‚¬ë³„ ìš”ì•½ í’ˆì§ˆ ì²´í¬"""
        title = row.get("title", "")
        content = row.get("content", "")
        summary = row.get("summary", "")
        
        if not content or not summary or pd.isna(summary) or str(summary).strip() == "":
            return "ìš”ì•½ ì—†ìŒ"
        
        content_str = str(content)
        summary_str = str(summary)
        
        # 1. ì œëª© ê·¸ëŒ€ë¡œ ë³µë¶™ ì—¬ë¶€
        if title and str(title) in summary_str:
            return "ì œëª© ê·¸ëŒ€ë¡œ í¬í•¨"
        
        # 2. ë³¸ë¬¸ ê¸¸ì´ ëŒ€ë¹„ ìš”ì•½ ê¸¸ì´
        content_len = len(content_str)
        summary_len = len(summary_str)
        if content_len > 0:
            summary_ratio = (summary_len / content_len) * 100
            if content_len < 500 and summary_ratio > 50:
                return "ìš”ì•½ ê³¼ë„í•˜ê²Œ ì¥í™©"
            elif summary_ratio < 5:
                return "ìš”ì•½ ë„ˆë¬´ ì§§ìŒ"
        
        # 3. ìˆ«ì/ë³€í™”í­ í¬í•¨ ì—¬ë¶€ (ê¸ˆìœµ/ì •ì±… ë‰´ìŠ¤)
        if selected_category == "ê¸ˆìœµ":
            numbers_in_content = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', content_str))
            numbers_in_summary = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', summary_str))
            if numbers_in_content > 0 and numbers_in_summary == 0:
                return "ê¸ˆìœµ ë‰´ìŠ¤ ìˆ«ì ëˆ„ë½"
        
        # 4. ë³¸ë¬¸ ì•ë¶€ë¶„ë§Œ ë³µì‚¬ ì—¬ë¶€
        if content_str[:200] in summary_str:
            return "ë³¸ë¬¸ ì•ë¶€ë¶„ ë³µì‚¬"
        
        return "í’ˆì§ˆ í†µê³¼"
    
    # í•„í„°ë§ëœ ëª¨ë“  ê¸°ì‚¬ì— ëŒ€í•´ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì‹¤í–‰
    for idx, row in filtered_df.iterrows():
        quality_result = check_summary_quality(row, selected_category)
        if quality_result in quality_stats:
            quality_stats[quality_result] += 1
    
    # í†µê³„ í‘œì‹œ
    total_checked = sum(quality_stats.values())
    if total_checked > 0:
        # íŒŒì´ ê·¸ë˜í”„ìš© ë°ì´í„° ì¤€ë¹„
        quality_items = []
        quality_counts = []
        
        for key, count in quality_stats.items():
            if count > 0:
                quality_items.append(key)
                quality_counts.append(count)
        
        if quality_items and px is not None:
            quality_df = pd.DataFrame({
                "í•­ëª©": quality_items,
                "ê±´ìˆ˜": quality_counts
            })
            
            # íŒŒì´ ê·¸ë˜í”„ ìƒì„±
            fig = px.pie(
                quality_df,
                values="ê±´ìˆ˜",
                names="í•­ëª©",
                title="ìš”ì•½ í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ë³„ ë¹„ìœ¨",
                hole=0.4  # ë„ë„› ì°¨íŠ¸
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # í†µê³„ í…Œì´ë¸”
        st.markdown("##### ğŸ“‹ ìƒì„¸ í†µê³„")
        stats_df = pd.DataFrame([
            {"í•­ëª©": key, "ê±´ìˆ˜": count, "ë¹„ìœ¨": f"{(count/total_checked*100):.1f}%"}
            for key, count in quality_stats.items() if count > 0
        ])
        st.dataframe(stats_df, use_container_width=True)
    else:
        st.info("ğŸ“Š ìš”ì•½ì´ ìˆëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ê¸°ì‚¬ ì„ íƒ UI
    st.markdown("---")
    st.markdown("#### ğŸ“° ê¸°ì‚¬ ì„ íƒ")
    
    # ê¸°ì‚¬ ì„ íƒì„ ìœ„í•œ selectbox
    if "title" in filtered_df.columns:
        # ì œëª©ìœ¼ë¡œ ì„ íƒ
        if "news_id" in filtered_df.columns:
            titles_with_id = [f"[{row['news_id']}] {row['title']}" if pd.notna(row['title']) else f"[{row['news_id']}] ì œëª© ì—†ìŒ" 
                             for _, row in filtered_df.iterrows()]
        else:
            titles_with_id = filtered_df["title"].fillna("ì œëª© ì—†ìŒ").tolist()
        
        selected_index = st.selectbox(
            "ê¸°ì‚¬ ì„ íƒ",
            range(len(titles_with_id)),
            format_func=lambda x: titles_with_id[x][:100] + "..." if len(titles_with_id[x]) > 100 else titles_with_id[x],
            key="prompt_tuning_selected_article"
        )
        
        selected_article = filtered_df.iloc[selected_index].to_dict()
    else:
        st.warning("âš ï¸ ì œëª© ì»¬ëŸ¼ì´ ì—†ì–´ ê¸°ì‚¬ë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¢Œìš° ë¹„êµ í™”ë©´
    st.markdown("---")
    st.markdown("#### ğŸ“Š ì›ë¬¸ vs ìš”ì•½ ë¹„êµ")
    
    col_left, col_right = st.columns(2)
    
    with col_left:
        st.markdown("##### ğŸ“„ ì›ë¬¸")
        
        # ì œëª©
        title = selected_article.get("title", "ì œëª© ì—†ìŒ")
        st.markdown(f"**{title}**")
        
        # ë©”íƒ€ ì •ë³´
        meta_info = []
        if "source" in selected_article and pd.notna(selected_article.get("source")):
            meta_info.append(f"ì¶œì²˜: {selected_article['source']}")
        if "published_at" in selected_article and pd.notna(selected_article.get("published_at")):
            pub_date = pd.to_datetime(selected_article["published_at"])
            meta_info.append(f"ë‚ ì§œ: {pub_date.strftime('%Y-%m-%d %H:%M')}")
        if "content" in selected_article:
            content_len = len(str(selected_article.get("content", "")))
            meta_info.append(f"ê¸¸ì´: {content_len:,}ì")
        
        if meta_info:
            st.caption(" | ".join(meta_info))
        
        # ë³¸ë¬¸
        content = selected_article.get("content", "")
        if content:
            st.markdown("**ë³¸ë¬¸:**")
            st.text_area(
                "ë³¸ë¬¸ ë‚´ìš©",
                value=str(content),
                height=400,
                disabled=True,
                key="original_content_display"
            )
        else:
            st.warning("âš ï¸ ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    with col_right:
        st.markdown("##### ğŸ¤– LLM ìš”ì•½ ê²°ê³¼")
        
        # ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        summary = selected_article.get("summary", "")
        
        if summary and pd.notna(summary) and str(summary).strip():
            st.markdown("**ìš”ì•½:**")
            st.text_area(
                "ìš”ì•½ ë‚´ìš©",
                value=str(summary),
                height=400,
                disabled=True,
                key="summary_display"
            )
        else:
            st.info("ğŸ’¡ ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤. Supabase `summary` ì»¬ëŸ¼ì„ í™•ì¸í•˜ê±°ë‚˜ on-demand ìƒì„± ê¸°ëŠ¥ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.")
            st.caption("(í–¥í›„ on-demand ìš”ì•½ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€ ì˜ˆì •)")
        
        # ìš”ì•½ í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸
        st.markdown("**ğŸ” í”„ë¡¬í”„íŠ¸ ê°œì„  ì²´í¬ë¦¬ìŠ¤íŠ¸:**")
        
        if content and summary:
            content_str = str(content)
            summary_str = str(summary)
            
            checks = []
            
            # 1. ì œëª© ê·¸ëŒ€ë¡œ ë³µë¶™ ì—¬ë¶€
            if title and title in summary_str:
                checks.append("âš ï¸ ì œëª©ì´ ìš”ì•½ì— ê·¸ëŒ€ë¡œ í¬í•¨ë¨ â†’ ì œëª© ì¬ì‘ì„± ê·œì¹™ ê°•í™” í•„ìš”")
            
            # 2. ë³¸ë¬¸ ê¸¸ì´ ëŒ€ë¹„ ìš”ì•½ ê¸¸ì´
            content_len = len(content_str)
            summary_len = len(summary_str)
            if content_len > 0:
                summary_ratio = (summary_len / content_len) * 100
                if content_len < 500 and summary_ratio > 50:
                    checks.append("âš ï¸ ì§§ì€ ê¸°ì‚¬ì¸ë° ìš”ì•½ì´ ê³¼ë„í•˜ê²Œ ì¥í™©í•¨ â†’ ë‹¨ì‹ ì€ í•´ì„ ì¤„ì´ê¸°")
                elif summary_ratio < 5:
                    checks.append("âš ï¸ ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŒ â†’ í•µì‹¬ ì •ë³´ í¬í•¨ ê°•í™”")
            
            # 3. ìˆ«ì/ë³€í™”í­ í¬í•¨ ì—¬ë¶€ (ê¸ˆìœµ/ì •ì±… ë‰´ìŠ¤)
            # selected_categoryê°€ "ê¸ˆìœµ"ì´ë©´ ì´ë¯¸ ê¸ˆìœµ ê¸°ì‚¬ë¡œ í•„í„°ë§ëœ ìƒíƒœ
            # is_finance_news ì»¬ëŸ¼ì´ ì—†ì–´ë„ selected_categoryë¡œ íŒë‹¨ ê°€ëŠ¥
            if selected_category == "ê¸ˆìœµ":
                numbers_in_content = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', content_str))
                numbers_in_summary = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', summary_str))
                if numbers_in_content > 0 and numbers_in_summary == 0:
                    checks.append("âš ï¸ ê¸ˆë¦¬/ì •ì±… ë‰´ìŠ¤ì¸ë° ìˆ«ì/ë³€í™”í­ì´ ìš”ì•½ì— ì—†ìŒ â†’ ìˆ«ì í•„ìˆ˜ ê·œì¹™ ì¶”ê°€")
            
            # 4. ë³¸ë¬¸ ì•ë¶€ë¶„ë§Œ ë³µì‚¬ ì—¬ë¶€
            if content_str[:200] in summary_str:
                checks.append("âš ï¸ ë³¸ë¬¸ ì•ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•œ ë“¯í•¨ â†’ ìš”ì•½ ì¬ì‘ì„± ê°•í™”")
            
            if checks:
                for check in checks:
                    st.warning(check)
            else:
                st.success("âœ… ê¸°ë³¸ì ì¸ í’ˆì§ˆ ì²´í¬ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ’¡ ì›ë¬¸ê³¼ ìš”ì•½ì„ ë¹„êµí•˜ë ¤ë©´ ë‘˜ ë‹¤ í•„ìš”í•©ë‹ˆë‹¤.")

# ============================================================================
# íƒ­ 3: ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (User Behavior)
# ============================================================================

def _render_user_behavior_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° íƒ­
    â†’ "ì‚¬ìš©ìê°€ ìš°ë¦¬ê°€ ë§Œë“  ê¸°ëŠ¥ì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê³  ìˆëŠ”ê°€?"
    """
    st.markdown("### ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (User Behavior)")
    st.markdown("**ëª©í‘œ**: ì‚¬ìš©ìì˜ ì‹¤ì œ í–‰ë™ íŒ¨í„´ ë¶„ì„ - MVP ê¸°ëŠ¥ì˜ ê°€ì¹˜ ì—¬ë¶€ íŒë‹¨")
    
    # ì£¼ìš” ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        news_clicks = int((df_view["event_name"] == "news_click").sum())
        st.metric("ë‰´ìŠ¤ í´ë¦­", news_clicks)
    with col2:
        detail_opens = int((df_view["event_name"] == "news_detail_open").sum())
        st.metric("ìƒì„¸ ì§„ì…", detail_opens)
        if news_clicks > 0:
            ctr = (detail_opens / news_clicks) * 100
            st.caption(f"ì§„ì…ë¥ : {ctr:.1f}%")
    with col3:
        chat_questions = int((df_view["event_name"] == "chat_question").sum())
        st.metric("ì±— ì§ˆë¬¸", chat_questions)
    with col4:
        search_requests = int((df_view["event_name"] == "news_search_from_chat").sum())
        st.metric("ë‰´ìŠ¤ ê²€ìƒ‰", search_requests)
    
    # 1. ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)
    _render_news_ctr(df_view)
    
    # 2. ê¸°ì‚¬ ìƒì„¸ ì§„ì…ë¥  (ìœ„ì— í¬í•¨)
    
    # 3. ê¸°ì‚¬ ì½ê¸° ì‹œê°„(dwell time)
    _render_dwell_time(df_view)
    
    # 4. ìš”ì•½ í´ë¦­ë¥ 
    _render_summary_clicks(df_view)
    
    # 5. ìš©ì–´ í´ë¦­ë¥ 
    _render_term_clicks(df_view)
    
    # 6. ìì—°ì–´ ê²€ìƒ‰ ì„±ê³µë¥ 
    _render_search_success_rate(df_view)
    
    # 7. ê²€ìƒ‰ â†’ í´ë¦­ ì „í™˜ë¥ 
    _render_search_to_click_conversion(df_view)
    
    # 8. URL ì…ë ¥ ê¸°ëŠ¥ ì‚¬ìš©ë¥ 
    _render_url_input_usage(df_view)
    
    # 9. ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬
    _render_chat_question_types(df_view)
    
    # 10. ì¬ë°©ë¬¸ ì„¸ì…˜ ìˆ˜
    _render_returning_sessions(df_view, session_column)

def _render_news_ctr(df_view: pd.DataFrame):
    """ë‰´ìŠ¤ í´ë¦­ë¥  ë¶„ì„"""
    st.markdown("#### ğŸ“Š ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)")
    
    clicks = df_view[df_view["event_name"] == "news_click"]
    views = df_view[df_view["event_name"].isin(["news_click", "news_detail_open"])]
    
    if len(views) > 0:
        ctr = (len(clicks) / len(views)) * 100
        st.metric("í´ë¦­ë¥ ", f"{ctr:.1f}%")
        
        # ì‹œê°„ëŒ€ë³„ CTR
        if len(clicks) > 0:
            clicks["hour"] = clicks["event_time"].dt.floor("H")
            hourly_clicks = clicks.groupby("hour").size().reset_index(name="í´ë¦­ ìˆ˜")
            
            if px is not None and len(hourly_clicks) > 0:
                fig = px.bar(
                    hourly_clicks,
                    x="hour",
                    y="í´ë¦­ ìˆ˜",
                    title="ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ í´ë¦­ ìˆ˜"
                )
                st.plotly_chart(fig, use_container_width=True)

def _render_dwell_time(df_view: pd.DataFrame):
    """ê¸°ì‚¬ ì½ê¸° ì‹œê°„ ë¶„ì„"""
    view_duration_events = df_view[df_view["event_name"] == "view_duration"]
    
    if view_duration_events.empty:
        return
    
    st.markdown("#### â±ï¸ ê¸°ì‚¬ ì½ê¸° ì‹œê°„ (Dwell Time)")
    
    duration_data = []
    for idx, row in view_duration_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        duration_sec = payload.get("duration_sec")
        if duration_sec is not None:
            duration_data.append({
                "event_time": row.get("event_time"),
                "ì½ê¸° ì‹œê°„ (ì´ˆ)": duration_sec,
            })
    
    if duration_data:
        duration_df = pd.DataFrame(duration_data)
        avg_duration = duration_df["ì½ê¸° ì‹œê°„ (ì´ˆ)"].mean()
        st.metric("í‰ê·  ì½ê¸° ì‹œê°„", f"{avg_duration:.1f}ì´ˆ")
        
        if px is not None and len(duration_df) > 0:
            fig = px.histogram(
                duration_df,
                x="ì½ê¸° ì‹œê°„ (ì´ˆ)",
                nbins=30,
                title="ê¸°ì‚¬ ì½ê¸° ì‹œê°„ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)

def _render_summary_clicks(df_view: pd.DataFrame):
    """ìš”ì•½ í´ë¦­ë¥  ë¶„ì„"""
    # ìš”ì•½ ê´€ë ¨ ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ ë¶„ì„
    summary_events = df_view[df_view["event_name"].str.contains("summary", case=False, na=False)]
    
    if summary_events.empty:
        return
    
    st.markdown("#### ğŸ“ ìš”ì•½ í´ë¦­ë¥ ")
    summary_clicks = len(summary_events)
    st.metric("ìš”ì•½ í´ë¦­", summary_clicks)

def _render_term_clicks(df_view: pd.DataFrame):
    """ìš©ì–´ í´ë¦­ë¥  ë¶„ì„"""
    term_clicks = df_view[df_view["event_name"] == "glossary_click"]
    term_answers = df_view[df_view["event_name"] == "glossary_answer"]
    
    if term_clicks.empty and term_answers.empty:
        return
    
    st.markdown("#### ğŸ’¡ ìš©ì–´ í´ë¦­ë¥ ")
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ìš©ì–´ í´ë¦­", len(term_clicks))
    with col2:
        st.metric("ìš©ì–´ ë‹µë³€", len(term_answers))
    
    # ì¸ê¸° ìš©ì–´ Top 10
    if not term_clicks.empty:
        term_list = []
        for idx, row in term_clicks.iterrows():
            term = _get_term_from_row(row)
            if term:
                term_list.append(term)
        
        if term_list:
            term_counts = pd.Series(term_list).value_counts().head(10)
            if px is not None and len(term_counts) > 0:
                fig = px.bar(
                    x=term_counts.index,
                    y=term_counts.values,
                    title="ì¸ê¸° ìš©ì–´ Top 10",
                    labels={"x": "ìš©ì–´", "y": "í´ë¦­ ìˆ˜"}
                )
                st.plotly_chart(fig, use_container_width=True)

def _render_search_success_rate(df_view: pd.DataFrame):
    """ìì—°ì–´ ê²€ìƒ‰ ì„±ê³µë¥ """
    search_success = df_view[df_view["event_name"] == "news_search_from_chat"]
    search_failed = df_view[df_view["event_name"] == "news_search_failed"]
    
    if search_success.empty and search_failed.empty:
        return
    
    st.markdown("#### ğŸ” ìì—°ì–´ ê²€ìƒ‰ ì„±ê³µë¥ ")
    
    success_count = len(search_success)
    failed_count = len(search_failed)
    total_count = success_count + failed_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê²€ìƒ‰ ì„±ê³µ", success_count)
    with col2:
        st.metric("ê²€ìƒ‰ ì‹¤íŒ¨", failed_count)
    with col3:
        if total_count > 0:
            success_rate = (success_count / total_count) * 100
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        else:
            st.metric("ì„±ê³µë¥ ", "N/A")

def _render_search_to_click_conversion(df_view: pd.DataFrame):
    """ê²€ìƒ‰ â†’ í´ë¦­ ì „í™˜ë¥ """
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"]
    selected_from_search = df_view[df_view["event_name"] == "news_selected_from_chat"]
    
    if search_events.empty:
        return
    
    st.markdown("#### ğŸ¯ ê²€ìƒ‰ â†’ í´ë¦­ ì „í™˜ë¥ ")
    
    search_count = len(search_events)
    selected_count = len(selected_from_search)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ê²€ìƒ‰ ì‹¤í–‰", search_count)
    with col2:
        st.metric("ê²€ìƒ‰ ê²°ê³¼ ì„ íƒ", selected_count)
        if search_count > 0:
            conversion_rate = (selected_count / search_count) * 100
            st.caption(f"ì „í™˜ìœ¨: {conversion_rate:.1f}%")

def _render_url_input_usage(df_view: pd.DataFrame):
    """URL ì…ë ¥ ê¸°ëŠ¥ ì‚¬ìš©ë¥ """
    url_added = df_view[df_view["event_name"] == "news_url_added_from_chat"]
    
    if url_added.empty:
        return
    
    st.markdown("#### ğŸ”— URL ì…ë ¥ ê¸°ëŠ¥ ì‚¬ìš©ë¥ ")
    
    url_count = len(url_added)
    total_sessions = df_view["session_id"].nunique() if "session_id" in df_view.columns else 0
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("URLë¡œ ì¶”ê°€ëœ ê¸°ì‚¬", url_count)
    with col2:
        if total_sessions > 0:
            usage_rate = (url_count / total_sessions) * 100
            st.metric("ì„¸ì…˜ë‹¹ ì‚¬ìš©ë¥ ", f"{usage_rate:.1f}%")
        else:
            st.metric("ì„¸ì…˜ë‹¹ ì‚¬ìš©ë¥ ", "N/A")

def _render_chat_question_types(df_view: pd.DataFrame):
    """ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬"""
    chat_events = df_view[df_view["event_name"].isin([
        "chat_question", "glossary_answer", "chat_response", "news_search_from_chat", "news_url_added_from_chat"
    ])]
    
    if chat_events.empty:
        return
    
    st.markdown("#### ğŸ’¬ ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬")
    
    question_types = {
        "ìš©ì–´ ì§ˆë¬¸": len(df_view[df_view["event_name"] == "glossary_answer"]),
        "ì¼ë°˜ ì§ˆë¬¸": len(df_view[df_view["event_name"] == "chat_response"]),
        "ë‰´ìŠ¤ ê²€ìƒ‰": len(df_view[df_view["event_name"] == "news_search_from_chat"]),
        "URL ì…ë ¥": len(df_view[df_view["event_name"] == "news_url_added_from_chat"]),
    }
    
    question_types = {k: v for k, v in question_types.items() if v > 0}
    
    if question_types:
        type_df = pd.DataFrame(list(question_types.items()), columns=["íƒ€ì…", "ê±´ìˆ˜"])
        
        if px is not None:
            fig = px.pie(
                type_df,
                values="ê±´ìˆ˜",
                names="íƒ€ì…",
                title="ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(type_df, use_container_width=True)

def _render_returning_sessions(df_view: pd.DataFrame, session_column: str):
    """ì¬ë°©ë¬¸ ì„¸ì…˜ ìˆ˜"""
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### ğŸ”„ ì¬ë°©ë¬¸ ì„¸ì…˜ ë¶„ì„")
    
    user_sessions = df_view.groupby("user_id")[session_column].nunique().reset_index()
    user_sessions.columns = ["user_id", "ì„¸ì…˜ ìˆ˜"]
    
    total_users = len(user_sessions)
    returning_users = len(user_sessions[user_sessions["ì„¸ì…˜ ìˆ˜"] > 1])
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì´ ì‚¬ìš©ì", total_users)
    with col2:
        st.metric("ì¬ë°©ë¬¸ ì‚¬ìš©ì", returning_users)
        if total_users > 0:
            return_rate = (returning_users / total_users) * 100
            st.caption(f"ì¬ë°©ë¬¸ë¥ : {return_rate:.1f}%")
    
    if px is not None and len(user_sessions) > 0:
        fig = px.histogram(
            user_sessions,
            x="ì„¸ì…˜ ìˆ˜",
            nbins=20,
            title="ì‚¬ìš©ìë³„ ì„¸ì…˜ ìˆ˜ ë¶„í¬"
        )
        st.plotly_chart(fig, use_container_width=True)
