"""
ì„œë²„ ì¤‘ì‹¬ ë¡œê·¸ ë·°ì–´
ì„œë²„ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ í‘œì‹œí•©ë‹ˆë‹¤.
event_log ì¤‘ì‹¬ ëª¨ë“œì—ì„œëŠ” Supabaseì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

from core.config import API_BASE_URL, API_ENABLE, SUPABASE_ENABLE
from core.logger import _get_user_id, _get_backend_session_id, _ensure_backend_session, get_supabase_client
import streamlit as st
import pandas as pd
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import json
import importlib
import re
import os

# wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # GUI ë°±ì—”ë“œ ì—†ì´ ì‚¬ìš©
    WORDCLOUD_AVAILABLE = True
except ImportError:
    WORDCLOUD_AVAILABLE = False
    if SUPABASE_ENABLE:
        try:
            st.warning("âš ï¸ wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install wordcloud matplotlibë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        except:
            pass

px = None
try:
    if importlib.util.find_spec("plotly.express"):
        px = importlib.import_module("plotly.express")
        try:
            import plotly.graph_objects as go
            from plotly.subplots import make_subplots
        except Exception:
            go = None
            make_subplots = None
    else:
        go = None
        make_subplots = None
except Exception:
    px = None
    go = None
    make_subplots = None

# requests ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# ============================================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def _parse_payload(payload: Any) -> Dict[str, Any]:
    """payloadë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    if isinstance(payload, dict):
        return payload
    if isinstance(payload, str):
        try:
            return json.loads(payload) if payload else {}
        except:
            return {}
    return {}

def _extract_from_payload(payload: Any, key: str, default=None):
    """payloadì—ì„œ íŠ¹ì • í‚¤ ê°’ì„ ì¶”ì¶œ"""
    parsed = _parse_payload(payload)
    return parsed.get(key, default)

def _extract_perf_data(row: pd.Series) -> Optional[Dict[str, Any]]:
    """payloadì—ì„œ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ (event_name ê¸°ë°˜)"""
    try:
        payload_raw = row.get("payload")
        event_name = row.get("event_name")
        
        if not payload_raw:
            return None
        
        payload = _parse_payload(payload_raw)
        if not payload:
            return None
        
        # event_nameì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
        if event_name == "news_detail_open":
            perf_steps = payload.get("perf_steps", {})
            if isinstance(perf_steps, dict) and "highlight_ms" in perf_steps:
                return {
                    "highlight_ms": perf_steps.get("highlight_ms"),
                    "terms_filter_ms": perf_steps.get("terms_filter_ms"),
                    "total_ms": perf_steps.get("total_ms"),
                    "terms_count": perf_steps.get("terms_count"),
                    "content_length": perf_steps.get("content_length"),
                    "cache_hit": payload.get("cache_hit", False),
                    "highlight_cache_hit": payload.get("highlight_cache_hit", False),
                    "terms_cache_hit": payload.get("terms_cache_hit", False),
                }
        
        elif event_name == "news_click":
            click_process_ms = payload.get("click_process_ms")
            if click_process_ms is not None:
                return {
                    "click_process_ms": click_process_ms,
                    "content_length": payload.get("content_length"),
                }
        
        elif event_name in ("glossary_click", "glossary_answer"):
            perf_steps = payload.get("perf_steps", {})
            if isinstance(perf_steps, dict) and "explanation_ms" in perf_steps:
                return {
                    "explanation_ms": perf_steps.get("explanation_ms"),
                    "total_ms": perf_steps.get("total_ms"),
                    "answer_length": perf_steps.get("answer_length"),
                }
        
        # RAG ì‘ë‹µ ì‹œê°„ (latency_ms ì§ì ‘ ì‚¬ìš©)
        if event_name in ("chat_response", "glossary_answer"):
            latency_ms = payload.get("latency_ms") or row.get("latency_ms")
            if latency_ms is not None:
                return {
                    "latency_ms": latency_ms,
                    "answer_length": payload.get("answer_len") or payload.get("answer_length"),
                }
        
        return None
    except:
        return None

def _get_news_id_from_row(row: pd.Series) -> Optional[str]:
    """rowì—ì„œ news_id ì¶”ì¶œ (ì—¬ëŸ¬ ì†ŒìŠ¤ í™•ì¸)"""
    news_id = row.get("news_id")
    if news_id and not pd.isna(news_id):
        return str(news_id)
    
    # payloadì—ì„œ ì¶”ì¶œ ì‹œë„
    payload = _parse_payload(row.get("payload"))
    if payload:
        news_id = payload.get("news_id") or payload.get("article_id")
        if news_id:
            return str(news_id)
    
    # ref_id í™•ì¸
    ref_id = row.get("ref_id")
    if ref_id and not pd.isna(ref_id):
        return str(ref_id)
    
    return None

def _format_news_id_display(news_id: Optional[str]) -> str:
    """news_idë¥¼ í‘œì‹œìš©ìœ¼ë¡œ í¬ë§·íŒ… (ìŒìˆ˜ë©´ ì„ì‹œ ë‰´ìŠ¤ë¡œ í‘œì‹œ)"""
    if not news_id:
        return "N/A"
    
    try:
        news_id_num = float(news_id)
        if news_id_num < 0:
            return f"ì„ì‹œ ë‰´ìŠ¤ ({news_id})"
        else:
            return str(int(news_id_num))
    except (ValueError, TypeError):
        return str(news_id)

def _get_term_from_row(row: pd.Series) -> Optional[str]:
    """rowì—ì„œ term ì¶”ì¶œ (ì—¬ëŸ¬ ì†ŒìŠ¤ í™•ì¸)"""
    term = row.get("term")
    if term and not pd.isna(term) and term != "":
        return str(term)
    
    # payloadì—ì„œ ì¶”ì¶œ
    payload = _parse_payload(row.get("payload"))
    if payload:
        term = payload.get("term")
        if term:
            return str(term)
    
    # term_from_payload ì»¬ëŸ¼ í™•ì¸
    term_from_payload = row.get("term_from_payload")
    if term_from_payload and not pd.isna(term_from_payload):
        return str(term_from_payload)
    
    return None

# ============================================================================
# ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ë“¤
# ============================================================================

def _fetch_news_from_supabase(limit: int = 1000) -> pd.DataFrame:
    """
    Supabaseì—ì„œ news í…Œì´ë¸” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    
    ì •ë ¬ ê¸°ì¤€ (ìš°ì„ ìˆœìœ„ ìˆœ):
    1. published_at ìµœì‹ ìˆœ (ê°€ì¥ ì¤‘ìš”)
    2. impact_score ë†’ì€ ìˆœ (ë‘ ë²ˆì§¸)
    3. credibility_score ë†’ì€ ìˆœ (ì„¸ ë²ˆì§¸)
    4. urgency_score ë†’ì€ ìˆœ (ë„¤ ë²ˆì§¸)
    
    í•„í„°:
    - deleted_atì´ NULLì¸ ë‰´ìŠ¤ë§Œ (ì‚­ì œë˜ì§€ ì•Šì€ ë‰´ìŠ¤)
    """
    if not SUPABASE_ENABLE:
        return pd.DataFrame()
    
    supabase = get_supabase_client()
    if not supabase:
        return pd.DataFrame()
    
    try:
        # deleted_atì´ NULLì¸ ë‰´ìŠ¤ë§Œ ê°€ì ¸ì˜¤ê¸° (ì‚­ì œë˜ì§€ ì•Šì€ ë‰´ìŠ¤)
        # Supabaseì—ì„œëŠ” ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜¨ í›„, Pythonì—ì„œ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬
        query = (
            supabase.table("news")
            .select("*")
            .is_("deleted_at", "null")
            .order("published_at", desc=True)  # ë¨¼ì € ìµœì‹ ìˆœìœ¼ë¡œ ê°€ì ¸ì˜¤ê¸°
            .limit(limit * 3)  # ë” ë§ì´ ê°€ì ¸ì˜¨ í›„ ì •ë ¬ (ë†’ì€ ì ìˆ˜ì˜ ìµœì‹  ë‰´ìŠ¤ í™•ë³´)
        )
        
        response = query.execute()
        
        if response.data:
            df = pd.DataFrame(response.data)
            
            # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
            date_columns = ["published_at", "created_at", "updated_at", "deleted_at"]
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], errors="coerce")
            
            # ì •ë ¬ ê¸°ì¤€: published_at > impact_score > credibility_score > urgency_score
            # ì ìˆ˜ê°€ NULLì¸ ê²½ìš° -1ë¡œ ë³€í™˜í•˜ì—¬ ë‚®ì€ ìš°ì„ ìˆœìœ„ë¡œ ì²˜ë¦¬
            sort_columns = []
            ascending_list = []
            
            # 1ìˆœìœ„: published_at ìµœì‹ ìˆœ
            if "published_at" in df.columns:
                sort_columns.append("published_at")
                ascending_list.append(False)
            
            # 2ìˆœìœ„: impact_score ë†’ì€ ìˆœ
            if "impact_score" in df.columns:
                df["impact_score_sorted"] = df["impact_score"].fillna(-1)
                sort_columns.append("impact_score_sorted")
                ascending_list.append(False)
            
            # 3ìˆœìœ„: credibility_score ë†’ì€ ìˆœ
            if "credibility_score" in df.columns:
                df["credibility_score_sorted"] = df["credibility_score"].fillna(-1)
                sort_columns.append("credibility_score_sorted")
                ascending_list.append(False)
            
            # 4ìˆœìœ„: urgency_score ë†’ì€ ìˆœ
            if "urgency_score" in df.columns:
                df["urgency_score_sorted"] = df["urgency_score"].fillna(-1)
                sort_columns.append("urgency_score_sorted")
                ascending_list.append(False)
            
            # ì •ë ¬ ì‹¤í–‰
            if sort_columns:
                df = df.sort_values(sort_columns, ascending=ascending_list)
                # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
                temp_cols = [col for col in df.columns if col.endswith("_sorted")]
                df = df.drop(columns=temp_cols)
                # ìƒìœ„ limitê°œë§Œ ë°˜í™˜
                df = df.head(limit)
            else:
                # ì ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ published_at ê¸°ì¤€ìœ¼ë¡œë§Œ ì •ë ¬
                if "published_at" in df.columns:
                    df = df.sort_values("published_at", ascending=False).head(limit)
                else:
                    df = df.head(limit)
            
            return df
        return pd.DataFrame()
    except Exception as e:
        st.warning(f"âš ï¸ Supabaseì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def _fetch_event_logs_from_supabase(user_id: Optional[str] = None, limit: int = 1000) -> pd.DataFrame:
    """Supabaseì—ì„œ event_logs ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if not SUPABASE_ENABLE:
        return pd.DataFrame()
    
    supabase = get_supabase_client()
    if not supabase:
        return pd.DataFrame()
    
    try:
        query = supabase.table("event_logs").select("*")
        
        if user_id:
            query = query.eq("user_id", user_id)
        
        query = query.order("event_time", desc=True).limit(limit)
        
        response = query.execute()
        
        if response.data:
            df = pd.DataFrame(response.data)
            if "event_time" in df.columns:
                df["event_time"] = pd.to_datetime(df["event_time"], errors="coerce")
            if "payload" in df.columns:
                def _extract_from_payload(payload, key):
                    """payloadì—ì„œ íŠ¹ì • í‚¤ ê°’ì„ ì¶”ì¶œ"""
                    parsed = _parse_payload(payload)
                    return parsed.get(key)
                
                # term ì¶”ì¶œ
                df["term_from_payload"] = df["payload"].apply(lambda p: _extract_from_payload(p, "term"))
                
                # news_id ì¶”ì¶œ
                if "news_id" not in df.columns or df["news_id"].isna().all():
                    df["news_id_from_payload"] = df["payload"].apply(
                        lambda p: _extract_from_payload(p, "news_id") or _extract_from_payload(p, "article_id")
                    )
                    if "news_id" not in df.columns:
                        df["news_id"] = df["news_id_from_payload"]
                    else:
                        df["news_id"] = df["news_id"].fillna(df["news_id_from_payload"])
                
                # latency_ms ì¶”ì¶œ
                if "latency_ms" not in df.columns or df["latency_ms"].isna().all():
                    df["latency_ms_from_payload"] = df["payload"].apply(lambda p: _extract_from_payload(p, "latency_ms"))
                    if "latency_ms" not in df.columns:
                        df["latency_ms"] = df["latency_ms_from_payload"]
                    else:
                        df["latency_ms"] = df["latency_ms"].fillna(df["latency_ms_from_payload"])
            return df
        return pd.DataFrame()
    except Exception as e:
        st.warning(f"âš ï¸ Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def _to_kst(series):
    """UTC ì‹œê°„ì„ KSTë¡œ ë³€í™˜"""
    dt = pd.to_datetime(series, errors="coerce", utc=True)
    return dt.dt.tz_convert("Asia/Seoul")

def _fill_sessions_from_time(
    df: pd.DataFrame,
    *,
    threshold_minutes: int = 30,
    time_column: str = "event_time",
    user_column: str = "user_id",
) -> pd.DataFrame:
    """
    ì„¸ì…˜ ID ê³„ì‚°: ì´ë²¤íŠ¸ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì„¸ì…˜ êµ¬ë¶„
    
    ë¡œì§:
    1. user_idë³„ë¡œ ì´ë²¤íŠ¸ë¥¼ ì‹œê°„ìˆœ ì •ë ¬
    2. ì´ì „ ì´ë²¤íŠ¸ì™€ì˜ ì‹œê°„ ì°¨ì´ê°€ threshold_minutes(ê¸°ë³¸ 30ë¶„)ë¥¼ ì´ˆê³¼í•˜ë©´ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ êµ¬ë¶„
    3. ì²« ì´ë²¤íŠ¸ëŠ” í•­ìƒ ìƒˆ ì„¸ì…˜ ì‹œì‘
    
    Args:
        df: ì´ë²¤íŠ¸ ë¡œê·¸ DataFrame
        threshold_minutes: ì„¸ì…˜ êµ¬ë¶„ ê¸°ì¤€ ì‹œê°„(ë¶„) - ê¸°ë³¸ê°’ 30ë¶„
        time_column: ì‹œê°„ ì»¬ëŸ¼ëª…
        user_column: ì‚¬ìš©ì ID ì»¬ëŸ¼ëª…
    
    Returns:
        session_id_resolved ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    """event_time ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ IDë¥¼ ì¶”ì‚°í•©ë‹ˆë‹¤."""
    if df.empty or time_column not in df.columns:
        result = df.copy()
        if "session_id" in result.columns:
            result["session_id_resolved"] = result["session_id"]
        return result

    work = df.copy()
    work[time_column] = pd.to_datetime(work[time_column], errors="coerce")

    has_user_column = user_column in work.columns
    if has_user_column:
        session_users = work[user_column].fillna("anonymous").astype(str)
        session_users = session_users.where(session_users.str.len() > 0, "anonymous")
    else:
        session_users = pd.Series(["anonymous"] * len(work), index=work.index)

    threshold = pd.Timedelta(minutes=threshold_minutes)
    order = work.index.to_series(name="_session_order")
    work = work.assign(_session_user=session_users, _session_order=order)
    work = work.sort_values(["_session_user", time_column, "_session_order"])

    gaps = work.groupby("_session_user")[time_column].diff()
    new_session_flags = gaps.isna() | (gaps > threshold) | work[time_column].isna()
    session_sequence = new_session_flags.astype(int).groupby(work["_session_user"]).cumsum()

    inferred_ids = work["_session_user"].astype(str) + "-" + session_sequence.astype(str)
    work["session_id_inferred"] = inferred_ids

    resolved = work.sort_values("_session_order")["session_id_inferred"]
    result = df.copy()
    result["session_id_inferred"] = resolved

    if "session_id" in result.columns:
        session_series = result["session_id"]
        session_series = session_series.where(session_series.notna(), None)
        if session_series.dtype != object:
            session_series = session_series.astype("object")
        missing_mask = session_series.isna() | (session_series.astype(str).str.len() == 0)
        session_series = session_series.astype("object")
        session_series.loc[missing_mask] = result.loc[missing_mask, "session_id_inferred"]
        result["session_id"] = session_series
    else:
        result["session_id"] = result["session_id_inferred"]

    result["session_id_resolved"] = result["session_id"]
    return result

# ============================================================================
# ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜
# ============================================================================

def render(show_mode: str = "dashboard"):
    """
    ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜: ì„œë²„ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ëŒ€ì‹œë³´ë“œ/ë¡œê·¸ ë·°ì–´ ë Œë”ë§
    
    ì „ì²´ íë¦„:
    1. Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 2000ê±´)
    2. UTC ì‹œê°„ì„ KSTë¡œ ë³€í™˜
    3. ì„¸ì…˜ ê³„ì‚° (30ë¶„ ê°„ê²©)
    4. show_modeì— ë”°ë¼ ëŒ€ì‹œë³´ë“œ ë˜ëŠ” ë¡œê·¸ ë·°ì–´ í‘œì‹œ
    
    Args:
        show_mode: "dashboard" (ëŒ€ì‹œë³´ë“œ) ë˜ëŠ” "log_viewer" (ë¡œê·¸ ë·°ì–´)
    """
    from core.logger import _get_user_id

    # Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
    with st.spinner("ğŸ”„ Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        # WAU ê³„ì‚°ì„ ìœ„í•´ ìµœê·¼ 7ì¼ ë°ì´í„°ë¥¼ ì¶©ë¶„íˆ ê°€ì ¸ì™€ì•¼ í•¨
        # limitì„ ëŠ˜ë ¤ì„œ ìµœê·¼ ë°ì´í„°ë¥¼ ë” ë§ì´ ê°€ì ¸ì˜¤ê¸°
        df = _fetch_event_logs_from_supabase(user_id=None, limit=5000)  # 2000 -> 5000ìœ¼ë¡œ ì¦ê°€

        if df.empty:
            st.info("ğŸ“­ ì•„ì§ ì´ë²¤íŠ¸ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì•±ì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ê°€ ìˆ˜ì§‘ë©ë‹ˆë‹¤.")
            return

        # ì‹œê°„ëŒ€ ë³€í™˜: UTC â†’ KST (í•œêµ­ í‘œì¤€ì‹œ)
        df["event_time"] = _to_kst(df["event_time"])
        df = df.sort_values("event_time")

        # ì„¸ì…˜ ê³„ì‚°: 30ë¶„ ê°„ê²©ìœ¼ë¡œ ì„¸ì…˜ êµ¬ë¶„ (ëª¨ë“  íƒ­ì—ì„œ ì‚¬ìš©)
        # user_idë³„ë¡œ ì´ë²¤íŠ¸ë¥¼ ì‹œê°„ìˆœ ì •ë ¬í•˜ê³ , 30ë¶„ ì´ìƒ ê°„ê²©ì´ ìˆìœ¼ë©´ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ êµ¬ë¶„
        session_gap_minutes = 30
        df = _fill_sessions_from_time(df, threshold_minutes=session_gap_minutes)
        session_column = "session_id_resolved" if "session_id_resolved" in df.columns else "session_id"

        # show_modeì— ë”°ë¼ ë‹¤ë¥¸ í˜ì´ì§€ í‘œì‹œ
        if show_mode == "dashboard":
            st.markdown("## ğŸ“Š ëŒ€ì‹œë³´ë“œ")
            
            # ìƒìœ„ ë ˆë²¨ íƒ­: 4ê°œ ì¹´í…Œê³ ë¦¬
            tab1, tab2, tab3, tab4 = st.tabs([
                "ğŸ“Š KPI Dashboard",      # í•µì‹¬ ì§€í‘œ ìš”ì•½ (DAU, WAU, ì„¸ì…˜ ê¸¸ì´ ë“±)
                "ğŸ”´ Service Health",     # ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë° ì•ˆì •ì„±
                "ğŸŸ¡ Content Quality",     # ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ
                "ğŸŸ¢ User Behavior"       # ì‚¬ìš©ì í–‰ë™ ë¶„ì„
            ])
            
            # íƒ­ 1: KPI Dashboard - í•µì‹¬ ì§€í‘œ ìš”ì•½
            with tab1:
                _render_kpi_dashboard(df, session_column)
            
            # íƒ­ 2: Service Health - ì„±ëŠ¥ ë©”íŠ¸ë¦­, RAG ì‘ë‹µ ì‹œê°„, URL íŒŒì‹± ë“±
            with tab2:
                _render_service_health_tab(df, session_column)
            
            # íƒ­ 3: Content Quality - ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„, ë³¸ë¬¸ í’ˆì§ˆ, ì›Œë“œí´ë¼ìš°ë“œ ë“±
            with tab3:
                _render_content_quality_tab(df)
            
            # íƒ­ 4: User Behavior - í´ë¦­ë¥ , ì½ê¸° ì‹œê°„, ìš©ì–´ í´ë¦­ë¥  ë“±
            with tab4:
                _render_user_behavior_tab(df, session_column)
        
        elif show_mode == "log_viewer":
            st.markdown("## ğŸ“ ë¡œê·¸ ë·°ì–´")
            # ë¡œê·¸ ë·°ì–´: ê°œë³„ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ í•„í„°ë§í•˜ì—¬ ìƒì„¸ í™•ì¸
            _render_log_viewer_tab(df, session_column)

# ============================================================================
# íƒ­ 1: ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (Service Health)
# ============================================================================

def _render_service_health_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° íƒ­: "MVPê°€ ë©ˆì¶”ì§€ ì•Šê³  ë²„í‹¸ ìˆ˜ ìˆëŠ”ê°€?"
    
    ì£¼ìš” ë¶„ì„ í•­ëª©:
    - ë‰´ìŠ¤ í´ë¦­/ìƒì„¸ ì§„ì… ë©”íŠ¸ë¦­
    - ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ë¡œë”© ì‹œê°„ (í•˜ì´ë¼ì´íŠ¸, ìš©ì–´ í•„í„°ë§)
    - RAG ì‘ë‹µ ì‹œê°„ (glossary_answer, chat_response)
    - ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„
    - URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨ìœ¨
    - Streamlit ì„¸ì…˜ ìˆ˜ ë° ë™ì‹œ ì ‘ì† ë¶€í•˜
    """
    st.markdown("### ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (Service Health)")
    st.markdown("**ëª©í‘œ**: ì„œë¹„ìŠ¤ì˜ ê¸°ìˆ ì  ì•ˆì •ì„± ì¸¡ì • - ëª¨ë“  ë¶„ì„ì˜ ê¸°ë°˜")
    
    # ì£¼ìš” ì„±ëŠ¥ ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        news_clicks = int((df_view["event_name"] == "news_click").sum())
        st.metric("ë‰´ìŠ¤ í´ë¦­", news_clicks)
    with col2:
        detail_opens = int((df_view["event_name"] == "news_detail_open").sum())
        st.metric("ìƒì„¸ ì§„ì…", detail_opens)
    with col3:
        chat_questions = int((df_view["event_name"] == "chat_question").sum())
        st.metric("ì±— ì§ˆë¬¸", chat_questions)
    with col4:
        url_errors = int((df_view["event_name"] == "news_url_add_error").sum())
        st.metric("URL íŒŒì‹± ì‹¤íŒ¨", url_errors)
    
    # ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ
    perf_events = df_view[df_view["event_name"].isin([
        "news_click", "news_detail_open", "glossary_click", "glossary_answer",
        "chat_response", "news_search_from_chat"
    ])].copy()
    
    if not perf_events.empty:
        perf_events["perf_data"] = perf_events.apply(_extract_perf_data, axis=1)
        perf_events_with_data = perf_events[perf_events["perf_data"].notna()]
        
        # 1. ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ë¡œë”© ì‹œê°„
        _render_detail_performance(perf_events_with_data)
        
        # 2. RAG ì‘ë‹µ ì‹œê°„
        _render_rag_performance(perf_events_with_data)
        
        # 3. ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„
        _render_search_performance(df_view)
        
        # 4. í•˜ì´ë¼ì´íŠ¸ ê³„ì‚° ì‹œê°„ (detail ì„±ëŠ¥ì— í¬í•¨)
        
        # 5. URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨
        _render_url_parsing_quality(df_view)
        
        # 6. Streamlit ì„¸ì…˜ ìˆ˜ / ë™ì‹œ ì ‘ì† ë¶€í•˜
        _render_session_load(df_view, session_column)
    else:
        st.info("ğŸ“Š ì„±ëŠ¥ ë°ì´í„°ê°€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.")
    
    # ìµœê·¼ ì´ë²¤íŠ¸ ë¡œê·¸
    st.markdown("### ğŸ“„ ìµœê·¼ ì„±ëŠ¥ ì´ë²¤íŠ¸")
    perf_recent = df_view[df_view["event_name"].isin([
        "news_click", "news_detail_open", "glossary_click", "glossary_answer",
        "chat_question", "chat_response", "news_url_add_error"
    ])].sort_values("event_time", ascending=False).head(50)
    
    display_cols = ["event_time", "event_name", "user_id", "session_id", "latency_ms"]
    available_cols = [col for col in display_cols if col in perf_recent.columns]
    if available_cols:
        st.dataframe(perf_recent[available_cols], use_container_width=True, height=300)

def _render_detail_performance(perf_events_with_data: pd.DataFrame):
    """ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ì„±ëŠ¥ ë¶„ì„"""
    detail_events = perf_events_with_data[perf_events_with_data["event_name"] == "news_detail_open"]
    
    if detail_events.empty:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ë¡œë”© ì‹œê°„")
    
    perf_data_list = []
    for idx, row in detail_events.iterrows():
        perf = row["perf_data"]
        if perf and isinstance(perf, dict):
            news_id = _get_news_id_from_row(row)
            highlight_ms = perf.get("highlight_ms", 0)
            terms_filter_ms = perf.get("terms_filter_ms", 0)
            total_ms = perf.get("total_ms") or (highlight_ms + terms_filter_ms)
            
            cache_status = []
            if perf.get("highlight_cache_hit"):
                cache_status.append("í•˜ì´ë¼ì´íŠ¸âœ…")
            if perf.get("terms_cache_hit"):
                cache_status.append("ìš©ì–´âœ…")
            if not cache_status:
                cache_status.append("âŒ")
            
            perf_data_list.append({
                "event_time": row.get("event_time"),
                "news_id": _format_news_id_display(news_id),
                "í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)": highlight_ms,
                "ìš©ì–´ í•„í„°ë§ (ms)": terms_filter_ms,
                "ì „ì²´ ë Œë”ë§ (ms)": total_ms,
                "ë°œê²¬ëœ ìš©ì–´ ìˆ˜": perf.get("terms_count", 0),
                "ê¸°ì‚¬ ê¸¸ì´ (ì)": perf.get("content_length", 0),
                "ìºì‹œ íˆíŠ¸": " / ".join(cache_status),
            })
    
    if perf_data_list:
        perf_df = pd.DataFrame(perf_data_list)
        perf_df = perf_df.sort_values("event_time", ascending=False)
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### ğŸ“Š ì„±ëŠ¥ í†µê³„")
            avg_highlight = perf_df["í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)"].mean()
            avg_filter = perf_df["ìš©ì–´ í•„í„°ë§ (ms)"].mean()
            avg_total = perf_df["ì „ì²´ ë Œë”ë§ (ms)"].mean()
            cache_hit_rate = (perf_df["ìºì‹œ íˆíŠ¸"].str.contains("âœ…", na=False).sum() / len(perf_df) * 100) if len(perf_df) > 0 else 0
            
            st.metric("í‰ê·  í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬", f"{avg_highlight:.0f}ms")
            st.metric("í‰ê·  ìš©ì–´ í•„í„°ë§", f"{avg_filter:.0f}ms")
            st.metric("í‰ê·  ì „ì²´ ë Œë”ë§", f"{avg_total:.0f}ms")
            st.metric("ìºì‹œ íˆíŠ¸ìœ¨", f"{cache_hit_rate:.1f}%")
        
        with col2:
            st.markdown("##### ğŸ” ë³‘ëª© ì§€ì  ë¶„ì„")
            if len(perf_df) > 0:
                highlight_pct = (perf_df["í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)"] / perf_df["ì „ì²´ ë Œë”ë§ (ms)"] * 100).mean()
                filter_pct = (perf_df["ìš©ì–´ í•„í„°ë§ (ms)"] / perf_df["ì „ì²´ ë Œë”ë§ (ms)"] * 100).mean()
                
                st.write(f"**í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ ë¹„ìœ¨**: {highlight_pct:.1f}%")
                st.write(f"**ìš©ì–´ í•„í„°ë§ ë¹„ìœ¨**: {filter_pct:.1f}%")
                
                if highlight_pct > 50:
                    st.warning("âš ï¸ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ê°€ ì£¼ìš” ë³‘ëª©ì…ë‹ˆë‹¤!")
                elif filter_pct > 30:
                    st.warning("âš ï¸ ìš©ì–´ í•„í„°ë§ì´ ë³‘ëª©ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                else:
                    st.success("âœ… ì„±ëŠ¥ì´ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        
        # ì‹œê°í™”
        if px is not None and len(perf_df) > 0:
            fig = px.scatter(
                perf_df.head(100),
                x="í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)",
                y="ìš©ì–´ í•„í„°ë§ (ms)",
                size="ì „ì²´ ë Œë”ë§ (ms)",
                color="ìºì‹œ íˆíŠ¸",
                hover_data=["news_id", "ë°œê²¬ëœ ìš©ì–´ ìˆ˜", "ê¸°ì‚¬ ê¸¸ì´ (ì)"],
                title="ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ì„±ëŠ¥ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(perf_df.head(20), use_container_width=True, height=300)

def _render_rag_performance(perf_events_with_data: pd.DataFrame):
    """RAG ì‘ë‹µ ì‹œê°„ ë¶„ì„"""
    rag_events = perf_events_with_data[perf_events_with_data["event_name"].isin(["glossary_answer", "chat_response"])]
    
    if rag_events.empty:
        return
    
    st.markdown("#### ğŸ¤– RAG ì‘ë‹µ ì‹œê°„")
    
    rag_data_list = []
    for idx, row in rag_events.iterrows():
        perf = row["perf_data"]
        if perf and isinstance(perf, dict):
            term = _get_term_from_row(row)
            latency_ms = perf.get("latency_ms") or perf.get("total_ms") or perf.get("explanation_ms")
            answer_length = perf.get("answer_length") or perf.get("answer_len", 0)
            
            if latency_ms is not None:
                rag_data_list.append({
                    "event_time": row.get("event_time"),
                    "event_name": row.get("event_name"),
                    "term": term or "",
                    "ì‘ë‹µ ì‹œê°„ (ms)": latency_ms,
                    "ë‹µë³€ ê¸¸ì´ (ì)": answer_length,
                })
    
    if rag_data_list:
        rag_df = pd.DataFrame(rag_data_list)
        rag_df = rag_df.sort_values("event_time", ascending=False)
        
        col1, col2 = st.columns(2)
        with col1:
            avg_latency = rag_df["ì‘ë‹µ ì‹œê°„ (ms)"].mean()
            st.metric("í‰ê·  RAG ì‘ë‹µ ì‹œê°„", f"{avg_latency:.0f}ms")
        with col2:
            avg_length = rag_df["ë‹µë³€ ê¸¸ì´ (ì)"].mean()
            st.metric("í‰ê·  ë‹µë³€ ê¸¸ì´", f"{avg_length:.0f}ì")
        
        # ì‹œê°í™”
        if px is not None and len(rag_df) > 0:
            fig = px.histogram(
                rag_df,
                x="ì‘ë‹µ ì‹œê°„ (ms)",
                nbins=30,
                title="RAG ì‘ë‹µ ì‹œê°„ ë¶„í¬",
                labels={"ì‘ë‹µ ì‹œê°„ (ms)": "ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)", "count": "ë¹ˆë„"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(rag_df.head(20), use_container_width=True, height=300)

def _render_search_performance(df_view: pd.DataFrame):
    """ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„"""
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"]
    
    if search_events.empty:
        return
    
    st.markdown("#### ğŸ” ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„")
    
    search_data_list = []
    for idx, row in search_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        latency_ms = payload.get("latency_ms") or row.get("latency_ms")
        message = payload.get("message") or row.get("message", "")
        
        # latency_msë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê³  ìœ íš¨í•œ ê°’ë§Œ ì‚¬ìš©
        try:
            if latency_ms is not None:
                latency_ms = pd.to_numeric(latency_ms, errors='coerce')
                if pd.notna(latency_ms) and latency_ms > 0:
                    search_data_list.append({
                        "event_time": row.get("event_time"),
                        "ê²€ìƒ‰ì–´": message[:50] if message else "",
                        "ì²˜ë¦¬ ì‹œê°„ (ms)": float(latency_ms),
                    })
        except (ValueError, TypeError):
            continue
    
    if search_data_list:
        search_df = pd.DataFrame(search_data_list)
        search_df = search_df.sort_values("event_time", ascending=False)
        
        # ìœ íš¨í•œ ê°’ë§Œìœ¼ë¡œ í‰ê·  ê³„ì‚°
        valid_latencies = search_df["ì²˜ë¦¬ ì‹œê°„ (ms)"].dropna()
        if len(valid_latencies) > 0:
            avg_latency = valid_latencies.mean()
            st.metric("í‰ê·  ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„", f"{avg_latency:.0f}ms")
        else:
            st.metric("í‰ê·  ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„", "N/A")
        
        if px is not None and len(search_df) > 0 and len(valid_latencies) > 0:
            fig = px.box(
                search_df,
                y="ì²˜ë¦¬ ì‹œê°„ (ms)",
                title="ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(search_df.head(20), use_container_width=True, height=200)
    else:
        st.info("ğŸ“Š ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _render_url_parsing_quality(df_view: pd.DataFrame):
    """URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨ ì´ë²¤íŠ¸"""
    url_events = df_view[df_view["event_name"].isin(["news_url_added_from_chat", "news_url_add_error"])]
    
    if url_events.empty:
        return
    
    st.markdown("#### ğŸ”— URL íŒŒì‹± í’ˆì§ˆ")
    
    success_count = int((url_events["event_name"] == "news_url_added_from_chat").sum())
    error_count = int((url_events["event_name"] == "news_url_add_error").sum())
    total_count = success_count + error_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("URL íŒŒì‹± ì„±ê³µ", success_count)
    with col2:
        st.metric("URL íŒŒì‹± ì‹¤íŒ¨", error_count)
    with col3:
        if total_count > 0:
            success_rate = (success_count / total_count) * 100
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        else:
            st.metric("ì„±ê³µë¥ ", "N/A")
    
    if total_count > 0 and px is not None:
        fig = px.pie(
            values=[success_count, error_count],
            names=["ì„±ê³µ", "ì‹¤íŒ¨"],
            title="URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_session_load(df_view: pd.DataFrame, session_column: str):
    """Streamlit ì„¸ì…˜ ìˆ˜ / ë™ì‹œ ì ‘ì† ë¶€í•˜"""
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### ğŸ‘¥ ì„¸ì…˜ ë¶€í•˜ ë¶„ì„")
    
    # ì‹œê°„ëŒ€ë³„ ì„¸ì…˜ ìˆ˜
    df_view_copy = df_view.copy()
    df_view_copy["hour"] = df_view_copy["event_time"].dt.floor("H")
    hourly_sessions = df_view_copy.groupby("hour")[session_column].nunique().reset_index()
    hourly_sessions.columns = ["ì‹œê°„", "ì„¸ì…˜ ìˆ˜"]
    
    col1, col2 = st.columns(2)
    with col1:
        max_sessions = hourly_sessions["ì„¸ì…˜ ìˆ˜"].max() if len(hourly_sessions) > 0 else 0
        st.metric("ìµœëŒ€ ë™ì‹œ ì„¸ì…˜ ìˆ˜", f"{max_sessions}ê°œ")
    with col2:
        avg_sessions = hourly_sessions["ì„¸ì…˜ ìˆ˜"].mean() if len(hourly_sessions) > 0 else 0
        st.metric("í‰ê·  ì„¸ì…˜ ìˆ˜", f"{avg_sessions:.1f}ê°œ")
    
    if px is not None and len(hourly_sessions) > 0:
        fig = px.line(
            hourly_sessions,
            x="ì‹œê°„",
            y="ì„¸ì…˜ ìˆ˜",
            title="ì‹œê°„ëŒ€ë³„ ì„¸ì…˜ ìˆ˜ ì¶”ì´"
        )
        st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# íƒ­ 2: ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (Content Quality)
# ============================================================================

def _render_content_quality_tab(df_view: pd.DataFrame):
    """
    ğŸŸ¡ ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° íƒ­: "ìš°ë¦¬ ì œí’ˆì´ ì œê³µí•˜ëŠ” ë‰´ìŠ¤ ë°ì´í„° ìì²´ê°€ ì¢‹ì€ê°€?"
    
    ì£¼ìš” ë¶„ì„ í•­ëª©:
    - ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„ (DB ë‰´ìŠ¤ vs ì„ì‹œ ë‰´ìŠ¤)
    - ë‰´ìŠ¤ ì¶œì²˜(ì–¸ë¡ ì‚¬) ë¶„í¬
    - ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘
    - ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬ ë° ëˆ„ë½ ë¹„ìœ¨
    - ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µë¥ 
    - ì„íŒ©íŠ¸ ì ìˆ˜ ë¶„í¬
    - ì¤‘ë³µ ê¸°ì‚¬ ë¹„ìœ¨
    - ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸
    - ì›Œë“œí´ë¼ìš°ë“œ (íŠ¹ì • ë‚ ì§œ)
    - ê¸°ì´ˆ ë‰´ìŠ¤ ì§€í‘œ ë¶„ì„ + ë¼ì´ë‹¤ ì°¨íŠ¸
    - í”„ë¡¬í”„íŠ¸ íŠœë‹ìš© ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë¹„êµ
    - ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„
    - URL íŒŒì‹± í’ˆì§ˆ
    """
    st.markdown("### ğŸŸ¡ ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (Content Quality)")
    st.markdown("**ëª©í‘œ**: ë‰´ìŠ¤ ì½˜í…ì¸ ì˜ í’ˆì§ˆ ì¸¡ì • - ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ìì‚°")
    
    # ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„ (DB ë‰´ìŠ¤ vs ì„ì‹œ ë‰´ìŠ¤) - ì´ë²¤íŠ¸ ë¡œê·¸ ê¸°ë°˜
    _render_news_source_analysis(df_view)
    
    # URL íŒŒì‹± í’ˆì§ˆ (ì´ë²¤íŠ¸ ë¡œê·¸ ê¸°ë°˜)
    _render_url_parsing_quality_for_content(df_view)
    
    # ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„ (ì´ë²¤íŠ¸ ë¡œê·¸ ê¸°ë°˜)
    _render_search_result_news_popularity(df_view)
    
    # Supabase news í…Œì´ë¸” ì—°ë™ ë¶„ì„
    with st.spinner("ğŸ”„ Supabaseì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        news_df = _fetch_news_from_supabase(limit=5000)
        
        if news_df.empty:
            st.warning("âš ï¸ Supabase `news` í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ Supabase ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success(f"âœ… {len(news_df):,}ê°œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            
            # ì£¼ìš” ë¶„ì„ í•­ëª©ë“¤
            _render_news_source_distribution(news_df)
            _render_financial_news_ratio(news_df)
            _render_content_length_analysis(news_df)
            _render_content_missing_analysis(news_df)
            _render_title_content_duplication(news_df)
            _render_impact_score_distribution(news_df)
            _render_duplicate_news_analysis(news_df)
            _render_news_collection_trends(news_df)
            
            # ì›Œë“œí´ë¼ìš°ë“œ (ì‚¬ìš©ì ì„¤ì • ê°€ëŠ¥)
            st.markdown("---")
            _render_wordcloud(news_df)
            
            # ê¸°ì´ˆ ë‰´ìŠ¤ ì§€í‘œ ë¶„ì„ + ë¼ì´ë‹¤ ì°¨íŠ¸
            st.markdown("---")
            _render_news_radar_analysis(news_df)
            
            # í”„ë¡¬í”„íŠ¸ íŠœë‹ìš© ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë¹„êµ
            st.markdown("---")
            _render_prompt_tuning_comparison(news_df)

def _render_news_source_analysis(df_view: pd.DataFrame):
    """ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„ (DB ë‰´ìŠ¤ vs ì„ì‹œ ë‰´ìŠ¤)"""
    # news_idê°€ ìˆëŠ” ì´ë²¤íŠ¸ í•„í„°ë§
    news_events = df_view[df_view["news_id"].notna()].copy()
    
    if news_events.empty:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„")
    
    # news_idë¥¼ ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
    def is_temp_news(news_id):
        """news_idê°€ ì„ì‹œ ë‰´ìŠ¤(ìŒìˆ˜)ì¸ì§€ í™•ì¸"""
        try:
            news_id_num = float(news_id)
            return news_id_num < 0
        except (ValueError, TypeError):
            return False
    
    news_events["is_temp"] = news_events["news_id"].apply(is_temp_news)
    
    db_news_count = (~news_events["is_temp"]).sum()
    temp_news_count = news_events["is_temp"].sum()
    total_count = len(news_events)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("DB ë‰´ìŠ¤", f"{db_news_count:,}ê±´")
    with col2:
        st.metric("ì„ì‹œ ë‰´ìŠ¤ (URL ì§ì ‘ ì…ë ¥)", f"{temp_news_count:,}ê±´")
    with col3:
        if total_count > 0:
            temp_ratio = (temp_news_count / total_count) * 100
            st.metric("ì„ì‹œ ë‰´ìŠ¤ ë¹„ìœ¨", f"{temp_ratio:.1f}%")
        else:
            st.metric("ì„ì‹œ ë‰´ìŠ¤ ë¹„ìœ¨", "N/A")
    
    if total_count > 0 and px is not None:
        source_df = pd.DataFrame({
            "ì†ŒìŠ¤": ["DB ë‰´ìŠ¤", "ì„ì‹œ ë‰´ìŠ¤"],
            "ê±´ìˆ˜": [db_news_count, temp_news_count]
        })
        fig = px.pie(
            source_df,
            values="ê±´ìˆ˜",
            names="ì†ŒìŠ¤",
            title="ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„í¬"
        )
        st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# Supabase news í…Œì´ë¸” ê¸°ë°˜ ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜ë“¤
# ============================================================================

def _render_news_source_distribution(news_df: pd.DataFrame):
    """ë‰´ìŠ¤ ì¶œì²˜(ì–¸ë¡ ì‚¬) ë¶„í¬"""
    if "source" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ì¶œì²˜(ì–¸ë¡ ì‚¬) ë¶„í¬")
    
    # sourceê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_source = news_df[news_df["source"].notna() & (news_df["source"] != "")]
    
    if news_with_source.empty:
        st.info("ğŸ“Š ì¶œì²˜ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    source_counts = news_with_source["source"].value_counts()
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ê³ ìœ  ì¶œì²˜ ìˆ˜", len(source_counts))
    with col2:
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", len(news_with_source))
    
    # Top 10 ì¶œì²˜
    top_sources = source_counts.head(10).reset_index()
    top_sources.columns = ["ì¶œì²˜", "ê±´ìˆ˜"]
    
    if px is not None and len(top_sources) > 0:
        fig = px.bar(
            top_sources,
            x="ì¶œì²˜",
            y="ê±´ìˆ˜",
            title="ë‰´ìŠ¤ ì¶œì²˜ Top 10",
            labels={"ì¶œì²˜": "ì–¸ë¡ ì‚¬", "ê±´ìˆ˜": "ê¸°ì‚¬ ìˆ˜"}
        )
        fig.update_xaxes(tickangle=-45)
        st.plotly_chart(fig, use_container_width=True)
    
    st.dataframe(top_sources, use_container_width=True, height=300)

def _render_financial_news_ratio(news_df: pd.DataFrame):
    """ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘"""
    # is_finance_news ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
    if "is_finance_news" in news_df.columns:
        news_with_flag = news_df[news_df["is_finance_news"].notna()].copy()
        
        if not news_with_flag.empty:
            financial_count = (news_with_flag["is_finance_news"] == True).sum()
            non_financial_count = (news_with_flag["is_finance_news"] == False).sum()
            total_count = len(news_with_flag)
            
            st.markdown("#### ğŸ’° ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘ (is_finance_news ì»¬ëŸ¼ ê¸°ì¤€)")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ê¸ˆìœµ ê¸°ì‚¬", f"{financial_count:,}ê±´")
                if total_count > 0:
                    financial_ratio = (financial_count / total_count) * 100
                    st.caption(f"ë¹„ìœ¨: {financial_ratio:.1f}%")
            with col2:
                st.metric("ë¹„ê¸ˆìœµ ê¸°ì‚¬", f"{non_financial_count:,}ê±´")
                if total_count > 0:
                    non_financial_ratio = (non_financial_count / total_count) * 100
                    st.caption(f"ë¹„ìœ¨: {non_financial_ratio:.1f}%")
            with col3:
                st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_count:,}ê±´")
            
            if total_count > 0 and px is not None:
                ratio_df = pd.DataFrame({
                    "ë¶„ë¥˜": ["ê¸ˆìœµ ê¸°ì‚¬", "ë¹„ê¸ˆìœµ ê¸°ì‚¬"],
                    "ê±´ìˆ˜": [financial_count, non_financial_count]
                })
                # Donut chart (hole íŒŒë¼ë¯¸í„° ì‚¬ìš©)
                fig = px.pie(
                    ratio_df,
                    values="ê±´ìˆ˜",
                    names="ë¶„ë¥˜",
                    title="ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘",
                    hole=0.4  # ë„ë„› ì°¨íŠ¸ë¡œ ë§Œë“¤ê¸°
                )
                st.plotly_chart(fig, use_container_width=True)
            return
    
    # is_finance_news ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
    if "title" not in news_df.columns and "content" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ’° ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘ (í‚¤ì›Œë“œ ê¸°ë°˜)")
    
    # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ (ì œëª©/ë³¸ë¬¸ì—ì„œ ê²€ìƒ‰)
    financial_keywords = [
        "ê¸ˆìœµ", "ì€í–‰", "ì¦ê¶Œ", "ì£¼ì‹", "ì±„ê¶Œ", "ë¶€ë™ì‚°", "ê²½ì œ", "ì‹œì¥", "íˆ¬ì", "ìì‚°",
        "ì´ì", "ê¸ˆë¦¬", "í™˜ìœ¨", "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "ê²½ê¸°", "ê²½ì œì„±ì¥", "GDP",
        "ê¸°ì—…", "ìƒì¥", "IPO", "ë°°ë‹¹", "ìˆ˜ìµ", "ì†ì‹¤", "ì¬ë¬´", "íšŒê³„", "ì„¸ê¸ˆ", "ì •ì±…",
        "í•œêµ­ì€í–‰", "ê¸ˆìœµê°ë…ì›", "ê¸ˆìœµìœ„ì›íšŒ", "ì¦ì„ ìœ„", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ë‚˜ìŠ¤ë‹¥",
        "ë¹„íŠ¸ì½”ì¸", "ì•”í˜¸í™”í", "ë¸”ë¡ì²´ì¸", "ê¸ˆìœµê¶Œ", "ê¸ˆìœµì‚¬", "ì€í–‰ê¶Œ"
    ]
    
    def is_financial(row):
        """ë‰´ìŠ¤ê°€ ê¸ˆìœµ ê´€ë ¨ì¸ì§€ íŒë‹¨"""
        title = str(row.get("title", "")).lower()
        content = str(row.get("content", "")).lower()
        
        text = title + " " + content[:500]  # ë³¸ë¬¸ ì•ë¶€ë¶„ 500ìë§Œ í™•ì¸
        
        for keyword in financial_keywords:
            if keyword in text:
                return True
        return False
    
    news_df_copy = news_df.copy()
    news_df_copy["is_financial"] = news_df_copy.apply(is_financial, axis=1)
    
    financial_count = news_df_copy["is_financial"].sum()
    non_financial_count = (~news_df_copy["is_financial"]).sum()
    total_count = len(news_df_copy)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê¸ˆìœµ ê¸°ì‚¬", f"{financial_count:,}ê±´")
        if total_count > 0:
            financial_ratio = (financial_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {financial_ratio:.1f}%")
    with col2:
        st.metric("ë¹„ê¸ˆìœµ ê¸°ì‚¬", f"{non_financial_count:,}ê±´")
        if total_count > 0:
            non_financial_ratio = (non_financial_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {non_financial_ratio:.1f}%")
    with col3:
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_count:,}ê±´")
    
    if total_count > 0 and px is not None:
        ratio_df = pd.DataFrame({
            "ë¶„ë¥˜": ["ê¸ˆìœµ ê¸°ì‚¬", "ë¹„ê¸ˆìœµ ê¸°ì‚¬"],
            "ê±´ìˆ˜": [financial_count, non_financial_count]
        })
        # Donut chart (hole íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        fig = px.pie(
            ratio_df,
            values="ê±´ìˆ˜",
            names="ë¶„ë¥˜",
            title="ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘",
            hole=0.4  # ë„ë„› ì°¨íŠ¸ë¡œ ë§Œë“¤ê¸°
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_content_length_analysis(news_df: pd.DataFrame):
    """ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬"""
    # content ë˜ëŠ” raw_content_length ì»¬ëŸ¼ í™•ì¸
    content_col = None
    if "raw_content_length" in news_df.columns:
        content_col = "raw_content_length"
    elif "content" in news_df.columns:
        content_col = "content"
    else:
        return
    
    st.markdown("#### ğŸ“ ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬")
    
    if content_col == "raw_content_length":
        # raw_content_lengthê°€ ìˆ«ì ì»¬ëŸ¼ì¸ ê²½ìš°
        news_with_content = news_df[news_df[content_col].notna()].copy()
        news_with_content["content_length"] = pd.to_numeric(news_with_content[content_col], errors='coerce')
    else:
        # content ì»¬ëŸ¼ì—ì„œ ê¸¸ì´ ê³„ì‚°
        news_with_content = news_df[news_df[content_col].notna() & (news_df[content_col] != "")].copy()
        news_with_content["content_length"] = news_with_content[content_col].astype(str).str.len()
    
    news_with_content = news_with_content[news_with_content["content_length"].notna() & (news_with_content["content_length"] > 0)]
    
    if news_with_content.empty:
        st.info("ğŸ“Š ë³¸ë¬¸ ê¸¸ì´ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        avg_length = news_with_content["content_length"].mean()
        st.metric("í‰ê·  ë³¸ë¬¸ ê¸¸ì´", f"{avg_length:.0f}ì")
    with col2:
        median_length = news_with_content["content_length"].median()
        st.metric("ì¤‘ê°„ê°’ ë³¸ë¬¸ ê¸¸ì´", f"{median_length:.0f}ì")
    with col3:
        min_length = news_with_content["content_length"].min()
        max_length = news_with_content["content_length"].max()
        st.metric("ìµœì†Œ/ìµœëŒ€ ê¸¸ì´", f"{min_length:.0f} / {max_length:.0f}ì")
    
    if px is not None and len(news_with_content) > 0:
        fig = px.histogram(
            news_with_content,
            x="content_length",
            nbins=50,
            title="ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬",
            labels={"content_length": "ë³¸ë¬¸ ê¸¸ì´ (ì)", "count": "ë¹ˆë„"}
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_content_missing_analysis(news_df: pd.DataFrame):
    """ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨"""
    # content ë˜ëŠ” raw_content_length ì»¬ëŸ¼ í™•ì¸
    content_col = None
    if "content" in news_df.columns:
        content_col = "content"
    elif "raw_content_length" in news_df.columns:
        content_col = "raw_content_length"
    else:
        return
    
    st.markdown("#### ğŸ“ ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨")
    
    total_count = len(news_df)
    
    if content_col == "raw_content_length":
        # raw_content_lengthê°€ ìˆ«ì ì»¬ëŸ¼ì¸ ê²½ìš°
        missing_content = news_df[news_df[content_col].isna() | (pd.to_numeric(news_df[content_col], errors='coerce') == 0)]
        missing_count = len(missing_content)
        
        short_content = news_df[
            news_df[content_col].notna() &
            (pd.to_numeric(news_df[content_col], errors='coerce') < 50)
        ]
        short_count = len(short_content)
    else:
        # content ì»¬ëŸ¼ì¸ ê²½ìš°
        missing_content = news_df[news_df[content_col].isna() | (news_df[content_col] == "")]
        missing_count = len(missing_content)
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš°ë„ ëˆ„ë½ìœ¼ë¡œ ê°„ì£¼ (50ì ë¯¸ë§Œ)
        short_content = news_df[
            news_df[content_col].notna() & 
            (news_df[content_col] != "") &
            (news_df[content_col].astype(str).str.len() < 50)
        ]
        short_count = len(short_content)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ë³¸ë¬¸ ì™„ì „ ëˆ„ë½", f"{missing_count:,}ê±´")
        if total_count > 0:
            missing_rate = (missing_count / total_count) * 100
            st.caption(f"ëˆ„ë½ë¥ : {missing_rate:.1f}%")
    with col2:
        st.metric("ë³¸ë¬¸ ì§§ìŒ (<50ì)", f"{short_count:,}ê±´")
        if total_count > 0:
            short_rate = (short_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {short_rate:.1f}%")
    with col3:
        total_issue = missing_count + short_count
        st.metric("ì´ ë¬¸ì œ ê¸°ì‚¬", f"{total_issue:,}ê±´")
        if total_count > 0:
            issue_rate = (total_issue / total_count) * 100
            st.caption(f"ë¬¸ì œ ë¹„ìœ¨: {issue_rate:.1f}%")
    
    # ì‹œê°„ëŒ€ë³„ ëˆ„ë½/ì§§ì€ ê¸°ì‚¬ ë¹„ìœ¨ ì¶”ì´ (Line chart)
    if "published_at" in news_df.columns and total_count > 0:
        news_with_date = news_df[news_df["published_at"].notna()].copy()
        if not news_with_date.empty:
            news_with_date["date"] = news_with_date["published_at"].dt.date
            
            if content_col == "raw_content_length":
                news_with_date["is_missing"] = news_with_date[content_col].isna() | (pd.to_numeric(news_with_date[content_col], errors='coerce') == 0)
                news_with_date["is_short"] = news_with_date[content_col].notna() & (pd.to_numeric(news_with_date[content_col], errors='coerce') < 50)
            else:
                news_with_date["is_missing"] = news_with_date[content_col].isna() | (news_with_date[content_col] == "")
                news_with_date["is_short"] = (
                    news_with_date[content_col].notna() & 
                    (news_with_date[content_col] != "") &
                    (news_with_date[content_col].astype(str).str.len() < 50)
                )
            
            daily_stats = news_with_date.groupby("date").agg({
                "is_missing": "sum",
                "is_short": "sum"
            }).reset_index()
            daily_stats["total"] = news_with_date.groupby("date").size().values
            daily_stats["missing_rate"] = (daily_stats["is_missing"] / daily_stats["total"] * 100).fillna(0)
            daily_stats["short_rate"] = (daily_stats["is_short"] / daily_stats["total"] * 100).fillna(0)
            daily_stats["issue_rate"] = ((daily_stats["is_missing"] + daily_stats["is_short"]) / daily_stats["total"] * 100).fillna(0)
            
            if px is not None and len(daily_stats) > 0:
                fig = px.line(
                    daily_stats,
                    x="date",
                    y=["missing_rate", "short_rate", "issue_rate"],
                    title="ì¼ë³„ ë³¸ë¬¸ í’ˆì§ˆ ë¬¸ì œ ë¹„ìœ¨ ì¶”ì´",
                    labels={"date": "ë‚ ì§œ", "value": "ë¹„ìœ¨ (%)", "variable": "ìœ í˜•"},
                    color_discrete_map={
                        "missing_rate": "#ef4444",
                        "short_rate": "#f59e0b",
                        "issue_rate": "#3b82f6"
                    }
                )
                fig.update_traces(mode='lines+markers')
                st.plotly_chart(fig, use_container_width=True)
    
    if total_count > 0 and px is not None:
        quality_df = pd.DataFrame({
            "ìƒíƒœ": ["ì •ìƒ", "ëˆ„ë½", "ì§§ìŒ"],
            "ê±´ìˆ˜": [total_count - total_issue, missing_count, short_count]
        })
        fig = px.pie(
            quality_df,
            values="ê±´ìˆ˜",
            names="ìƒíƒœ",
            title="ë³¸ë¬¸ í’ˆì§ˆ ìƒíƒœ"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_title_content_duplication(news_df: pd.DataFrame):
    """ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µë¥ """
    if "title" not in news_df.columns or "content" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ”„ ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µë¥ ")
    
    # titleê³¼ contentê°€ ëª¨ë‘ ìˆëŠ” ë‰´ìŠ¤ë§Œ ë¶„ì„
    valid_news = news_df[
        news_df["title"].notna() & 
        (news_df["title"] != "") &
        news_df["content"].notna() & 
        (news_df["content"] != "")
    ].copy()
    
    if valid_news.empty:
        st.info("ğŸ“Š ì œëª©ê³¼ ë³¸ë¬¸ì´ ëª¨ë‘ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì œëª©ì´ ë³¸ë¬¸ ì•ë¶€ë¶„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    def check_duplication(row):
        title = str(row["title"]).strip()
        content = str(row["content"]).strip()
        
        if not title or not content:
            return False
        
        # ë³¸ë¬¸ ì•ë¶€ë¶„(ì œëª© ê¸¸ì´ì˜ 2ë°°)ì—ì„œ ì œëª© í¬í•¨ ì—¬ë¶€ í™•ì¸
        content_preview = content[:len(title) * 2]
        return title in content_preview
    
    valid_news["has_duplication"] = valid_news.apply(check_duplication, axis=1)
    
    duplication_count = valid_news["has_duplication"].sum()
    total_count = len(valid_news)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì¤‘ë³µ ë°œê²¬", f"{duplication_count:,}ê±´")
    with col2:
        if total_count > 0:
            dup_rate = (duplication_count / total_count) * 100
            st.metric("ì¤‘ë³µë¥ ", f"{dup_rate:.1f}%")
        else:
            st.metric("ì¤‘ë³µë¥ ", "N/A")
    
    if total_count > 0 and px is not None:
        dup_df = pd.DataFrame({
            "ìƒíƒœ": ["ì¤‘ë³µ ì—†ìŒ", "ì¤‘ë³µ ìˆìŒ"],
            "ê±´ìˆ˜": [total_count - duplication_count, duplication_count]
        })
        fig = px.pie(
            dup_df,
            values="ê±´ìˆ˜",
            names="ìƒíƒœ",
            title="ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µ ë¶„í¬"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_impact_score_distribution(news_df: pd.DataFrame):
    """ì„íŒ©íŠ¸ ì ìˆ˜ ë¶„í¬"""
    if "impact_score" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“Š ì„íŒ©íŠ¸ ì ìˆ˜ ë¶„í¬")
    
    # impact_scoreê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_score = news_df[news_df["impact_score"].notna()].copy()
    
    if news_with_score.empty:
        st.info("ğŸ“Š ì„íŒ©íŠ¸ ì ìˆ˜ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        avg_score = news_with_score["impact_score"].mean()
        st.metric("í‰ê·  ì„íŒ©íŠ¸ ì ìˆ˜", f"{avg_score:.1f}")
    with col2:
        median_score = news_with_score["impact_score"].median()
        st.metric("ì¤‘ê°„ê°’ ì ìˆ˜", f"{median_score:.1f}")
    with col3:
        min_score = news_with_score["impact_score"].min()
        max_score = news_with_score["impact_score"].max()
        st.metric("ìµœì†Œ/ìµœëŒ€ ì ìˆ˜", f"{min_score:.0f} / {max_score:.0f}")
    
    if px is not None and len(news_with_score) > 0:
        fig = px.histogram(
            news_with_score,
            x="impact_score",
            nbins=30,
            title="ì„íŒ©íŠ¸ ì ìˆ˜ ë¶„í¬",
            labels={"impact_score": "ì„íŒ©íŠ¸ ì ìˆ˜", "count": "ë¹ˆë„"}
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬
        score_bins = [0, 10, 20, 30, 40, 50, 100, float('inf')]
        score_labels = ["0-10", "10-20", "20-30", "30-40", "40-50", "50-100", "100+"]
        news_with_score["score_range"] = pd.cut(
            news_with_score["impact_score"], 
            bins=score_bins, 
            labels=score_labels, 
            right=False
        )
        range_counts = news_with_score["score_range"].value_counts().sort_index()
        
        if len(range_counts) > 0:
            range_df = range_counts.reset_index()
            range_df.columns = ["ì ìˆ˜ êµ¬ê°„", "ê±´ìˆ˜"]
            fig2 = px.bar(
                range_df,
                x="ì ìˆ˜ êµ¬ê°„",
                y="ê±´ìˆ˜",
                title="ì„íŒ©íŠ¸ ì ìˆ˜ êµ¬ê°„ë³„ ë¶„í¬"
            )
            st.plotly_chart(fig2, use_container_width=True)

def _render_duplicate_news_analysis(news_df: pd.DataFrame):
    """ì¤‘ë³µ ê¸°ì‚¬ ë¹„ìœ¨"""
    if "url" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ” ì¤‘ë³µ ê¸°ì‚¬ ë¹„ìœ¨")
    
    # urlì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_url = news_df[news_df["url"].notna() & (news_df["url"] != "")]
    
    if news_with_url.empty:
        st.info("ğŸ“Š URLì´ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸
    url_counts = news_with_url["url"].value_counts()
    duplicate_urls = url_counts[url_counts > 1]
    
    total_news = len(news_with_url)
    unique_urls = len(url_counts)
    duplicate_count = len(duplicate_urls)
    total_duplicate_instances = duplicate_urls.sum() - duplicate_count  # ì¤‘ë³µ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê³ ìœ  URL ìˆ˜", f"{unique_urls:,}ê°œ")
    with col2:
        st.metric("ì¤‘ë³µ URL ìˆ˜", f"{duplicate_count:,}ê°œ")
    with col3:
        if total_news > 0:
            dup_rate = (total_duplicate_instances / total_news) * 100
            st.metric("ì¤‘ë³µ ë¹„ìœ¨", f"{dup_rate:.1f}%")
        else:
            st.metric("ì¤‘ë³µ ë¹„ìœ¨", "N/A")
    
    # ì¤‘ë³µì´ ë§ì€ URL Top 10
    if len(duplicate_urls) > 0:
        top_duplicates = duplicate_urls.head(10).reset_index()
        top_duplicates.columns = ["URL", "ì¤‘ë³µ íšŸìˆ˜"]
        st.dataframe(top_duplicates, use_container_width=True, height=200)

def _render_news_collection_trends(news_df: pd.DataFrame):
    """RSS/í¬ë¡¤ë§ë³„ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸"""
    if "published_at" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸")
    
    # published_atì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_date = news_df[news_df["published_at"].notna()].copy()
    
    if news_with_date.empty:
        st.info("ğŸ“Š ë°œí–‰ì¼ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¼ë³„ ìˆ˜ì§‘ëŸ‰
    news_with_date["date"] = news_with_date["published_at"].dt.date
    daily_counts = news_with_date.groupby("date").size().reset_index(name="ê±´ìˆ˜")
    daily_counts = daily_counts.sort_values("date")
    
    col1, col2 = st.columns(2)
    with col1:
        total_news = len(news_with_date)
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_news:,}ê±´")
    with col2:
        if len(daily_counts) > 0:
            avg_daily = daily_counts["ê±´ìˆ˜"].mean()
            st.metric("ì¼í‰ê·  ìˆ˜ì§‘ëŸ‰", f"{avg_daily:.1f}ê±´")
    
    if px is not None and len(daily_counts) > 0:
        fig = px.line(
            daily_counts,
            x="date",
            y="ê±´ìˆ˜",
            title="ì¼ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì´",
            labels={"date": "ë‚ ì§œ", "ê±´ìˆ˜": "ìˆ˜ì§‘ ê±´ìˆ˜"}
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # sourceë³„ ìˆ˜ì§‘ëŸ‰ (sourceê°€ ìˆëŠ” ê²½ìš°)
    if "source" in news_df.columns:
        news_with_source = news_with_date[news_with_date["source"].notna() & (news_with_date["source"] != "")]
        if not news_with_source.empty:
            source_counts = news_with_source["source"].value_counts().head(10)
            
            if px is not None and len(source_counts) > 0:
                source_df = source_counts.reset_index()
                source_df.columns = ["ì¶œì²˜", "ê±´ìˆ˜"]
                fig2 = px.bar(
                    source_df,
                    x="ì¶œì²˜",
                    y="ê±´ìˆ˜",
                    title="ì¶œì²˜ë³„ ìˆ˜ì§‘ëŸ‰ Top 10",
                    labels={"ì¶œì²˜": "ì–¸ë¡ ì‚¬/ì†ŒìŠ¤", "ê±´ìˆ˜": "ìˆ˜ì§‘ ê±´ìˆ˜"}
                )
                fig2.update_xaxes(tickangle=-45)
                st.plotly_chart(fig2, use_container_width=True)

def _get_korean_font_path():
    """í•œê¸€ í°íŠ¸ ê²½ë¡œ ì°¾ê¸°"""
    # Windows í°íŠ¸ ê²½ë¡œë“¤
    windows_font_paths = [
        "C:/Windows/Fonts/NanumGothic.ttf",
        "C:/Windows/Fonts/NanumBarunGothic.ttf",
        "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
        "C:/Windows/Fonts/gulim.ttc",  # êµ´ë¦¼
    ]
    
    # Linux/Mac í°íŠ¸ ê²½ë¡œë“¤
    linux_font_paths = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/System/Library/Fonts/AppleGothic.ttf",  # Mac
    ]
    
    # Windows ê²½ë¡œ í™•ì¸
    for font_path in windows_font_paths:
        if os.path.exists(font_path):
            return font_path
    
    # Linux/Mac ê²½ë¡œ í™•ì¸
    for font_path in linux_font_paths:
        if os.path.exists(font_path):
            return font_path
    
    return None

def _extract_korean_words(text: str) -> List[str]:
    """í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ (ì¡°ì‚¬/ë¶ˆìš©ì–´ ì œê±°)"""
    if not text:
        return []
    
    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë° ì¡°ì‚¬ (ë‰´ìŠ¤ íŠ¹í™”)
    stopwords = {
        # ì¡°ì‚¬
        "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
        "ì—ì„œ", "ì—ê²Œ", "í•œí…Œ", "ê»˜", "ë¶€í„°", "ê¹Œì§€", "ë§Œ", "ì¡°ì°¨", "ë§ˆì €", "ë¿",
        
        # ëŒ€ëª…ì‚¬
        "ê·¸", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ", "ê·¸ë“¤", "ì´ë“¤", "ì €ë“¤", "ê·¸ë…€", "ê·¸ë¶„",
        
        # ì ‘ì†ì‚¬/ë¶€ì‚¬
        "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê·¸ëŸ°ë°", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ",
        "ë•Œë¬¸ì—", "ìœ„í•´", "ëŒ€í•´", "ê´€ë ¨", "ë”", "ë˜", "ë˜ëŠ”", "ë°",
        
        # ë‰´ìŠ¤ ê´€ë ¨ ìš©ì–´
        "ê¸°ì", "ì—°í•©ë‰´ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ì·¨ì¬", "ì „ë¬¸", "ì¸í„°ë·°",
        "ë°œí‘œ", "ë°œìƒ", "í™•ì¸", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "í–ˆë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
        "ì „ë§", "ì˜ˆìƒ", "ì˜ˆì¸¡", "ì¶”ì •", "ë¶„ì„", "ì¡°ì‚¬", "ê²°ê³¼", "ë°œê²¬",
        "ì´ë²ˆ", "ì´ë‚ ", "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼", "ì§€ë‚œ", "ì˜¬í•´", "ì‘ë…„", "ë‚´ë…„",
        "ì§€ë‚œí•´", "ì˜¬", "ì‘", "ë‚´",
        
        # ì‹œê°„ ê´€ë ¨
        "ì›”", "ì¼", "ë…„", "ì‹œ", "ë¶„", "ì´ˆ", "ìš”ì¼", "ì£¼", "ê°œì›”", "ë…„ë„",
        "ì˜¤ì „", "ì˜¤í›„", "ìƒˆë²½", "ë°¤", "ë‚®", "ì €ë…", "ë‚ ì§œ",
        
        # ìˆ˜ëŸ‰/ë‹¨ìœ„ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ë§Œ", "ì–µ", "ì¡°", "ì›", "ë‹¬ëŸ¬", "í¼ì„¼íŠ¸", "í”„ë¡œ",
        "ê°œ", "ëª…", "ê±´", "ê³³", "ì°¨ë¡€", "ë²ˆ", "íšŒ", "ì°¨",
        
        # ì¼ë°˜ ë™ì‚¬/í˜•ìš©ì‚¬ (ë„ˆë¬´ ë¹ˆë²ˆí•œ ê²ƒë“¤)
        "ìˆë‹¤", "ì—†ë‹¤", "ë˜ë‹¤", "í•˜ë‹¤", "ì´ë‹¤", "ì•Šë‹¤", "ì•„ë‹ˆë‹¤",
        "ê°™ë‹¤", "ë‹¤ë¥´ë‹¤", "í¬ë‹¤", "ì‘ë‹¤", "ë§ë‹¤", "ì ë‹¤", "ë†’ë‹¤", "ë‚®ë‹¤",
        "ì¢‹ë‹¤", "ë‚˜ì˜ë‹¤", "ìƒˆë¡­ë‹¤", "ì˜¤ë˜ë˜ë‹¤", "ìµœê·¼", "ìµœì‹ ",
        "ìˆëŠ”", "ì—†ëŠ”", "ë˜ëŠ”", "í•˜ëŠ”", "ê²ƒìœ¼ë¡œ", "ê²ƒì´ë‹¤", "ê²ƒì´", "ê²ƒì„", "ê²ƒì„",
        
        # ì¼ë°˜ì ì¸ í˜•ìš©ì‚¬/ë¶€ì‚¬
        "ë§¤ìš°", "ì•„ì£¼", "ë„ˆë¬´", "ì •ë§", "ì§„ì§œ", "ì™„ì „", "ì „í˜€", "ë³„ë¡œ",
        "ê°€ì¥", "ìµœê³ ", "ìµœëŒ€", "ìµœì†Œ",
        
        # ë‰´ìŠ¤ ë¬¸ì²´ íŠ¹í™”
        "ë”°ë¥´ë©´", "ì— ë”°ë¥´ë©´", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "ì „í–ˆë‹¤", "ì•Œë ¤ì¡Œë‹¤",
        "í™•ì¸ëë‹¤", "ë°œìƒí–ˆë‹¤", "ë‚˜íƒ€ë‚¬ë‹¤", "ì§€ì í–ˆë‹¤", "ê°•ì¡°í–ˆë‹¤",
        "ì£¼ì¥í–ˆë‹¤", "ì œê¸°í–ˆë‹¤", "ìš”êµ¬í–ˆë‹¤", "ì´‰êµ¬í–ˆë‹¤", "ì œì•ˆí–ˆë‹¤",
        "ë°œí‘œí–ˆë‹¤", "ê³µê°œí–ˆë‹¤", "ë°œí‘œëë‹¤", "ê³µê°œëë‹¤",
        
        # ê¸°ê´€/ì§ì±… (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ì •ë¶€", "êµ­ê°€", "ê¸°ê´€", "ë‹¨ì²´", "ì¡°ì§", "íšŒì‚¬", "ê¸°ì—…", "ë²•ì¸",
        "ëŒ€í‘œ", "ì‚¬ì¥", "íšŒì¥", "ì´ì‚¬", "ì§ì›", "ê´€ê³„ì", "ë‹¹êµ­",
        
        # ì¼ë°˜ì ì¸ ëª…ì‚¬ (ë‰´ìŠ¤ì—ì„œ ë„ˆë¬´ ìì£¼ ë‚˜ì˜¤ëŠ” ê²ƒë“¤)
        "ì‚¬ì‹¤", "ë‚´ìš©", "ìƒí™©", "ë¬¸ì œ", "ì´ìŠˆ", "ì‚¬ê±´", "ì‚¬ê³ ", "ì‚¬ë¡€",
        "ê²½ìš°", "ë•Œë¬¸", "ì´ìœ ", "ì›ì¸", "ê²°ê³¼", "ì˜í–¥", "íš¨ê³¼",
        "ë°©ë²•", "ë°©ì•ˆ", "ëŒ€ì±…", "ì •ì±…", "ì œë„", "ì‹œìŠ¤í…œ",
        "ê³¼ì •", "ì ˆì°¨", "ë‹¨ê³„", "ìˆ˜ì¤€", "ì •ë„", "ë²”ìœ„",
        "êµ­ë‚´", "ëŒ€ë¹„",
        
        # ìœ„ì¹˜/ë°©í–¥ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ìœ„", "ì•„ë˜", "ì•", "ë’¤", "ì™¼ìª½", "ì˜¤ë¥¸ìª½", "ì¤‘ì•™", "ì¤‘ì‹¬",
        "ë‚´ë¶€", "ì™¸ë¶€", "ì•ìª½", "ë’¤ìª½", "ì–‘ìª½",
        
        # ê¸°íƒ€ ë¹ˆë²ˆí•œ ë‹¨ì–´
        "ë“±", "ë°", "ë˜", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜",
        "ì´ë¯¸", "ì•„ì§", "ë²Œì¨", "ê³§", "ê³§ë°”ë¡œ", "ì¦‰ì‹œ", "ë°”ë¡œ",
    }
    
    # ì˜ë¯¸ì—†ëŠ” ìˆ«ì íŒ¨í„´ ì œê±° (1-31ì¼, 1-12ì›” ë“±)
    # ë‹¨ì–´ ì¶”ì¶œ í›„ ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±°
    
    # í•œê¸€ë§Œ ì¶”ì¶œ (í•œê¸€, ê³µë°±, ìˆ«ì)
    korean_text = re.sub(r'[^ê°€-í£\s0-9]', ' ', text)
    
    # ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
    words = korean_text.split()
    
    # í•„í„°ë§ í•¨ìˆ˜
    def should_keep_word(word):
        word = word.strip()
        
        # 2ê¸€ì ë¯¸ë§Œ ì œê±°
        if len(word) < 2:
            return False
        
        # ë¶ˆìš©ì–´ ì œê±°
        if word in stopwords:
            return False
        
        # ì˜ë¯¸ì—†ëŠ” ìˆ«ì íŒ¨í„´ ì œê±° (1-31ì¼, 1-12ì›” ë“±)
        # ìˆ«ìë§Œ ìˆê±°ë‚˜ ìˆ«ì+ì¼/ì›”/ë…„/ì‹œ/ë¶„ ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ì–´
        if re.match(r'^\d+[ì¼ì›”ë…„ì‹œë¶„ì´ˆ]?$', word):
            # 1-31ì¼, 1-12ì›” ê°™ì€ ì˜ë¯¸ì—†ëŠ” ìˆ«ìëŠ” ì œê±°
            num_match = re.match(r'^(\d+)([ì¼ì›”ë…„ì‹œë¶„ì´ˆ]?)$', word)
            if num_match:
                num = int(num_match.group(1))
                suffix = num_match.group(2)
                # 1-31ì¼, 1-12ì›” ê°™ì€ íŒ¨í„´ì€ ì œê±°
                if suffix in ['ì¼', 'ì›”'] and 1 <= num <= 31:
                    return False
                # 1-24ì‹œ ê°™ì€ íŒ¨í„´ë„ ì œê±°
                if suffix == 'ì‹œ' and 1 <= num <= 24:
                    return False
                # 1-60ë¶„ ê°™ì€ íŒ¨í„´ë„ ì œê±°
                if suffix == 'ë¶„' and 1 <= num <= 60:
                    return False
                # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° (ë„ˆë¬´ ì§§ì€ ìˆ«ì)
                if not suffix and num < 100:
                    return False
        
        return True
    
    # í•„í„°ë§ ì ìš©
    filtered_words = [
        word.strip() 
        for word in words 
        if should_keep_word(word.strip())
    ]
    
    return filtered_words

def _render_wordcloud(news_df: pd.DataFrame):
    """ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if not WORDCLOUD_AVAILABLE:
        st.info("ğŸ’¡ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `pip install wordcloud matplotlib`ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return
    
    st.markdown("#### â˜ï¸ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ")
    
    # ë‚ ì§œ ì„ íƒ UI
    if "published_at" in news_df.columns:
        news_with_date = news_df[news_df["published_at"].notna()].copy()
        if not news_with_date.empty:
            news_with_date["date"] = pd.to_datetime(news_with_date["published_at"]).dt.date
            available_dates = sorted(news_with_date["date"].unique(), reverse=True)
            
            if len(available_dates) > 0:
                # ê¸°ë³¸ê°’: ê°€ì¥ ìµœê·¼ ë‚ ì§œ
                default_index = 0
                selected_date = st.selectbox(
                    "ë‚ ì§œ ì„ íƒ",
                    options=available_dates,
                    index=default_index,
                    format_func=lambda x: x.strftime("%Yë…„ %mì›” %dì¼") if hasattr(x, 'strftime') else str(x),
                    key="wordcloud_date"
                )
                
                # ì„ íƒí•œ ë‚ ì§œì˜ ë‰´ìŠ¤ë§Œ í•„í„°ë§
                selected_news = news_with_date[
                    news_with_date["date"] == selected_date
                ].copy()
                
                if selected_news.empty:
                    date_str = selected_date.strftime('%Yë…„ %mì›” %dì¼') if hasattr(selected_date, 'strftime') else str(selected_date)
                    st.info(f"ğŸ“Š {date_str}ì— ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    return
            else:
                st.info("ğŸ“Š ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
        else:
            st.info("ğŸ“Š ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
    else:
        # published_atì´ ì—†ìœ¼ë©´ ì „ì²´ ë‰´ìŠ¤ ì‚¬ìš©
        selected_news = news_df.copy()
        st.info("âš ï¸ published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        selected_date = None
    
    if selected_news.empty:
        st.info("ğŸ“Š ì„ íƒí•œ ë‚ ì§œì— ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    recent_news = selected_news
    
    # titleê³¼ content í•©ì¹˜ê¸°
    text_list = []
    for idx, row in recent_news.iterrows():
        title = str(row.get("title", "")).strip()
        content = str(row.get("content", "")).strip()
        
        if title or content:
            combined_text = f"{title} {content}"
            text_list.append(combined_text)
    
    if not text_list:
        st.info("ğŸ“Š ì œëª©ê³¼ ë³¸ë¬¸ì´ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    all_text = " ".join(text_list)
    
    # í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ
    words = _extract_korean_words(all_text)
    
    if not words:
        st.info("ğŸ“Š ì¶”ì¶œëœ í•œêµ­ì–´ ë‹¨ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë‹¨ì–´ ë¹ˆë„ìˆ˜ ê³„ì‚°
    word_freq = {}
    for word in words:
        word_freq[word] = word_freq.get(word, 0) + 1
    
    if not word_freq:
        st.info("ğŸ“Š í‚¤ì›Œë“œ ë¹ˆë„ìˆ˜ê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # í•œê¸€ í°íŠ¸ ê²½ë¡œ ì°¾ê¸°
    font_path = _get_korean_font_path()
    
    if not font_path:
        st.warning("âš ï¸ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì›Œë“œí´ë¼ìš°ë“œê°€ ì œëŒ€ë¡œ í‘œì‹œë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.info("ğŸ’¡ Windows: C:/Windows/Fonts/NanumGothic.ttf ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    try:
        wordcloud_params = {
            "width": 800,
            "height": 400,
            "background_color": "white",
            "max_words": 100,
            "relative_scaling": 0.5,
            "colormap": "viridis"
        }
        
        if font_path:
            wordcloud_params["font_path"] = font_path
        
        wordcloud = WordCloud(**wordcloud_params).generate_from_frequencies(word_freq)
        
        # ì´ë¯¸ì§€ í‘œì‹œ
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        if selected_date:
            date_str = selected_date.strftime('%Yë…„ %mì›” %dì¼') if hasattr(selected_date, 'strftime') else str(selected_date)
            title = f"{date_str} ë‰´ìŠ¤ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ"
        else:
            title = "ë‰´ìŠ¤ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ"
        ax.set_title(title, fontsize=16, pad=20)
        
        st.pyplot(fig)
        plt.close(fig)
        
        # ìƒìœ„ í‚¤ì›Œë“œ í‘œì‹œ
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:20]
        if top_keywords:
            st.markdown("##### ğŸ” ìƒìœ„ í‚¤ì›Œë“œ Top 20")
            keyword_df = pd.DataFrame(top_keywords, columns=["í‚¤ì›Œë“œ", "ë¹ˆë„"])
            st.dataframe(keyword_df, use_container_width=True, height=300)
    
    except Exception as e:
        st.error(f"âŒ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        st.info("ğŸ’¡ í•œê¸€ í°íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¬ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")

def _render_news_radar_analysis(news_df: pd.DataFrame):
    """
    ë‰´ìŠ¤ ì ìˆ˜ ë¶„ì„ + ë¼ì´ë‹¤ ì°¨íŠ¸
    
    ë¶„ì„ ì§€í‘œ:
    - impact_score: ë‰´ìŠ¤ì˜ ì˜í–¥ë„ ì ìˆ˜
    - credibility_score: ë‰´ìŠ¤ì˜ ì‹ ë¢°ë„ ì ìˆ˜
    - urgency_score: ë‰´ìŠ¤ì˜ ê¸´ê¸‰ë„ ì ìˆ˜
    """
    st.markdown("### ğŸ“Š ë‰´ìŠ¤ ì ìˆ˜ ë¶„ì„ + ë¼ì´ë‹¤ ì°¨íŠ¸")
    st.markdown("**ëª©ì **: ë‰´ìŠ¤ì˜ 3ê°€ì§€ ì ìˆ˜(impact_score, credibility_score, urgency_score)ë¥¼ ì‹œê°í™”")
    
    if news_df.empty:
        st.info("ğŸ“Š ë¶„ì„í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•„ìš”í•œ ì ìˆ˜ ì»¬ëŸ¼ í™•ì¸
    required_columns = ["impact_score", "credibility_score", "urgency_score"]
    missing_columns = [col for col in required_columns if col not in news_df.columns]
    
    if missing_columns:
        st.warning(f"âš ï¸ í•„ìš”í•œ ì ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_columns)}")
        st.info("ğŸ’¡ ë‰´ìŠ¤ ë°ì´í„°ì— impact_score, credibility_score, urgency_score ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return
    
    # ë‰´ìŠ¤ ì„ íƒ UI
    st.markdown("#### ğŸ“° ë¶„ì„í•  ë‰´ìŠ¤ ì„ íƒ")
    
    if "title" in news_df.columns:
        # ì œëª©ìœ¼ë¡œ ì„ íƒ
        if "news_id" in news_df.columns:
            titles_with_id = [f"[{row['news_id']}] {row['title']}" if pd.notna(row['title']) else f"[{row['news_id']}] ì œëª© ì—†ìŒ" 
                             for _, row in news_df.iterrows()]
        else:
            titles_with_id = news_df["title"].fillna("ì œëª© ì—†ìŒ").tolist()
        
        selected_index = st.selectbox(
            "ë‰´ìŠ¤ ì„ íƒ",
            range(len(titles_with_id)),
            format_func=lambda x: titles_with_id[x][:100] + "..." if len(titles_with_id[x]) > 100 else titles_with_id[x],
            key="radar_news_selection"
        )
        
        selected_news = news_df.iloc[selected_index].to_dict()
    else:
        st.warning("âš ï¸ ì œëª© ì»¬ëŸ¼ì´ ì—†ì–´ ë‰´ìŠ¤ë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„ íƒí•œ ë‰´ìŠ¤ì˜ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    impact_score = float(selected_news.get("impact_score", 0)) if pd.notna(selected_news.get("impact_score")) else 0
    credibility_score = float(selected_news.get("credibility_score", 0)) if pd.notna(selected_news.get("credibility_score")) else 0
    urgency_score = float(selected_news.get("urgency_score", 0)) if pd.notna(selected_news.get("urgency_score")) else 0
    
    # ë¼ì´ë‹¤ ì°¨íŠ¸ ìƒì„±
    if go is not None:
        st.markdown("#### ğŸ“ˆ ì„ íƒí•œ ë‰´ìŠ¤ ë¼ì´ë‹¤ ì°¨íŠ¸")
        
        # ë¼ì´ë‹¤ ì°¨íŠ¸ ë°ì´í„° ì¤€ë¹„
        categories = ["Impact Score", "Credibility Score", "Urgency Score"]
        values = [impact_score, credibility_score, urgency_score]
        
        # ìµœëŒ€ê°’ ê³„ì‚° (ì ìˆ˜ ë²”ìœ„ê°€ 0-100ì´ ì•„ë‹ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
        max_score = max(values) if values else 100
        max_range = max(100, max_score * 1.2)  # ì—¬ìœ  ê³µê°„ì„ ìœ„í•´ 20% ì¶”ê°€
        
        # ë¼ì´ë‹¤ ì°¨íŠ¸ ìƒì„±
        fig = go.Figure()
        
        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=categories,
            fill='toself',
            name='ì„ íƒí•œ ë‰´ìŠ¤',
            line_color='rgb(59, 130, 246)'  # íŒŒë€ìƒ‰
        ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, max_range]
                )),
            showlegend=True,
            title="ë‰´ìŠ¤ ì ìˆ˜ ë¶„ì„",
            height=500
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # ì ìˆ˜ ìƒì„¸ ì •ë³´
        st.markdown("##### ğŸ“‹ ìƒì„¸ ì ìˆ˜")
        score_df = pd.DataFrame([
            {"ì§€í‘œ": "Impact Score", "ì ìˆ˜": f"{impact_score:.1f}"},
            {"ì§€í‘œ": "Credibility Score", "ì ìˆ˜": f"{credibility_score:.1f}"},
            {"ì§€í‘œ": "Urgency Score", "ì ìˆ˜": f"{urgency_score:.1f}"},
        ])
        st.dataframe(score_df, use_container_width=True)
        
        # ìµœê·¼ 7ì¼ í‰ê·  ë¼ì´ë‹¤ ì°¨íŠ¸
        st.markdown("#### ğŸ“Š ìµœê·¼ 7ì¼ ê¸°ì‚¬ í‰ê·  ë¼ì´ë‹¤ ì°¨íŠ¸")
        
        # ìµœê·¼ 7ì¼ ë‰´ìŠ¤ í•„í„°ë§
        if "published_at" in news_df.columns:
            cutoff_date = datetime.now() - timedelta(days=7)
            recent_news = news_df[
                news_df["published_at"].notna() & 
                (pd.to_datetime(news_df["published_at"]) >= cutoff_date)
            ].copy()
        else:
            recent_news = news_df.copy()
            st.info("âš ï¸ published_at ì»¬ëŸ¼ì´ ì—†ì–´ ì „ì²´ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        if not recent_news.empty:
            # ìµœê·¼ 7ì¼ ë‰´ìŠ¤ë“¤ì˜ ì ìˆ˜ ìˆ˜ì§‘
            impact_scores = []
            credibility_scores = []
            urgency_scores = []
            
            for idx, row in recent_news.iterrows():
                impact = row.get("impact_score")
                credibility = row.get("credibility_score")
                urgency = row.get("urgency_score")
                
                if pd.notna(impact):
                    impact_scores.append(float(impact))
                if pd.notna(credibility):
                    credibility_scores.append(float(credibility))
                if pd.notna(urgency):
                    urgency_scores.append(float(urgency))
            
            if impact_scores or credibility_scores or urgency_scores:
                # ê¸°ìˆ í†µê³„ ê³„ì‚° í•¨ìˆ˜
                def calc_stats(scores):
                    """ì ìˆ˜ ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ê¸°ìˆ í†µê³„ ê³„ì‚°"""
                    if not scores:
                        return {
                            "í‰ê· ": 0.0,
                            "í‘œì¤€í¸ì°¨": 0.0,
                            "ìµœì†Œê°’": 0.0,
                            "ìµœëŒ€ê°’": 0.0,
                            "ì¤‘ì•™ê°’": 0.0,
                            "ê°œìˆ˜": 0
                        }
                    # numpyë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìˆ í†µê³„ ê³„ì‚°
                    try:
                        import numpy as np
                        scores_array = np.array(scores)
                        return {
                            "í‰ê· ": float(np.mean(scores_array)),
                            "í‘œì¤€í¸ì°¨": float(np.std(scores_array)),
                            "ìµœì†Œê°’": float(np.min(scores_array)),
                            "ìµœëŒ€ê°’": float(np.max(scores_array)),
                            "ì¤‘ì•™ê°’": float(np.median(scores_array)),
                            "ê°œìˆ˜": len(scores)
                        }
                    except ImportError:
                        # numpyê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ Pythonìœ¼ë¡œ ê³„ì‚°
                        n = len(scores)
                        mean = sum(scores) / n
                        variance = sum((x - mean) ** 2 for x in scores) / n
                        std_dev = variance ** 0.5
                        sorted_scores = sorted(scores)
                        median = sorted_scores[n // 2] if n % 2 == 1 else (sorted_scores[n // 2 - 1] + sorted_scores[n // 2]) / 2
                        return {
                            "í‰ê· ": float(mean),
                            "í‘œì¤€í¸ì°¨": float(std_dev),
                            "ìµœì†Œê°’": float(min(scores)),
                            "ìµœëŒ€ê°’": float(max(scores)),
                            "ì¤‘ì•™ê°’": float(median),
                            "ê°œìˆ˜": n
                        }
                
                # ê° ì ìˆ˜ë³„ ê¸°ìˆ í†µê³„ ê³„ì‚°
                impact_stats = calc_stats(impact_scores)
                credibility_stats = calc_stats(credibility_scores)
                urgency_stats = calc_stats(urgency_scores)
                
                # í‰ê· ê°’ (ë¼ì´ë‹¤ ì°¨íŠ¸ìš©)
                avg_impact = impact_stats["í‰ê· "]
                avg_credibility = credibility_stats["í‰ê· "]
                avg_urgency = urgency_stats["í‰ê· "]
                
                avg_values = [avg_impact, avg_credibility, avg_urgency]
                avg_max_range = max(100, max(avg_values) * 1.2) if avg_values else 100
                
                # í‰ê·  ë¼ì´ë‹¤ ì°¨íŠ¸ ìƒì„±
                avg_fig = go.Figure()
                
                avg_fig.add_trace(go.Scatterpolar(
                    r=avg_values,
                    theta=categories,
                    fill='toself',
                    name='ìµœê·¼ 7ì¼ í‰ê· ',
                    line_color='rgb(34, 197, 94)'  # ì´ˆë¡ìƒ‰
                ))
                
                avg_fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, avg_max_range]
                        )),
                    showlegend=True,
                    title="ìµœê·¼ 7ì¼ ê¸°ì‚¬ í‰ê·  ì ìˆ˜",
                    height=500
                )
                
                st.plotly_chart(avg_fig, use_container_width=True)
                
                # ê¸°ìˆ í†µê³„ ìƒì„¸ ì •ë³´
                st.markdown("##### ğŸ“‹ ìµœê·¼ 7ì¼ ê¸°ìˆ í†µê³„")
                stats_df = pd.DataFrame([
                    {
                        "ì§€í‘œ": "Impact Score",
                        "í‰ê· ": f"{impact_stats['í‰ê· ']:.1f}",
                        "í‘œì¤€í¸ì°¨": f"{impact_stats['í‘œì¤€í¸ì°¨']:.1f}",
                        "ìµœì†Œê°’": f"{impact_stats['ìµœì†Œê°’']:.1f}",
                        "ìµœëŒ€ê°’": f"{impact_stats['ìµœëŒ€ê°’']:.1f}",
                        "ì¤‘ì•™ê°’": f"{impact_stats['ì¤‘ì•™ê°’']:.1f}",
                        "ê°œìˆ˜": f"{impact_stats['ê°œìˆ˜']:,}ê±´"
                    },
                    {
                        "ì§€í‘œ": "Credibility Score",
                        "í‰ê· ": f"{credibility_stats['í‰ê· ']:.1f}",
                        "í‘œì¤€í¸ì°¨": f"{credibility_stats['í‘œì¤€í¸ì°¨']:.1f}",
                        "ìµœì†Œê°’": f"{credibility_stats['ìµœì†Œê°’']:.1f}",
                        "ìµœëŒ€ê°’": f"{credibility_stats['ìµœëŒ€ê°’']:.1f}",
                        "ì¤‘ì•™ê°’": f"{credibility_stats['ì¤‘ì•™ê°’']:.1f}",
                        "ê°œìˆ˜": f"{credibility_stats['ê°œìˆ˜']:,}ê±´"
                    },
                    {
                        "ì§€í‘œ": "Urgency Score",
                        "í‰ê· ": f"{urgency_stats['í‰ê· ']:.1f}",
                        "í‘œì¤€í¸ì°¨": f"{urgency_stats['í‘œì¤€í¸ì°¨']:.1f}",
                        "ìµœì†Œê°’": f"{urgency_stats['ìµœì†Œê°’']:.1f}",
                        "ìµœëŒ€ê°’": f"{urgency_stats['ìµœëŒ€ê°’']:.1f}",
                        "ì¤‘ì•™ê°’": f"{urgency_stats['ì¤‘ì•™ê°’']:.1f}",
                        "ê°œìˆ˜": f"{urgency_stats['ê°œìˆ˜']:,}ê±´"
                    },
                ])
                st.dataframe(stats_df, use_container_width=True)
                st.caption(f"ğŸ“Š ë¶„ì„ ê¸°ì‚¬ ìˆ˜: {len(recent_news):,}ê±´")
            else:
                st.info("ğŸ“Š ì ìˆ˜ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ“Š ìµœê·¼ 7ì¼ê°„ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ Plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ë¼ì´ë‹¤ ì°¨íŠ¸ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

def _calculate_news_scores(news: Dict[str, Any]) -> Dict[str, float]:
    """
    ë‰´ìŠ¤ì˜ 5ê°€ì§€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ë¼ì´ë‹¤ ì°¨íŠ¸ì— ì‚¬ìš©í•  ì ìˆ˜ë¥¼ ë°˜í™˜
    
    ê³„ì‚° ì§€í‘œ:
    1. ì‹œì¥ ì˜í–¥ë„: ê¸ˆìœµ ì‹œì¥ ê´€ë ¨ í‚¤ì›Œë“œ ê¸°ë°˜ (0-100)
    2. ì •ë³´ ë°€ë„: ë³¸ë¬¸ ê¸¸ì´ ë° ìˆ«ì í¬í•¨ ì—¬ë¶€ (0-100)
    3. ì´ˆë³´ì ë‚œì´ë„: RAG ìš©ì–´ ì‚¬ì „ì— ìˆëŠ” ì „ë¬¸ ìš©ì–´ê°€ ë§ì„ìˆ˜ë¡ ê°ì  (0-100, ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)
    4. í•™ìŠµ ê°€ì¹˜: êµìœ¡ì  í‚¤ì›Œë“œ ê¸°ë°˜ (0-100)
    5. ì‹¤í–‰ ê°€ì¹˜: ì‹¤ìš©ì  ì¡°ì–¸ í‚¤ì›Œë“œ ê¸°ë°˜ (0-100)
    
    Args:
        news: ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬ (title, content ë“± í¬í•¨)
    
    Returns:
        5ê°€ì§€ ì§€í‘œ ì ìˆ˜ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬
    """
    title = str(news.get("title", "")).strip()
    content = str(news.get("content", "")).strip()
    
    # ê¸°ë³¸ ì ìˆ˜: ëª¨ë“  ì§€í‘œëŠ” 50ì ì—ì„œ ì‹œì‘ (ì´ˆë³´ì ë‚œì´ë„ëŠ” 100ì ì—ì„œ ì‹œì‘)
    scores = {
        "market_impact": 50.0,      # ì‹œì¥ ì˜í–¥ë„: ê¸°ë³¸ 50ì 
        "info_density": 50.0,       # ì •ë³´ ë°€ë„: ê¸°ë³¸ 50ì  (ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¼ ì¬ì„¤ì •ë¨)
        "beginner_friendly": 100.0, # ì´ˆë³´ì ë‚œì´ë„: ê¸°ë³¸ 100ì  (ì „ë¬¸ ìš©ì–´ì— ë”°ë¼ ê°ì )
        "learning_value": 50.0,     # í•™ìŠµ ê°€ì¹˜: ê¸°ë³¸ 50ì 
        "action_value": 50.0        # ì‹¤í–‰ ê°€ì¹˜: ê¸°ë³¸ 50ì 
    }
    
    # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ì œëª© + ë³¸ë¬¸, ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ í‚¤ì›Œë“œ ë§¤ì¹­)
    text = f"{title} {content}".lower()
    content_len = len(content)
    title_len = len(title)
    
    # ========== 1. ì‹œì¥ ì˜í–¥ë„ (Market Impact) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ ê¸ˆìœµ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ í¬ê¸°ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ + ìˆ«ì í¬í•¨ ì—¬ë¶€
    
    # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡ (16ê°œ)
    market_keywords = [
        "ê¸ˆë¦¬", "ê¸ˆìœµ", "ì¦ê¶Œ", "ì£¼ì‹", "ì‹œì¥", "ê²½ì œ", "ì •ì±…", "í•œêµ­ì€í–‰",
        "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "í™˜ìœ¨", "ë¶€ë™ì‚°", "íˆ¬ì", "ìì‚°"
    ]
    market_count = sum(1 for keyword in market_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì  (ìµœëŒ€ 50ì  ì¶”ê°€ ê°€ëŠ¥)
    scores["market_impact"] = min(100, 50 + market_count * 5)
    
    # ìˆ«ì í¬í•¨ ì—¬ë¶€ (ê¸ˆë¦¬, í¼ì„¼íŠ¸, ê¸ˆì•¡ ë“±)
    # íŒ¨í„´: ìˆ«ì + ë‹¨ìœ„(%, ì›, ì–µ, ë§Œ, ì¡°)
    numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', text))
    # ìˆ«ì 1ê°œë‹¹ +2ì  (ìµœëŒ€ 20ì  ì¶”ê°€ ê°€ëŠ¥)
    scores["market_impact"] = min(100, scores["market_impact"] + min(numbers * 2, 20))
    
    # ========== 2. ì •ë³´ ë°€ë„ (Info Density) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ì— í¬í•¨ëœ ì •ë³´ì˜ ì–‘ê³¼ ì§ˆì„ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: ë³¸ë¬¸ ê¸¸ì´ + ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€
    
    if content_len > 0:
        # ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¥¸ ê¸°ë³¸ ì ìˆ˜ ì„¤ì •
        if content_len >= 2000:
            scores["info_density"] = 80  # ë§¤ìš° ê¸´ ê¸°ì‚¬: ì •ë³´ê°€ í’ë¶€í•¨
        elif content_len >= 1000:
            scores["info_density"] = 70  # ê¸´ ê¸°ì‚¬: ì •ë³´ê°€ ë§ìŒ
        elif content_len >= 500:
            scores["info_density"] = 60  # ì¤‘ê°„ ê¸¸ì´: ì ë‹¹í•œ ì •ë³´
        else:
            scores["info_density"] = 40  # ì§§ì€ ê¸°ì‚¬: ì •ë³´ê°€ ë¶€ì¡±í•¨
        
        # ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€ (ë³¸ë¬¸ ë‚´ì—ì„œë§Œ ê²€ìƒ‰)
        numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', content))
        # ìˆ«ì 1ê°œë‹¹ +1ì  (ìµœëŒ€ 20ì  ì¶”ê°€ ê°€ëŠ¥)
        scores["info_density"] = min(100, scores["info_density"] + min(numbers, 20))
    else:
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì •ë³´ ë°€ë„ëŠ” 0ì 
        scores["info_density"] = 0
    
    # ========== 3. ì´ˆë³´ì ë‚œì´ë„ (Beginner Friendly) ê³„ì‚° ==========
    # ëª©ì : ì´ˆë³´ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì •ë„ë¥¼ ì¸¡ì • (ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)
    # ê³„ì‚° ë°©ë²•: RAG ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ì˜ ìš©ì–´ ê°ì  + ë³¸ë¬¸ ê¸¸ì´ ê°ì 
    
    # RAG ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ì—ì„œ ìš©ì–´ ê°€ì ¸ì˜¤ê¸°
    try:
        from rag.glossary import ensure_financial_terms, DEFAULT_TERMS
        ensure_financial_terms()  # RAG ìš©ì–´ ì‚¬ì „ ì´ˆê¸°í™”
        financial_terms = st.session_state.get("financial_terms", DEFAULT_TERMS)
        expert_terms = list(financial_terms.keys()) if financial_terms else []
    except Exception:
        # RAG ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì „ë¬¸ ìš©ì–´ ëª©ë¡ ì‚¬ìš© (fallback)
        expert_terms = [
            "íŒŒìƒìƒí’ˆ", "ì˜µì…˜", "ì„ ë¬¼", "ìŠ¤ì™‘", "í—¤ì§€", "ë ˆë²„ë¦¬ì§€", "ë§ˆì§„ì½œ", "ì¦ê±°ê¸ˆ",
            "M&A", "IPO", "ê³µëª¨ì£¼", "ë°°ë‹¹ë½ì¼", "ì•¡ë©´ë¶„í• ", "ìœ ìƒì¦ì"
        ]
    
    # RAG ìš©ì–´ ì‚¬ì „ì— ìˆëŠ” ìš©ì–´ê°€ ë‰´ìŠ¤ì— í¬í•¨ëœ ê°œìˆ˜ ê³„ì‚°
    # ê¸´ ìš©ì–´ë¶€í„° ë§¤ì¹­í•˜ì—¬ ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€ (ì˜ˆ: "ê¸°ì¤€ê¸ˆë¦¬"ê°€ "ê¸ˆë¦¬"ë³´ë‹¤ ë¨¼ì € ë§¤ì¹­)
    expert_terms_sorted = sorted(expert_terms, key=len, reverse=True)
    matched_terms = []
    text_lower = text.lower()
    
    for term in expert_terms_sorted:
        if term.lower() in text_lower:
            matched_terms.append(term)
            # ì´ë¯¸ ë§¤ì¹­ëœ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€ (ê°„ë‹¨í•œ ë°©ë²•)
            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë§¤ì¹­ì´ í•„ìš”í•  ìˆ˜ ìˆì§€ë§Œ, ì„±ëŠ¥ì„ ìœ„í•´ ë‹¨ìˆœí™”
    
    expert_count = len(matched_terms)
    
    # RAG ìš©ì–´ê°€ ë§ì„ìˆ˜ë¡ ê°ì 
    # RAG ìš©ì–´ ì‚¬ì „ í¬ê¸°ì— ë”°ë¼ ê°ì  ë¹„ìœ¨ ì¡°ì •
    # - ìš©ì–´ ì‚¬ì „ì´ ì‘ìœ¼ë©´(50ê°œ ì´í•˜): ìš©ì–´ 1ê°œë‹¹ -2ì 
    # - ìš©ì–´ ì‚¬ì „ì´ ì¤‘ê°„(51-200ê°œ): ìš©ì–´ 1ê°œë‹¹ -1ì 
    # - ìš©ì–´ ì‚¬ì „ì´ í¬ë©´(200ê°œ ì´ìƒ): ìš©ì–´ 1ê°œë‹¹ -0.5ì 
    if len(expert_terms) <= 50:
        penalty_per_term = 2.0
    elif len(expert_terms) <= 200:
        penalty_per_term = 1.0
    else:
        penalty_per_term = 0.5
    
    total_penalty = min(50, expert_count * penalty_per_term)  # ìµœëŒ€ 50ì  ê°ì 
    scores["beginner_friendly"] = max(0, 100 - total_penalty)
    
    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì´í•´í•˜ê¸° ì–´ë ¤ì›€
    if content_len < 300:
        scores["beginner_friendly"] = max(0, scores["beginner_friendly"] - 20)
    
    # ========== 4. í•™ìŠµ ê°€ì¹˜ (Learning Value) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ êµìœ¡ì  ê°€ì¹˜ë¥¼ ì œê³µí•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: êµìœ¡ì  í‚¤ì›Œë“œ + ë³¸ë¬¸ ê¸¸ì´ ë³´ë„ˆìŠ¤
    
    # êµìœ¡ì  í‚¤ì›Œë“œ ëª©ë¡ (14ê°œ)
    learning_keywords = [
        "ì„¤ëª…", "ì´ìœ ", "ë°°ê²½", "ê³¼ì •", "ë°©ë²•", "ì›ë¦¬", "ê°œë…", "ì˜ë¯¸",
        "ì˜í–¥", "íš¨ê³¼", "ê²°ê³¼", "ë¶„ì„", "ì „ë§", "ì˜ˆìƒ"
    ]
    learning_count = sum(1 for keyword in learning_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
    scores["learning_value"] = min(100, 50 + learning_count * 5)
    
    # ë³¸ë¬¸ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ë” ë§ì€ ë°°ê²½ ì •ë³´ì™€ ì„¤ëª…ì„ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    if content_len >= 1500:
        scores["learning_value"] = min(100, scores["learning_value"] + 20)  # ë§¤ìš° ê¸´ ê¸°ì‚¬: +20ì 
    elif content_len >= 800:
        scores["learning_value"] = min(100, scores["learning_value"] + 10)   # ê¸´ ê¸°ì‚¬: +10ì 
    
    # ========== 5. ì‹¤í–‰ ê°€ì¹˜ (Action Value) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì´ë‚˜ í–‰ë™ ì§€ì¹¨ì„ ì œê³µí•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: í–‰ë™ ì§€ì¹¨ í‚¤ì›Œë“œ + êµ¬ì²´ì  ìˆ«ì/ê¸°ê°„ ë³´ë„ˆìŠ¤
    
    # í–‰ë™ ì§€ì¹¨ í‚¤ì›Œë“œ ëª©ë¡ (14ê°œ)
    action_keywords = [
        "ê¶Œì¥", "ì œì•ˆ", "ì¡°ì–¸", "ë°©ì•ˆ", "ëŒ€ì±…", "ì „ëµ", "ê³„íš", "ë°©ë²•",
        "í•´ì•¼", "í•„ìš”", "ì¤‘ìš”", "ì£¼ì˜", "ê²½ê³ ", "ì‹œì‚¬ì "
    ]
    action_count = sum(1 for keyword in action_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
    scores["action_value"] = min(100, 50 + action_count * 5)
    
    # êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ê¸°ê°„ì´ ìˆìœ¼ë©´ ì‹¤í–‰ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    # íŒ¨í„´: ìˆ«ì + ë‹¨ìœ„(%, ì›, ì–µ, ë§Œ, ì¡°, ì¼, ì›”, ë…„)
    specific_numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°ì¼ì›”ë…„]', text))
    if specific_numbers >= 3:
        scores["action_value"] = min(100, scores["action_value"] + 15)  # êµ¬ì²´ì  ì •ë³´ ë³´ë„ˆìŠ¤
    
    # ========== ìµœì¢… ì ìˆ˜ ì •ê·œí™” ==========
    # ëª¨ë“  ì ìˆ˜ë¥¼ 0-100 ë²”ìœ„ë¡œ ì œí•œ (ì•ˆì „ì¥ì¹˜)
    for key in scores:
        scores[key] = max(0, min(100, scores[key]))
    
    return scores

def _render_search_result_news_popularity(df_view: pd.DataFrame):
    """ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„"""
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"].copy()
    selected_events = df_view[df_view["event_name"] == "news_selected_from_chat"].copy()
    
    if search_events.empty:
        return
    
    st.markdown("#### ğŸ” ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„")
    st.markdown("**ëª©ì **: ì±—ë´‡ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì–´ë–¤ ë‰´ìŠ¤ê°€ ê°€ì¥ ì¸ê¸° ìˆëŠ”ì§€ ë¶„ì„")
    
    # ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ ë‰´ìŠ¤ ID ìˆ˜ì§‘
    news_appearances = {}  # {news_id: {count: int, keywords: set}}
    
    for idx, row in search_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        if payload and "article_ids" in payload:
            article_ids = payload.get("article_ids", [])
            keyword = payload.get("keyword", "")
            
            for news_id in article_ids:
                if news_id:
                    news_id_str = str(news_id)
                    if news_id_str not in news_appearances:
                        news_appearances[news_id_str] = {"count": 0, "keywords": set()}
                    news_appearances[news_id_str]["count"] += 1
                    if keyword:
                        news_appearances[news_id_str]["keywords"].add(keyword)
    
    # ì‹¤ì œ í´ë¦­ëœ ë‰´ìŠ¤ ID ìˆ˜ì§‘
    news_clicks = {}  # {news_id: count}
    news_titles = {}  # {news_id: title}
    
    for idx, row in selected_events.iterrows():
        news_id = row.get("news_id")
        if news_id:
            news_id_str = str(news_id)
            news_clicks[news_id_str] = news_clicks.get(news_id_str, 0) + 1
            
            # ì œëª© ì •ë³´ ìˆ˜ì§‘ (payloadì—ì„œ ë¨¼ì € ì‹œë„)
            payload = _parse_payload(row.get("payload"))
            if payload and "title" in payload and news_id_str not in news_titles:
                news_titles[news_id_str] = payload.get("title", "")
    
    # Supabase news í…Œì´ë¸”ì—ì„œ ì œëª© ê°€ì ¸ì˜¤ê¸° (payloadì— ì—†ëŠ” ê²½ìš°)
    all_news_ids = set(news_appearances.keys()) | set(news_clicks.keys())
    missing_title_ids = [nid for nid in all_news_ids if nid not in news_titles]
    
    if missing_title_ids:
        try:
            # Supabaseì—ì„œ ë‰´ìŠ¤ ì œëª© ê°€ì ¸ì˜¤ê¸°
            news_df = _fetch_news_from_supabase(limit=10000)  # ì¶©ë¶„íˆ ë§ì´ ê°€ì ¸ì˜¤ê¸°
            if not news_df.empty and "news_id" in news_df.columns and "title" in news_df.columns:
                # news_idë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­ (ì–‘ë°©í–¥ ë³€í™˜ ì‹œë„)
                news_df["news_id_str"] = news_df["news_id"].astype(str)
                # ì •ìˆ˜ë¡œë„ ë³€í™˜ ì‹œë„ (news_idê°€ ì •ìˆ˜ì¸ ê²½ìš°)
                try:
                    news_df["news_id_int"] = news_df["news_id"].astype(int)
                except:
                    news_df["news_id_int"] = None
                
                for news_id_str in missing_title_ids:
                    matched_news = None
                    
                    # ë¬¸ìì—´ë¡œ ë¨¼ì € ë§¤ì¹­ ì‹œë„
                    matched_news = news_df[news_df["news_id_str"] == news_id_str]
                    
                    # ë¬¸ìì—´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­ ì‹œë„
                    if matched_news.empty:
                        try:
                            news_id_int = int(news_id_str)
                            if "news_id_int" in news_df.columns:
                                matched_news = news_df[news_df["news_id_int"] == news_id_int]
                        except (ValueError, TypeError):
                            pass
                    
                    if not matched_news.empty:
                        title = matched_news.iloc[0].get("title", "")
                        if title and pd.notna(title) and str(title).strip():
                            news_titles[news_id_str] = str(title).strip()
        except Exception as e:
            # Supabase ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            import traceback
            print(f"âš ï¸ Supabaseì—ì„œ ë‰´ìŠ¤ ì œëª© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            print(f"ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰ (payloadì—ì„œ ê°€ì ¸ì˜¨ ì œëª©ë§Œ ì‚¬ìš©)
    
    # ì¸ê¸° ë‰´ìŠ¤ ë¶„ì„ ë°ì´í„° ìƒì„±
    if news_appearances:
        popularity_data = []
        for news_id, data in news_appearances.items():
            appearance_count = data["count"]
            click_count = news_clicks.get(news_id, 0)
            click_rate = (click_count / appearance_count * 100) if appearance_count > 0 else 0
            title = news_titles.get(news_id, f"ë‰´ìŠ¤ ID: {news_id}")
            
            popularity_data.append({
                "news_id": news_id,
                "ì œëª©": title[:50] + "..." if len(title) > 50 else title,
                "ê²€ìƒ‰ ê²°ê³¼ í¬í•¨": appearance_count,
                "í´ë¦­ ìˆ˜": click_count,
                "í´ë¦­ë¥  (%)": round(click_rate, 1)
            })
        
        if popularity_data:
            popularity_df = pd.DataFrame(popularity_data)
            popularity_df = popularity_df.sort_values("í´ë¦­ ìˆ˜", ascending=False)
            
            # ì£¼ìš” ë©”íŠ¸ë¦­
            col1, col2, col3 = st.columns(3)
            with col1:
                total_searches = len(search_events)
                st.metric("ì´ ê²€ìƒ‰ ì‹¤í–‰", f"{total_searches:,}ê±´")
            with col2:
                unique_news = len(news_appearances)
                st.metric("ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ ë‰´ìŠ¤", f"{unique_news:,}ê°œ")
            with col3:
                total_clicks = sum(news_clicks.values())
                st.metric("ì´ í´ë¦­ ìˆ˜", f"{total_clicks:,}ê±´")
            
            # Top 10 ì¸ê¸° ë‰´ìŠ¤
            top_10_df = popularity_df.head(10)
            
            if px is not None and len(top_10_df) > 0:
                fig = px.bar(
                    top_10_df,
                    x="í´ë¦­ ìˆ˜",
                    y="ì œëª©",
                    orientation='h',
                    title="ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì¥ ë§ì´ í´ë¦­ëœ ë‰´ìŠ¤ Top 10",
                    labels={"í´ë¦­ ìˆ˜": "í´ë¦­ ìˆ˜", "ì œëª©": "ë‰´ìŠ¤ ì œëª©"},
                    hover_data=["ê²€ìƒ‰ ê²°ê³¼ í¬í•¨", "í´ë¦­ë¥  (%)"]
                )
                st.plotly_chart(fig, use_container_width=True)
            
            st.dataframe(popularity_df, use_container_width=True, height=400)
    else:
        st.info("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _render_url_parsing_quality_for_content(df_view: pd.DataFrame):
    """ì½˜í…ì¸  í’ˆì§ˆ íƒ­ìš© URL íŒŒì‹± í’ˆì§ˆ"""
    url_events = df_view[df_view["event_name"].isin(["news_url_added_from_chat", "news_url_add_error"])]
    
    if url_events.empty:
        st.info("ğŸ“Š URL íŒŒì‹± ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.markdown("#### ğŸ“° URL íŒŒì‹± í’ˆì§ˆ (í¬ë¡¤ë§/íŒŒì‹± ì‹¤íŒ¨ìœ¨)")
    
    success_count = int((url_events["event_name"] == "news_url_added_from_chat").sum())
    error_count = int((url_events["event_name"] == "news_url_add_error").sum())
    total_count = success_count + error_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("íŒŒì‹± ì„±ê³µ", success_count)
    with col2:
        st.metric("íŒŒì‹± ì‹¤íŒ¨", error_count)
    with col3:
        if total_count > 0:
            failure_rate = (error_count / total_count) * 100
            st.metric("ì‹¤íŒ¨ìœ¨", f"{failure_rate:.1f}%")
        else:
            st.metric("ì‹¤íŒ¨ìœ¨", "N/A")
    
    # ì‹œê°„ëŒ€ë³„ ì‹¤íŒ¨ìœ¨ ì¶”ì´
    if total_count > 0:
        url_events_copy = url_events.copy()
        url_events_copy["hour"] = url_events_copy["event_time"].dt.floor("H")
        
        # ì„±ê³µ/ì‹¤íŒ¨ë³„ë¡œ ê·¸ë£¹í™”
        success_by_hour = url_events_copy[url_events_copy["event_name"] == "news_url_added_from_chat"].groupby("hour").size().reset_index(name="ì„±ê³µ")
        error_by_hour = url_events_copy[url_events_copy["event_name"] == "news_url_add_error"].groupby("hour").size().reset_index(name="ì‹¤íŒ¨")
        
        # ë³‘í•©
        hourly_stats = success_by_hour.merge(error_by_hour, on="hour", how="outer").fillna(0)
        
        if len(hourly_stats) > 0 and px is not None:
            hourly_stats["ì‹¤íŒ¨ìœ¨"] = (hourly_stats["ì‹¤íŒ¨"] / (hourly_stats["ì„±ê³µ"] + hourly_stats["ì‹¤íŒ¨"]) * 100).fillna(0)
            fig = px.line(
                hourly_stats,
                x="hour",
                y="ì‹¤íŒ¨ìœ¨",
                title="ì‹œê°„ëŒ€ë³„ URL íŒŒì‹± ì‹¤íŒ¨ìœ¨ ì¶”ì´"
            )
            st.plotly_chart(fig, use_container_width=True)

def _render_prompt_tuning_comparison(news_df: pd.DataFrame):
    """í”„ë¡¬í”„íŠ¸ íŠœë‹ìš© ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë¹„êµ ë¸”ë¡"""
    st.markdown("### 4ï¸âƒ£ ìƒ˜í”Œ ê¸°ì‚¬ ìƒì„¸ ë¹„êµ (í”„ë¡¬í”„íŠ¸ íŠœë‹ìš©)")
    st.markdown("**ëª©ì **: ë‰´ìŠ¤ ì›ë¬¸ + LLM ìš”ì•½ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬ í”„ë¡¬í”„íŠ¸ ê°œì„  í¬ì¸íŠ¸ ë°œê²¬")
    
    if news_df.empty:
        st.info("ğŸ“Š ë¹„êµí•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í•„í„°ë§ UI
    st.markdown("#### ğŸ” ê¸°ì‚¬ í•„í„°ë§")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # ë‚ ì§œ í•„í„°
        if "published_at" in news_df.columns:
            news_with_date = news_df[news_df["published_at"].notna()].copy()
            if not news_with_date.empty:
                min_date = news_with_date["published_at"].min().date()
                max_date = news_with_date["published_at"].max().date()
                date_range = st.date_input(
                    "ë°œí–‰ì¼ ë²”ìœ„",
                    value=(min_date, max_date),
                    min_value=min_date,
                    max_value=max_date,
                    key="prompt_tuning_date_range"
                )
            else:
                date_range = None
        else:
            date_range = None
    
    with col2:
        # ì–¸ë¡ ì‚¬ í•„í„°
        if "source" in news_df.columns:
            sources = ["ì „ì²´"] + sorted(news_df[news_df["source"].notna()]["source"].unique().tolist())
            selected_source = st.selectbox("ì–¸ë¡ ì‚¬", sources, key="prompt_tuning_source")
        else:
            selected_source = "ì „ì²´"
    
    with col3:
        # ë³¸ë¬¸ ê¸¸ì´ í•„í„°
        if "content" in news_df.columns:
            news_with_content = news_df[news_df["content"].notna() & (news_df["content"] != "")].copy()
            if not news_with_content.empty:
                news_with_content["content_length"] = news_with_content["content"].astype(str).str.len()
                min_len = int(news_with_content["content_length"].min())
                max_len = int(news_with_content["content_length"].max())
                length_range = st.slider(
                    "ë³¸ë¬¸ ê¸¸ì´ (ì)",
                    min_value=min_len,
                    max_value=max_len,
                    value=(min_len, max_len),
                    key="prompt_tuning_length"
                )
            else:
                length_range = None
        else:
            length_range = None
    
    with col4:
        # ê¸ˆìœµ/ë¹„ê¸ˆìœµ í•„í„°
        if "is_finance_news" in news_df.columns:
            category_options = ["ì „ì²´", "ê¸ˆìœµ", "ë¹„ê¸ˆìœµ"]
            selected_category = st.selectbox("ì¹´í…Œê³ ë¦¬", category_options, key="prompt_tuning_category")
        else:
            selected_category = "ì „ì²´"
    
    # í•„í„°ë§ ì ìš©
    filtered_df = news_df.copy()
    
    # ë‚ ì§œ í•„í„°
    if date_range and len(date_range) == 2 and "published_at" in filtered_df.columns:
        start_date, end_date = date_range
        filtered_df = filtered_df[
            (pd.to_datetime(filtered_df["published_at"]).dt.date >= start_date) &
            (pd.to_datetime(filtered_df["published_at"]).dt.date <= end_date)
        ]
    
    # ì–¸ë¡ ì‚¬ í•„í„°
    if selected_source != "ì „ì²´" and "source" in filtered_df.columns:
        filtered_df = filtered_df[filtered_df["source"] == selected_source]
    
    # ê¸¸ì´ í•„í„°
    if length_range and "content" in filtered_df.columns:
        filtered_df = filtered_df[
            filtered_df["content"].notna() & (filtered_df["content"] != "")
        ].copy()
        filtered_df["content_length"] = filtered_df["content"].astype(str).str.len()
        filtered_df = filtered_df[
            (filtered_df["content_length"] >= length_range[0]) &
            (filtered_df["content_length"] <= length_range[1])
        ]
    
    # ì¹´í…Œê³ ë¦¬ í•„í„°
    if selected_category != "ì „ì²´" and "is_finance_news" in filtered_df.columns:
        if selected_category == "ê¸ˆìœµ":
            filtered_df = filtered_df[filtered_df["is_finance_news"] == True]
        else:
            filtered_df = filtered_df[filtered_df["is_finance_news"] == False]
    
    if filtered_df.empty:
        st.warning("âš ï¸ í•„í„° ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.info(f"ğŸ“Š í•„í„°ë§ëœ ê¸°ì‚¬: {len(filtered_df):,}ê±´")
    
    # ìš”ì•½ í’ˆì§ˆ í†µê³„ ë¶„ì„
    st.markdown("---")
    st.markdown("#### ğŸ“Š ìš”ì•½ í’ˆì§ˆ í†µê³„ ë¶„ì„")
    
    # í•„í„°ë§ëœ ê¸°ì‚¬ë“¤ì— ëŒ€í•´ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì‹¤í–‰
    quality_stats = {
        "ì œëª© ê·¸ëŒ€ë¡œ í¬í•¨": 0,
        "ìš”ì•½ ê³¼ë„í•˜ê²Œ ì¥í™©": 0,
        "ìš”ì•½ ë„ˆë¬´ ì§§ìŒ": 0,
        "ê¸ˆìœµ ë‰´ìŠ¤ ìˆ«ì ëˆ„ë½": 0,
        "ë³¸ë¬¸ ì•ë¶€ë¶„ ë³µì‚¬": 0,
        "í’ˆì§ˆ í†µê³¼": 0,
        "ìš”ì•½ ì—†ìŒ": 0
    }
    
    # ì²´í¬ë¦¬ìŠ¤íŠ¸ í•¨ìˆ˜
    def check_summary_quality(row, selected_category):
        """ê¸°ì‚¬ë³„ ìš”ì•½ í’ˆì§ˆ ì²´í¬"""
        title = row.get("title", "")
        content = row.get("content", "")
        summary = row.get("summary", "")
        
        if not content or not summary or pd.isna(summary) or str(summary).strip() == "":
            return "ìš”ì•½ ì—†ìŒ"
        
        content_str = str(content)
        summary_str = str(summary)
        
        # 1. ì œëª© ê·¸ëŒ€ë¡œ ë³µë¶™ ì—¬ë¶€
        if title and str(title) in summary_str:
            return "ì œëª© ê·¸ëŒ€ë¡œ í¬í•¨"
        
        # 2. ë³¸ë¬¸ ê¸¸ì´ ëŒ€ë¹„ ìš”ì•½ ê¸¸ì´
        content_len = len(content_str)
        summary_len = len(summary_str)
        if content_len > 0:
            summary_ratio = (summary_len / content_len) * 100
            if content_len < 500 and summary_ratio > 50:
                return "ìš”ì•½ ê³¼ë„í•˜ê²Œ ì¥í™©"
            elif summary_ratio < 5:
                return "ìš”ì•½ ë„ˆë¬´ ì§§ìŒ"
        
        # 3. ìˆ«ì/ë³€í™”í­ í¬í•¨ ì—¬ë¶€ (ê¸ˆìœµ/ì •ì±… ë‰´ìŠ¤)
        if selected_category == "ê¸ˆìœµ":
            numbers_in_content = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', content_str))
            numbers_in_summary = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', summary_str))
            if numbers_in_content > 0 and numbers_in_summary == 0:
                return "ê¸ˆìœµ ë‰´ìŠ¤ ìˆ«ì ëˆ„ë½"
        
        # 4. ë³¸ë¬¸ ì•ë¶€ë¶„ë§Œ ë³µì‚¬ ì—¬ë¶€
        if content_str[:200] in summary_str:
            return "ë³¸ë¬¸ ì•ë¶€ë¶„ ë³µì‚¬"
        
        return "í’ˆì§ˆ í†µê³¼"
    
    # í•„í„°ë§ëœ ëª¨ë“  ê¸°ì‚¬ì— ëŒ€í•´ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì‹¤í–‰
    for idx, row in filtered_df.iterrows():
        quality_result = check_summary_quality(row, selected_category)
        if quality_result in quality_stats:
            quality_stats[quality_result] += 1
    
    # í†µê³„ í‘œì‹œ
    total_checked = sum(quality_stats.values())
    if total_checked > 0:
        # íŒŒì´ ê·¸ë˜í”„ìš© ë°ì´í„° ì¤€ë¹„
        quality_items = []
        quality_counts = []
        
        for key, count in quality_stats.items():
            if count > 0:
                quality_items.append(key)
                quality_counts.append(count)
        
        if quality_items and px is not None:
            quality_df = pd.DataFrame({
                "í•­ëª©": quality_items,
                "ê±´ìˆ˜": quality_counts
            })
            
            # íŒŒì´ ê·¸ë˜í”„ ìƒì„±
            fig = px.pie(
                quality_df,
                values="ê±´ìˆ˜",
                names="í•­ëª©",
                title="ìš”ì•½ í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª©ë³„ ë¹„ìœ¨",
                hole=0.4  # ë„ë„› ì°¨íŠ¸
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # í†µê³„ í…Œì´ë¸”
        st.markdown("##### ğŸ“‹ ìƒì„¸ í†µê³„")
        stats_df = pd.DataFrame([
            {"í•­ëª©": key, "ê±´ìˆ˜": count, "ë¹„ìœ¨": f"{(count/total_checked*100):.1f}%"}
            for key, count in quality_stats.items() if count > 0
        ])
        st.dataframe(stats_df, use_container_width=True)
    else:
        st.info("ğŸ“Š ìš”ì•½ì´ ìˆëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ê¸°ì‚¬ ì„ íƒ UI
    st.markdown("---")
    st.markdown("#### ğŸ“° ê¸°ì‚¬ ì„ íƒ")
    
    # ê¸°ì‚¬ ì„ íƒì„ ìœ„í•œ selectbox
    if "title" in filtered_df.columns:
        # ì œëª©ìœ¼ë¡œ ì„ íƒ
        if "news_id" in filtered_df.columns:
            titles_with_id = [f"[{row['news_id']}] {row['title']}" if pd.notna(row['title']) else f"[{row['news_id']}] ì œëª© ì—†ìŒ" 
                             for _, row in filtered_df.iterrows()]
        else:
            titles_with_id = filtered_df["title"].fillna("ì œëª© ì—†ìŒ").tolist()
        
        selected_index = st.selectbox(
            "ê¸°ì‚¬ ì„ íƒ",
            range(len(titles_with_id)),
            format_func=lambda x: titles_with_id[x][:100] + "..." if len(titles_with_id[x]) > 100 else titles_with_id[x],
            key="prompt_tuning_selected_article"
        )
        
        selected_article = filtered_df.iloc[selected_index].to_dict()
    else:
        st.warning("âš ï¸ ì œëª© ì»¬ëŸ¼ì´ ì—†ì–´ ê¸°ì‚¬ë¥¼ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¢Œìš° ë¹„êµ í™”ë©´
    st.markdown("---")
    st.markdown("#### ğŸ“Š ì›ë¬¸ vs ìš”ì•½ ë¹„êµ")
    
    col_left, col_right = st.columns(2)
    
    with col_left:
        st.markdown("##### ğŸ“„ ì›ë¬¸")
        
        # ì œëª©
        title = selected_article.get("title", "ì œëª© ì—†ìŒ")
        st.markdown(f"**{title}**")
        
        # ë©”íƒ€ ì •ë³´
        meta_info = []
        if "source" in selected_article and pd.notna(selected_article.get("source")):
            meta_info.append(f"ì¶œì²˜: {selected_article['source']}")
        if "published_at" in selected_article and pd.notna(selected_article.get("published_at")):
            pub_date = pd.to_datetime(selected_article["published_at"])
            meta_info.append(f"ë‚ ì§œ: {pub_date.strftime('%Y-%m-%d %H:%M')}")
        if "content" in selected_article:
            content_len = len(str(selected_article.get("content", "")))
            meta_info.append(f"ê¸¸ì´: {content_len:,}ì")
        
        if meta_info:
            st.caption(" | ".join(meta_info))
        
        # ë³¸ë¬¸
        content = selected_article.get("content", "")
        if content:
            st.markdown("**ë³¸ë¬¸:**")
            st.text_area(
                "ë³¸ë¬¸ ë‚´ìš©",
                value=str(content),
                height=400,
                disabled=True,
                key="original_content_display"
            )
        else:
            st.warning("âš ï¸ ë³¸ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    with col_right:
        st.markdown("##### ğŸ¤– LLM ìš”ì•½ ê²°ê³¼")
        
        # ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        summary = selected_article.get("summary", "")
        
        if summary and pd.notna(summary) and str(summary).strip():
            st.markdown("**ìš”ì•½:**")
            st.text_area(
                "ìš”ì•½ ë‚´ìš©",
                value=str(summary),
                height=400,
                disabled=True,
                key="summary_display"
            )
        else:
            st.info("ğŸ’¡ ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤. Supabase `summary` ì»¬ëŸ¼ì„ í™•ì¸í•˜ê±°ë‚˜ on-demand ìƒì„± ê¸°ëŠ¥ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.")
            st.caption("(í–¥í›„ on-demand ìš”ì•½ ìƒì„± ê¸°ëŠ¥ ì¶”ê°€ ì˜ˆì •)")
        
        # ìš”ì•½ í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸
        st.markdown("**ğŸ” í”„ë¡¬í”„íŠ¸ ê°œì„  ì²´í¬ë¦¬ìŠ¤íŠ¸:**")
        
        if content and summary:
            content_str = str(content)
            summary_str = str(summary)
            
            checks = []
            
            # 1. ì œëª© ê·¸ëŒ€ë¡œ ë³µë¶™ ì—¬ë¶€
            if title and title in summary_str:
                checks.append("âš ï¸ ì œëª©ì´ ìš”ì•½ì— ê·¸ëŒ€ë¡œ í¬í•¨ë¨ â†’ ì œëª© ì¬ì‘ì„± ê·œì¹™ ê°•í™” í•„ìš”")
            
            # 2. ë³¸ë¬¸ ê¸¸ì´ ëŒ€ë¹„ ìš”ì•½ ê¸¸ì´
            content_len = len(content_str)
            summary_len = len(summary_str)
            if content_len > 0:
                summary_ratio = (summary_len / content_len) * 100
                if content_len < 500 and summary_ratio > 50:
                    checks.append("âš ï¸ ì§§ì€ ê¸°ì‚¬ì¸ë° ìš”ì•½ì´ ê³¼ë„í•˜ê²Œ ì¥í™©í•¨ â†’ ë‹¨ì‹ ì€ í•´ì„ ì¤„ì´ê¸°")
                elif summary_ratio < 5:
                    checks.append("âš ï¸ ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŒ â†’ í•µì‹¬ ì •ë³´ í¬í•¨ ê°•í™”")
            
            # 3. ìˆ«ì/ë³€í™”í­ í¬í•¨ ì—¬ë¶€ (ê¸ˆìœµ/ì •ì±… ë‰´ìŠ¤)
            # selected_categoryê°€ "ê¸ˆìœµ"ì´ë©´ ì´ë¯¸ ê¸ˆìœµ ê¸°ì‚¬ë¡œ í•„í„°ë§ëœ ìƒíƒœ
            # is_finance_news ì»¬ëŸ¼ì´ ì—†ì–´ë„ selected_categoryë¡œ íŒë‹¨ ê°€ëŠ¥
            if selected_category == "ê¸ˆìœµ":
                numbers_in_content = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', content_str))
                numbers_in_summary = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', summary_str))
                if numbers_in_content > 0 and numbers_in_summary == 0:
                    checks.append("âš ï¸ ê¸ˆë¦¬/ì •ì±… ë‰´ìŠ¤ì¸ë° ìˆ«ì/ë³€í™”í­ì´ ìš”ì•½ì— ì—†ìŒ â†’ ìˆ«ì í•„ìˆ˜ ê·œì¹™ ì¶”ê°€")
            
            # 4. ë³¸ë¬¸ ì•ë¶€ë¶„ë§Œ ë³µì‚¬ ì—¬ë¶€
            if content_str[:200] in summary_str:
                checks.append("âš ï¸ ë³¸ë¬¸ ì•ë¶€ë¶„ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•œ ë“¯í•¨ â†’ ìš”ì•½ ì¬ì‘ì„± ê°•í™”")
            
            if checks:
                for check in checks:
                    st.warning(check)
            else:
                st.success("âœ… ê¸°ë³¸ì ì¸ í’ˆì§ˆ ì²´í¬ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤.")
        else:
            st.info("ğŸ’¡ ì›ë¬¸ê³¼ ìš”ì•½ì„ ë¹„êµí•˜ë ¤ë©´ ë‘˜ ë‹¤ í•„ìš”í•©ë‹ˆë‹¤.")

# ============================================================================
# íƒ­ 3: ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (User Behavior)
# ============================================================================

def _render_user_behavior_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° íƒ­: "ì‚¬ìš©ìê°€ ìš°ë¦¬ê°€ ë§Œë“  ê¸°ëŠ¥ì„ ì‹¤ì œë¡œ ì‚¬ìš©í•˜ê³  ìˆëŠ”ê°€?"
    
    ì£¼ìš” ë¶„ì„ í•­ëª©:
    - ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)
    - ê¸°ì‚¬ ì½ê¸° ì‹œê°„ (Dwell Time)
    - ìš”ì•½ í´ë¦­ë¥ 
    - ìš©ì–´ í´ë¦­ë¥  ë° ì¸ê¸° ìš©ì–´ Top 10
    - ìì—°ì–´ ê²€ìƒ‰ ì„±ê³µë¥ 
    - ê²€ìƒ‰ â†’ í´ë¦­ ì „í™˜ë¥ 
    - URL ì…ë ¥ ê¸°ëŠ¥ ì‚¬ìš©ë¥ 
    - ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬
    - ì¬ë°©ë¬¸ ì„¸ì…˜ ìˆ˜
    """
    st.markdown("### ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (User Behavior)")
    st.markdown("**ëª©í‘œ**: ì‚¬ìš©ìì˜ ì‹¤ì œ í–‰ë™ íŒ¨í„´ ë¶„ì„ - MVP ê¸°ëŠ¥ì˜ ê°€ì¹˜ ì—¬ë¶€ íŒë‹¨")
    
    # ì£¼ìš” ë©”íŠ¸ë¦­
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        news_clicks = int((df_view["event_name"] == "news_click").sum())
        st.metric("ë‰´ìŠ¤ í´ë¦­", news_clicks)
    with col2:
        detail_opens = int((df_view["event_name"] == "news_detail_open").sum())
        st.metric("ìƒì„¸ ì§„ì…", detail_opens)
        if news_clicks > 0:
            ctr = (detail_opens / news_clicks) * 100
            st.caption(f"ì§„ì…ë¥ : {ctr:.1f}%")
    with col3:
        chat_questions = int((df_view["event_name"] == "chat_question").sum())
        st.metric("ì±— ì§ˆë¬¸", chat_questions)
    with col4:
        search_requests = int((df_view["event_name"] == "news_search_from_chat").sum())
        st.metric("ë‰´ìŠ¤ ê²€ìƒ‰", search_requests)
    
    # 1. ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)
    _render_news_ctr(df_view)
    
    # 2. ê¸°ì‚¬ ìƒì„¸ ì§„ì…ë¥  (ìœ„ì— í¬í•¨)
    
    # 3. ê¸°ì‚¬ ì½ê¸° ì‹œê°„(dwell time)
    _render_dwell_time(df_view)
    
    # 4. ìš”ì•½ í´ë¦­ë¥ 
    _render_summary_clicks(df_view)
    
    # 5. ìš©ì–´ í´ë¦­ë¥ 
    _render_term_clicks(df_view)
    
    # 6. ìì—°ì–´ ê²€ìƒ‰ ì„±ê³µë¥ 
    _render_search_success_rate(df_view)
    
    # 7. ê²€ìƒ‰ â†’ í´ë¦­ ì „í™˜ë¥ 
    _render_search_to_click_conversion(df_view)
    
    # 8. URL ì…ë ¥ ê¸°ëŠ¥ ì‚¬ìš©ë¥ 
    _render_url_input_usage(df_view)
    
    # 9. ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬
    _render_chat_question_types(df_view)
    
    # 10. ì¬ë°©ë¬¸ ì„¸ì…˜ ìˆ˜
    _render_returning_sessions(df_view, session_column)

def _render_news_ctr(df_view: pd.DataFrame):
    """ë‰´ìŠ¤ í´ë¦­ë¥  ë¶„ì„"""
    st.markdown("#### ğŸ“Š ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)")
    
    clicks = df_view[df_view["event_name"] == "news_click"]
    views = df_view[df_view["event_name"].isin(["news_click", "news_detail_open"])]
    
    if len(views) > 0:
        ctr = (len(clicks) / len(views)) * 100
        st.metric("í´ë¦­ë¥ ", f"{ctr:.1f}%")
        
        # ì‹œê°„ëŒ€ë³„ CTR
        if len(clicks) > 0:
            clicks["hour"] = clicks["event_time"].dt.floor("H")
            hourly_clicks = clicks.groupby("hour").size().reset_index(name="í´ë¦­ ìˆ˜")
            
            if px is not None and len(hourly_clicks) > 0:
                fig = px.bar(
                    hourly_clicks,
                    x="hour",
                    y="í´ë¦­ ìˆ˜",
                    title="ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ í´ë¦­ ìˆ˜"
                )
                st.plotly_chart(fig, use_container_width=True)

def _render_dwell_time(df_view: pd.DataFrame):
    """ê¸°ì‚¬ ì½ê¸° ì‹œê°„ ë¶„ì„"""
    view_duration_events = df_view[df_view["event_name"] == "view_duration"]
    
    if view_duration_events.empty:
        return
    
    st.markdown("#### â±ï¸ ê¸°ì‚¬ ì½ê¸° ì‹œê°„ (Dwell Time)")
    
    duration_data = []
    for idx, row in view_duration_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        duration_sec = payload.get("duration_sec")
        if duration_sec is not None:
            duration_data.append({
                "event_time": row.get("event_time"),
                "ì½ê¸° ì‹œê°„ (ì´ˆ)": duration_sec,
            })
    
    if duration_data:
        duration_df = pd.DataFrame(duration_data)
        avg_duration = duration_df["ì½ê¸° ì‹œê°„ (ì´ˆ)"].mean()
        st.metric("í‰ê·  ì½ê¸° ì‹œê°„", f"{avg_duration:.1f}ì´ˆ")
        
        if px is not None and len(duration_df) > 0:
            fig = px.histogram(
                duration_df,
                x="ì½ê¸° ì‹œê°„ (ì´ˆ)",
                nbins=30,
                title="ê¸°ì‚¬ ì½ê¸° ì‹œê°„ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)

def _render_summary_clicks(df_view: pd.DataFrame):
    """ìš”ì•½ í´ë¦­ë¥  ë¶„ì„"""
    # ìš”ì•½ ê´€ë ¨ ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ ë¶„ì„
    summary_events = df_view[df_view["event_name"].str.contains("summary", case=False, na=False)]
    
    if summary_events.empty:
        return
    
    st.markdown("#### ğŸ“ ìš”ì•½ í´ë¦­ë¥ ")
    summary_clicks = len(summary_events)
    st.metric("ìš”ì•½ í´ë¦­", summary_clicks)

def _render_term_clicks(df_view: pd.DataFrame):
    """ìš©ì–´ í´ë¦­ë¥  ë¶„ì„"""
    term_clicks = df_view[df_view["event_name"] == "glossary_click"]
    term_answers = df_view[df_view["event_name"] == "glossary_answer"]
    
    if term_clicks.empty and term_answers.empty:
        return
    
    st.markdown("#### ğŸ’¡ ìš©ì–´ í´ë¦­ë¥ ")
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ìš©ì–´ í´ë¦­", len(term_clicks))
    with col2:
        st.metric("ìš©ì–´ ë‹µë³€", len(term_answers))
    
    # ì¸ê¸° ìš©ì–´ Top 10
    if not term_clicks.empty:
        term_list = []
        for idx, row in term_clicks.iterrows():
            term = _get_term_from_row(row)
            if term:
                term_list.append(term)
        
        if term_list:
            term_counts = pd.Series(term_list).value_counts().head(10)
            if px is not None and len(term_counts) > 0:
                fig = px.bar(
                    x=term_counts.index,
                    y=term_counts.values,
                    title="ì¸ê¸° ìš©ì–´ Top 10",
                    labels={"x": "ìš©ì–´", "y": "í´ë¦­ ìˆ˜"}
                )
                st.plotly_chart(fig, use_container_width=True)

def _render_search_success_rate(df_view: pd.DataFrame):
    """ìì—°ì–´ ê²€ìƒ‰ ì„±ê³µë¥ """
    search_success = df_view[df_view["event_name"] == "news_search_from_chat"]
    search_failed = df_view[df_view["event_name"] == "news_search_failed"]
    
    if search_success.empty and search_failed.empty:
        return
    
    st.markdown("#### ğŸ” ìì—°ì–´ ê²€ìƒ‰ ì„±ê³µë¥ ")
    
    success_count = len(search_success)
    failed_count = len(search_failed)
    total_count = success_count + failed_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê²€ìƒ‰ ì„±ê³µ", success_count)
    with col2:
        st.metric("ê²€ìƒ‰ ì‹¤íŒ¨", failed_count)
    with col3:
        if total_count > 0:
            success_rate = (success_count / total_count) * 100
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        else:
            st.metric("ì„±ê³µë¥ ", "N/A")

def _render_search_to_click_conversion(df_view: pd.DataFrame):
    """ê²€ìƒ‰ â†’ í´ë¦­ ì „í™˜ë¥ """
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"]
    selected_from_search = df_view[df_view["event_name"] == "news_selected_from_chat"]
    
    if search_events.empty:
        return
    
    st.markdown("#### ğŸ¯ ê²€ìƒ‰ â†’ í´ë¦­ ì „í™˜ë¥ ")
    
    search_count = len(search_events)
    selected_count = len(selected_from_search)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ê²€ìƒ‰ ì‹¤í–‰", search_count)
    with col2:
        st.metric("ê²€ìƒ‰ ê²°ê³¼ ì„ íƒ", selected_count)
        if search_count > 0:
            conversion_rate = (selected_count / search_count) * 100
            st.caption(f"ì „í™˜ìœ¨: {conversion_rate:.1f}%")

def _render_url_input_usage(df_view: pd.DataFrame):
    """URL ì…ë ¥ ê¸°ëŠ¥ ì‚¬ìš©ë¥ """
    url_added = df_view[df_view["event_name"] == "news_url_added_from_chat"]
    
    if url_added.empty:
        return
    
    st.markdown("#### ğŸ”— URL ì…ë ¥ ê¸°ëŠ¥ ì‚¬ìš©ë¥ ")
    
    url_count = len(url_added)
    total_sessions = df_view["session_id"].nunique() if "session_id" in df_view.columns else 0
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("URLë¡œ ì¶”ê°€ëœ ê¸°ì‚¬", url_count)
    with col2:
        if total_sessions > 0:
            usage_rate = (url_count / total_sessions) * 100
            st.metric("ì„¸ì…˜ë‹¹ ì‚¬ìš©ë¥ ", f"{usage_rate:.1f}%")
        else:
            st.metric("ì„¸ì…˜ë‹¹ ì‚¬ìš©ë¥ ", "N/A")

def _render_chat_question_types(df_view: pd.DataFrame):
    """ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬"""
    chat_events = df_view[df_view["event_name"].isin([
        "chat_question", "glossary_answer", "chat_response", "news_search_from_chat", "news_url_added_from_chat"
    ])]
    
    if chat_events.empty:
        return
    
    st.markdown("#### ğŸ’¬ ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬")
    
    question_types = {
        "ìš©ì–´ ì§ˆë¬¸": len(df_view[df_view["event_name"] == "glossary_answer"]),
        "ì¼ë°˜ ì§ˆë¬¸": len(df_view[df_view["event_name"] == "chat_response"]),
        "ë‰´ìŠ¤ ê²€ìƒ‰": len(df_view[df_view["event_name"] == "news_search_from_chat"]),
        "URL ì…ë ¥": len(df_view[df_view["event_name"] == "news_url_added_from_chat"]),
    }
    
    question_types = {k: v for k, v in question_types.items() if v > 0}
    
    if question_types:
        type_df = pd.DataFrame(list(question_types.items()), columns=["íƒ€ì…", "ê±´ìˆ˜"])
        
        if px is not None:
            fig = px.pie(
                type_df,
                values="ê±´ìˆ˜",
                names="íƒ€ì…",
                title="ì±—ë´‡ ì§ˆë¬¸ íƒ€ì… ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(type_df, use_container_width=True)

def _render_returning_sessions(df_view: pd.DataFrame, session_column: str):
    """ì¬ë°©ë¬¸ ì„¸ì…˜ ìˆ˜"""
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### ğŸ”„ ì¬ë°©ë¬¸ ì„¸ì…˜ ë¶„ì„")
    
    user_sessions = df_view.groupby("user_id")[session_column].nunique().reset_index()
    user_sessions.columns = ["user_id", "ì„¸ì…˜ ìˆ˜"]
    
    total_users = len(user_sessions)
    returning_users = len(user_sessions[user_sessions["ì„¸ì…˜ ìˆ˜"] > 1])
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì´ ì‚¬ìš©ì", total_users)
    with col2:
        st.metric("ì¬ë°©ë¬¸ ì‚¬ìš©ì", returning_users)
        if total_users > 0:
            return_rate = (returning_users / total_users) * 100
            st.caption(f"ì¬ë°©ë¬¸ë¥ : {return_rate:.1f}%")
    
    if px is not None and len(user_sessions) > 0:
        fig = px.histogram(
            user_sessions,
            x="ì„¸ì…˜ ìˆ˜",
            nbins=20,
            title="ì‚¬ìš©ìë³„ ì„¸ì…˜ ìˆ˜ ë¶„í¬"
        )
        st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# Log Viewer íƒ­
# ============================================================================

def _render_log_viewer_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸ“ Log Viewer íƒ­
    â†’ ìƒì„¸ ì´ë²¤íŠ¸ ë¡œê·¸ ë·°ì–´
    """
    st.markdown("### ğŸ“ Log Viewer")
    st.markdown("**ëª©ì **: ìƒì„¸ ì´ë²¤íŠ¸ ë¡œê·¸ í™•ì¸ ë° ë¶„ì„")
    
    # í•„í„° ì˜µì…˜
    col1, col2, col3 = st.columns(3)
    with col1:
        event_filter = st.multiselect(
            "ì´ë²¤íŠ¸ í•„í„°",
            options=sorted(df_view["event_name"].unique()) if "event_name" in df_view.columns else [],
            default=[],
            key="log_viewer_event_filter"
        )
    with col2:
        user_filter = st.text_input(
            "ì‚¬ìš©ì ID í•„í„°",
            value="",
            key="log_viewer_user_filter"
        )
    with col3:
        limit = st.number_input(
            "í‘œì‹œí•  ë¡œê·¸ ìˆ˜",
            min_value=10,
            max_value=1000,
            value=100,
            step=10,
            key="log_viewer_limit"
        )
    
    # í•„í„° ì ìš©
    filtered_df = df_view.copy()
    
    if event_filter:
        filtered_df = filtered_df[filtered_df["event_name"].isin(event_filter)]
    
    if user_filter:
        filtered_df = filtered_df[filtered_df["user_id"].astype(str).str.contains(user_filter, case=False, na=False)]
    
    # ìµœì‹ ìˆœ ì •ë ¬
    filtered_df = filtered_df.sort_values("event_time", ascending=False).head(limit)
    
    # í†µê³„
    st.markdown("#### ğŸ“Š ë¡œê·¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ì´ ë¡œê·¸ ìˆ˜", f"{len(df_view):,}ê±´")
    with col2:
        st.metric("í•„í„°ëœ ë¡œê·¸", f"{len(filtered_df):,}ê±´")
    with col3:
        unique_events = filtered_df["event_name"].nunique() if "event_name" in filtered_df.columns else 0
        st.metric("ê³ ìœ  ì´ë²¤íŠ¸", f"{unique_events}ê°œ")
    with col4:
        unique_users = filtered_df["user_id"].nunique() if "user_id" in filtered_df.columns else 0
        st.metric("ê³ ìœ  ì‚¬ìš©ì", f"{unique_users}ëª…")
    
    st.markdown("---")
    
    # ë¡œê·¸ í…Œì´ë¸”
    st.markdown("#### ğŸ“‹ ì´ë²¤íŠ¸ ë¡œê·¸")
    
    # í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ
    display_cols = ["event_time", "event_name", "user_id", session_column]
    if "news_id" in filtered_df.columns:
        display_cols.append("news_id")
    if "term" in filtered_df.columns:
        display_cols.append("term")
    if "message" in filtered_df.columns:
        display_cols.append("message")
    if "latency_ms" in filtered_df.columns:
        display_cols.append("latency_ms")
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_cols = [col for col in display_cols if col in filtered_df.columns]
    
    if len(filtered_df) > 0:
        st.dataframe(
            filtered_df[available_cols],
            use_container_width=True,
            height=600
        )
        
        # Payload ìƒì„¸ ë³´ê¸° (ì„ íƒì )
        if st.checkbox("Payload ìƒì„¸ ë³´ê¸°", key="log_viewer_show_payload"):
            st.markdown("#### ğŸ” Payload ìƒì„¸")
            selected_index = st.selectbox(
                "ë¡œê·¸ ì„ íƒ",
                options=range(len(filtered_df)),
                format_func=lambda x: f"{filtered_df.iloc[x]['event_time']} - {filtered_df.iloc[x]['event_name']}" if x < len(filtered_df) else ""
            )
            
            if selected_index < len(filtered_df):
                selected_row = filtered_df.iloc[selected_index]
                payload = _parse_payload(selected_row.get("payload"))
                if payload:
                    st.json(payload)
                else:
                    st.info("Payloadê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ“­ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ============================================================================
# KPI ëŒ€ì‹œë³´ë“œ
# ============================================================================

def _render_kpi_dashboard(df_view: pd.DataFrame, session_column: str):
    """
    ğŸ“Š KPI ëŒ€ì‹œë³´ë“œ ë©”ì¸ í˜ì´ì§€
    â†’ "ê¸ˆìœµ ì´ˆë³´ìì˜ ë‰´ìŠ¤ ì´í•´"ë¥¼ ë•ëŠ” ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ì§€í‘œ ëª¨ë‹ˆí„°ë§
    """
    st.markdown("#### ğŸ“Š KPI ëŒ€ì‹œë³´ë“œ ìš”ì•½")
    st.markdown("**í•µì‹¬ ì§ˆë¬¸**: ì‚¬ìš©ìëŠ” ì‹¤ì œë¡œ ë‰´ìŠ¤ë¥¼ ì½ê³  ìˆëŠ”ê°€? ìš©ì–´/Glossary ê¸°ëŠ¥ì€ ì´í•´ë¥¼ ë•ê³  ìˆëŠ”ê°€? ì±—ë´‡ì€ ì§„ì§œ ì‚¬ìš©ë˜ëŠ”ê°€? ì„±ëŠ¥ì€ UXë¥¼ ë§ì¹˜ì§€ ì•ŠëŠ”ê°€?")
    
    # ë‚ ì§œ í•„í„°
    selected_start_date = None
    selected_end_date = None
    date_range_days = None
    
    if "event_time" in df_view.columns and not df_view.empty:
        df_view = df_view.copy()
        df_view["date"] = pd.to_datetime(df_view["event_time"]).dt.date
        df_view["datetime"] = pd.to_datetime(df_view["event_time"])
        df_view["hour"] = df_view["datetime"].dt.hour
        
        min_date = df_view["date"].min()
        max_date = df_view["date"].max()
        
        if min_date and max_date:
            date_range = st.date_input(
                "ê¸°ê°„ ì„ íƒ",
                value=(min_date, max_date),
                min_value=min_date,
                max_value=max_date,
                key="kpi_date_range"
            )
            
            if isinstance(date_range, tuple) and len(date_range) == 2:
                selected_start_date, selected_end_date = date_range
                df_view = df_view[(df_view["date"] >= selected_start_date) & (df_view["date"] <= selected_end_date)]
                date_range_days = (selected_end_date - selected_start_date).days + 1
    
    # ========== A. ìƒë‹¨ Summary (ë©”íŠ¸ë¦­ ì¹´ë“œ 9ê°œ) ==========
    st.markdown("#### ğŸ“ˆ í•µì‹¬ ì§€í‘œ ìš”ì•½")
    
    # 1. DAU / WAU ê³„ì‚° (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
    # DAU: ì˜¤ëŠ˜ ë‚ ì§œì— ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œí‚¨ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
    # WAU: ìµœê·¼ 7ì¼ê°„ ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œí‚¨ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
    # ì£¼ì˜: ë‚ ì§œ í•„í„°ê°€ ì ìš©ëœ ê²½ìš°ì—ë„ ì „ì²´ ê¸°ê°„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
    if "user_id" in df_view.columns and "date" in df_view.columns:
        # ì›ë³¸ ë°ì´í„°ì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ (í•„í„°ë§ ì „)
        # í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œ ê³„ì‚°
        today_kst = pd.Timestamp.now(tz="Asia/Seoul").date()
        week_ago = today_kst - pd.Timedelta(days=7)
        
        # DAU: ì˜¤ëŠ˜ ë‚ ì§œì˜ ê³ ìœ  ì‚¬ìš©ì ìˆ˜ (í•„í„°ë§ëœ df_view ê¸°ì¤€)
        dau = df_view[df_view["date"] == today_kst]["user_id"].nunique()
        
        # WAU: ìµœê·¼ 7ì¼ê°„ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
        # ë‚ ì§œ í•„í„°ê°€ ì ìš©ëœ ê²½ìš°, í•„í„° ë²”ìœ„ ë‚´ì—ì„œë§Œ ê³„ì‚°
        # í•˜ì§€ë§Œ ì›ë³¸ ë°ì´í„° ì „ì²´ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ render í•¨ìˆ˜ì—ì„œ ì „ì²´ ë°ì´í„°ë¥¼ ì „ë‹¬í•´ì•¼ í•¨
        wau_df = df_view[df_view["date"] >= week_ago]
        wau = wau_df["user_id"].nunique()
        
        # ë””ë²„ê¹… ì •ë³´ (í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜ì— í‘œì‹œ)
        with st.expander("ğŸ” DAU/WAU ê³„ì‚° ìƒì„¸ ì •ë³´", expanded=False):
            st.markdown(f"**ê³„ì‚° ê¸°ì¤€ ë‚ ì§œ**: {today_kst}")
            st.markdown(f"**7ì¼ ì „ ë‚ ì§œ**: {week_ago}")
            st.markdown(f"**ë°ì´í„° ë²”ìœ„**: {df_view['date'].min()} ~ {df_view['date'].max()}")
            st.markdown(f"**í•„í„°ë§ëœ ë°ì´í„° ê±´ìˆ˜**: {len(df_view):,}ê±´")
            st.markdown(f"**ìµœê·¼ 7ì¼ ë°ì´í„° ê±´ìˆ˜**: {len(wau_df):,}ê±´")
            st.markdown(f"**ì „ì²´ ê³ ìœ  user_id ìˆ˜**: {df_view['user_id'].nunique()}ëª…")
            st.markdown(f"**ìµœê·¼ 7ì¼ ê³ ìœ  user_id ìˆ˜**: {wau}ëª…")
            
            # user_id ëª©ë¡ í‘œì‹œ (ìƒìœ„ 20ê°œ)
            if wau > 0:
                st.markdown("**ìµœê·¼ 7ì¼ í™œë™ user_id ëª©ë¡ (ìƒìœ„ 20ê°œ)**:")
                user_counts = wau_df.groupby("user_id").size().sort_values(ascending=False).head(20)
                st.dataframe(user_counts.reset_index().rename(columns={0: "ì´ë²¤íŠ¸ ìˆ˜", "user_id": "User ID"}), use_container_width=True)
                
                st.info("ğŸ’¡ **ì°¸ê³ **: user_idëŠ” ë¸Œë¼ìš°ì € localStorageì— ì €ì¥ë©ë‹ˆë‹¤. ë¡œì»¬(`localhost`)ê³¼ ë°°í¬ ì‚¬ì´íŠ¸ëŠ” ë‹¤ë¥¸ ë„ë©”ì¸ì´ë¯€ë¡œ ê°ê° ë‹¤ë¥¸ user_idê°€ ìƒì„±ë©ë‹ˆë‹¤. ê°™ì€ ì‚¬ìš©ìê°€ ë¡œì»¬ê³¼ ë°°í¬ ì‚¬ì´íŠ¸ì—ì„œ ì ‘ì†í•˜ë©´ 2ê°œì˜ user_idë¡œ ì§‘ê³„ë©ë‹ˆë‹¤.")
    else:
        dau = 0
        wau = 0
    
    # 2. í‰ê·  ì„¸ì…˜ ê¸¸ì´ ê³„ì‚°
    # ê° ì„¸ì…˜ì˜ ì²« ì´ë²¤íŠ¸ì™€ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì‹œê°„ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ í‰ê· 
    if session_column in df_view.columns and "event_time" in df_view.columns:
        session_durations = []
        for session_id in df_view[session_column].dropna().unique():
            session_events = df_view[df_view[session_column] == session_id]
            if len(session_events) > 1:
                session_start = session_events["event_time"].min()
                session_end = session_events["event_time"].max()
                duration = (session_end - session_start).total_seconds() / 60  # ë¶„ ë‹¨ìœ„
                if duration > 0:
                    session_durations.append(duration)
        avg_session_length = sum(session_durations) / len(session_durations) if session_durations else 0
    else:
        avg_session_length = 0
    
    # 3. ì„¸ì…˜ë‹¹ ë‰´ìŠ¤ í´ë¦­ ìˆ˜
    news_clicks = int((df_view["event_name"] == "news_click").sum())
    total_sessions = df_view[session_column].nunique() if session_column in df_view.columns else 1
    news_clicks_per_session = news_clicks / total_sessions if total_sessions > 0 else 0
    
    # 4. Glossary ì‚¬ìš© ì„¸ì…˜ ë¹„ìœ¨
    glossary_sessions = df_view[df_view["event_name"] == "glossary_click"][session_column].nunique() if session_column in df_view.columns else 0
    glossary_usage_rate = (glossary_sessions / total_sessions * 100) if total_sessions > 0 else 0
    
    # 5. ì±—ë´‡ ì‚¬ìš© ì„¸ì…˜ ë¹„ìœ¨
    chat_sessions = df_view[df_view["event_name"].isin(["chat_question", "chat_response"])][session_column].nunique() if session_column in df_view.columns else 0
    chat_usage_rate = (chat_sessions / total_sessions * 100) if total_sessions > 0 else 0
    
    # 6. í‰ê·  ë‰´ìŠ¤ í•˜ì´ë¼ì´íŠ¸ ì†ë„ (ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ êµ¬ë¶„)
    # news_detail_open ì´ë²¤íŠ¸ì—ì„œ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ ì‹œê°„ ì¶”ì¶œ
    # ìºì‹œ íˆíŠ¸ëŠ” ë§¤ìš° ë¹ ë¥´ë¯€ë¡œ(3ms ì •ë„) ë³„ë„ë¡œ êµ¬ë¶„í•˜ì—¬ í‘œì‹œ
    detail_events = df_view[df_view["event_name"] == "news_detail_open"].copy()
    highlight_latencies = []
    highlight_latencies_cache_miss = []  # ìºì‹œ ë¯¸ìŠ¤ë§Œ (ì‹¤ì œ ì„±ëŠ¥ ë¬¸ì œ íŒŒì•…ìš©)
    highlight_latencies_cache_hit = []   # ìºì‹œ íˆíŠ¸ë§Œ (ì°¸ê³ ìš©)
    
    for idx, row in detail_events.iterrows():
        perf_data = _extract_perf_data(row)
        if perf_data and isinstance(perf_data, dict):
            highlight_ms = perf_data.get("highlight_ms")
            cache_hit = perf_data.get("highlight_cache_hit", False)
            
            if highlight_ms is not None:
                try:
                    highlight_value = float(highlight_ms)
                    # ìœ íš¨í•œ ê°’ë§Œ ì¶”ê°€ (0ë³´ë‹¤ í¬ê³  í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´)
                    if highlight_value > 0 and highlight_value < 100000:  # 100ì´ˆ ì´ìƒì€ ì œì™¸
                        highlight_latencies.append(highlight_value)
                        # ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ êµ¬ë¶„
                        if cache_hit:
                            highlight_latencies_cache_hit.append(highlight_value)
                        else:
                            highlight_latencies_cache_miss.append(highlight_value)
                except (ValueError, TypeError):
                    pass
    
    avg_highlight_latency = sum(highlight_latencies) / len(highlight_latencies) if highlight_latencies else None
    avg_highlight_cache_miss = sum(highlight_latencies_cache_miss) / len(highlight_latencies_cache_miss) if highlight_latencies_cache_miss else None
    avg_highlight_cache_hit = sum(highlight_latencies_cache_hit) / len(highlight_latencies_cache_hit) if highlight_latencies_cache_hit else None
    
    # ë©”íŠ¸ë¦­ ì¹´ë“œ í‘œì‹œ
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("DAU", f"{dau}ëª…")
        st.metric("WAU", f"{wau}ëª…")
    with col2:
        st.metric("í‰ê·  ì„¸ì…˜ ê¸¸ì´", f"{avg_session_length:.1f}ë¶„")
        st.metric("ì„¸ì…˜ë‹¹ ë‰´ìŠ¤ í´ë¦­", f"{news_clicks_per_session:.1f}ê±´")
    with col3:
        st.metric("Glossary ì‚¬ìš© ì„¸ì…˜", f"{glossary_usage_rate:.1f}%")
        st.metric("ì±—ë´‡ ì‚¬ìš© ì„¸ì…˜", f"{chat_usage_rate:.1f}%")
    
    col4, col5, col6 = st.columns(3)
    with col4:
        if avg_highlight_latency is not None:
            st.metric("í‰ê·  í•˜ì´ë¼ì´íŠ¸ ì†ë„", f"{avg_highlight_latency:.0f}ms")
            # ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ ì •ë³´ í‘œì‹œ
            if avg_highlight_cache_hit is not None and avg_highlight_cache_miss is not None:
                st.caption(f"ìºì‹œ íˆíŠ¸: {avg_highlight_cache_hit:.0f}ms | ìºì‹œ ë¯¸ìŠ¤: {avg_highlight_cache_miss:.0f}ms")
            elif avg_highlight_cache_hit is not None:
                st.caption(f"ìºì‹œ íˆíŠ¸: {avg_highlight_cache_hit:.0f}ms")
            elif avg_highlight_cache_miss is not None:
                st.caption(f"ìºì‹œ ë¯¸ìŠ¤: {avg_highlight_cache_miss:.0f}ms")
        else:
            st.metric("í‰ê·  í•˜ì´ë¼ì´íŠ¸ ì†ë„", "N/A")
    
    # 95 percentile latency
    if highlight_latencies:
        p95_latency = pd.Series(highlight_latencies).quantile(0.95)
        with col5:
            st.metric("95% í•˜ì´ë¼ì´íŠ¸ ì†ë„", f"{p95_latency:.0f}ms")
    else:
        with col5:
            st.metric("95% í•˜ì´ë¼ì´íŠ¸ ì†ë„", "N/A")
    
    # ì±—ë´‡ ì‘ë‹µ ì†ë„
    chat_response_events = df_view[df_view["event_name"].isin(["chat_response", "glossary_answer"])].copy()
    chat_latencies = []
    for idx, row in chat_response_events.iterrows():
        # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ latency_ms ì¶”ì¶œ ì‹œë„
        latency_ms = None
        
        # 1. perf_dataì—ì„œ ì¶”ì¶œ
        perf_data = _extract_perf_data(row)
        if perf_data and isinstance(perf_data, dict):
            latency_ms = perf_data.get("latency_ms")
            # latency_msê°€ ì—†ìœ¼ë©´ total_ms ì‚¬ìš©
            if latency_ms is None:
                latency_ms = perf_data.get("total_ms")
        
        # 2. payloadì—ì„œ ì§ì ‘ ì¶”ì¶œ
        if latency_ms is None:
            payload = _parse_payload(row.get("payload"))
            if payload:
                latency_ms = payload.get("latency_ms")
                if latency_ms is None:
                    latency_ms = payload.get("total_ms")
        
        # 3. ì»¬ëŸ¼ì—ì„œ ì§ì ‘ ì¶”ì¶œ
        if latency_ms is None:
            latency_ms = row.get("latency_ms")
        
        if latency_ms is not None:
            try:
                latency_value = float(latency_ms)
                # ìœ íš¨í•œ ê°’ë§Œ ì¶”ê°€ (0ë³´ë‹¤ í¬ê³  í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´)
                if latency_value > 0 and latency_value < 1000000:  # 1000ì´ˆ ì´ìƒì€ ì œì™¸
                    chat_latencies.append(latency_value)
            except (ValueError, TypeError):
                pass
    avg_chat_latency = sum(chat_latencies) / len(chat_latencies) if chat_latencies else None
    with col6:
        if avg_chat_latency is not None:
            st.metric("í‰ê·  ì±—ë´‡ ì‘ë‹µ ì†ë„", f"{avg_chat_latency:.0f}ms")
        else:
            st.metric("í‰ê·  ì±—ë´‡ ì‘ë‹µ ì†ë„", "N/A")
    
    st.markdown("---")
    
    # ========== B. ì´ìš© íŒ¨í„´ (Line Chart) ==========
    st.markdown("#### ğŸ“ˆ ì´ìš© íŒ¨í„´ ì¶”ì´")
    
    # ì„ íƒí•œ ê¸°ê°„ì´ 1ì¼ ì´í•˜ë©´ ì‹œê°„ë³„, ê·¸ ì™¸ì—ëŠ” ì¼ìë³„ë¡œ í‘œì‹œ
    use_hourly = date_range_days is not None and date_range_days <= 1
    
    if "date" in df_view.columns and px is not None:
        if use_hourly and "hour" in df_view.columns:
            # ì‹œê°„ë³„ ì¶”ì´ (í•˜ë£¨ë§Œ ì„ íƒí•œ ê²½ìš°)
            # 1. ì‹œê°„ë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´
            hourly_news_clicks = df_view[df_view["event_name"] == "news_click"].groupby("hour").size().reset_index(name="í´ë¦­ ìˆ˜")
            
            if len(hourly_news_clicks) > 0:
                hourly_news_clicks = hourly_news_clicks.sort_values("hour")
                
                fig1 = px.line(
                    hourly_news_clicks,
                    x="hour",
                    y="í´ë¦­ ìˆ˜",
                    title="ì‹œê°„ë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´",
                    labels={"hour": "ì‹œê°„ (ì‹œ)", "í´ë¦­ ìˆ˜": "í´ë¦­ ìˆ˜"}
                )
                fig1.update_xaxes(tickmode='linear', tick0=0, dtick=3, tickformat='%Hì‹œ')
                st.plotly_chart(fig1, use_container_width=True)
                st.dataframe(hourly_news_clicks, use_container_width=True, height=200)
            
            # 2. ì‹œê°„ë³„ Glossary ì‚¬ìš© ì„¸ì…˜ ë¹„ìœ¨ ì¶”ì´
            hourly_sessions = df_view.groupby("hour")[session_column].nunique().reset_index(name="ì „ì²´ ì„¸ì…˜")
            hourly_glossary_sessions = df_view[df_view["event_name"] == "glossary_click"].groupby("hour")[session_column].nunique().reset_index(name="Glossary ì„¸ì…˜")
            
            if len(hourly_sessions) > 0 and len(hourly_glossary_sessions) > 0:
                hourly_usage = hourly_sessions.merge(hourly_glossary_sessions, on="hour", how="left")
                hourly_usage["Glossary ì„¸ì…˜"] = hourly_usage["Glossary ì„¸ì…˜"].fillna(0)
                hourly_usage["ì‚¬ìš© ë¹„ìœ¨ (%)"] = (hourly_usage["Glossary ì„¸ì…˜"] / hourly_usage["ì „ì²´ ì„¸ì…˜"] * 100).fillna(0)
                hourly_usage = hourly_usage.sort_values("hour")
                
                fig2 = px.line(
                    hourly_usage,
                    x="hour",
                    y="ì‚¬ìš© ë¹„ìœ¨ (%)",
                    title="ì‹œê°„ë³„ Glossary ì‚¬ìš© ì„¸ì…˜ ë¹„ìœ¨ ì¶”ì´",
                    labels={"hour": "ì‹œê°„ (ì‹œ)", "ì‚¬ìš© ë¹„ìœ¨ (%)": "ë¹„ìœ¨ (%)"}
                )
                fig2.update_xaxes(tickmode='linear', tick0=0, dtick=3, tickformat='%Hì‹œ')
                st.plotly_chart(fig2, use_container_width=True)
                st.dataframe(hourly_usage[["hour", "ì „ì²´ ì„¸ì…˜", "Glossary ì„¸ì…˜", "ì‚¬ìš© ë¹„ìœ¨ (%)"]], use_container_width=True, height=200)
        else:
            # ì¼ìë³„ ì¶”ì´ (ì—¬ëŸ¬ ë‚ ì§œ ì„ íƒí•œ ê²½ìš°)
            # 1. ì¼ìë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´
            daily_news_clicks = df_view[df_view["event_name"] == "news_click"].groupby("date").size().reset_index(name="í´ë¦­ ìˆ˜")
            
            if len(daily_news_clicks) > 0:
                # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ (ì‹œê°„ ì •ë³´ ì œê±°)
                daily_news_clicks["date_dt"] = pd.to_datetime(daily_news_clicks["date"])
                daily_news_clicks = daily_news_clicks.sort_values("date_dt")
                
                fig1 = px.line(
                    daily_news_clicks,
                    x="date_dt",
                    y="í´ë¦­ ìˆ˜",
                    title="ì¼ìë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´",
                    labels={"date_dt": "ë‚ ì§œ", "í´ë¦­ ìˆ˜": "í´ë¦­ ìˆ˜"}
                )
                # Xì¶• í¬ë§·ì„ ë‚ ì§œë§Œ í‘œì‹œí•˜ë„ë¡ ì„¤ì •
                fig1.update_xaxes(tickformat='%Y-%m-%d', dtick="D1")
                st.plotly_chart(fig1, use_container_width=True)
                # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
                display_df1 = daily_news_clicks.copy()
                display_df1["ë‚ ì§œ"] = display_df1["date"].astype(str)
                st.dataframe(display_df1[["ë‚ ì§œ", "í´ë¦­ ìˆ˜"]], use_container_width=True, height=200)
            
            # 2. Glossary ì‚¬ìš© ì„¸ì…˜ ë¹„ìœ¨ ì¶”ì´
            daily_sessions = df_view.groupby("date")[session_column].nunique().reset_index(name="ì „ì²´ ì„¸ì…˜")
            daily_glossary_sessions = df_view[df_view["event_name"] == "glossary_click"].groupby("date")[session_column].nunique().reset_index(name="Glossary ì„¸ì…˜")
            
            if len(daily_sessions) > 0 and len(daily_glossary_sessions) > 0:
                daily_usage = daily_sessions.merge(daily_glossary_sessions, on="date", how="left")
                daily_usage["Glossary ì„¸ì…˜"] = daily_usage["Glossary ì„¸ì…˜"].fillna(0)
                daily_usage["ì‚¬ìš© ë¹„ìœ¨ (%)"] = (daily_usage["Glossary ì„¸ì…˜"] / daily_usage["ì „ì²´ ì„¸ì…˜"] * 100).fillna(0)
                # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ (ì‹œê°„ ì •ë³´ ì œê±°)
                daily_usage["date_dt"] = pd.to_datetime(daily_usage["date"])
                daily_usage = daily_usage.sort_values("date_dt")
                
                fig2 = px.line(
                    daily_usage,
                    x="date_dt",
                    y="ì‚¬ìš© ë¹„ìœ¨ (%)",
                    title="ì¼ìë³„ Glossary ì‚¬ìš© ì„¸ì…˜ ë¹„ìœ¨ ì¶”ì´",
                    labels={"date_dt": "ë‚ ì§œ", "ì‚¬ìš© ë¹„ìœ¨ (%)": "ë¹„ìœ¨ (%)"}
                )
                # Xì¶• í¬ë§·ì„ ë‚ ì§œë§Œ í‘œì‹œí•˜ë„ë¡ ì„¤ì •
                fig2.update_xaxes(tickformat='%Y-%m-%d', dtick="D1")
                st.plotly_chart(fig2, use_container_width=True)
                # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í‘œì‹œ
                display_df2 = daily_usage.copy()
                display_df2["ë‚ ì§œ"] = display_df2["date"].astype(str)
                st.dataframe(display_df2[["ë‚ ì§œ", "ì „ì²´ ì„¸ì…˜", "Glossary ì„¸ì…˜", "ì‚¬ìš© ë¹„ìœ¨ (%)"]], use_container_width=True, height=200)
    
    st.markdown("---")
    
    # ========== C. í–‰ë™ íë¦„ (Funnel Chart) ==========
    st.markdown("#### ğŸ”½ í–‰ë™ íë¦„ ë¶„ì„")
    
    # ë‰´ìŠ¤ â†’ Glossary â†’ ì±—ë´‡ ì „í™˜ í¼ë„ (ì„¸ì…˜ ê¸°ë°˜, ìˆœì°¨ì )
    # ìˆœì°¨ì  í¼ë„: ê° ë‹¨ê³„ëŠ” ì´ì „ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•œ ì„¸ì…˜ ì¤‘ì—ì„œë§Œ ê³„ì‚°
    # 1ë‹¨ê³„: ë‰´ìŠ¤ í´ë¦­í•œ ì„¸ì…˜ ìˆ˜
    news_click_sessions = df_view[df_view["event_name"] == "news_click"][session_column].nunique() if session_column in df_view.columns else 0
    
    if news_click_sessions > 0:
        # ê° ì´ë²¤íŠ¸ë³„ ì„¸ì…˜ ì§‘í•© ìƒì„± (êµì§‘í•© ê³„ì‚°ì„ ìœ„í•´ set ì‚¬ìš©)
        news_sessions = set(df_view[df_view["event_name"] == "news_click"][session_column].dropna().unique())
        glossary_sessions = set(df_view[df_view["event_name"] == "glossary_click"][session_column].dropna().unique())
        chat_sessions = set(df_view[df_view["event_name"] == "chat_question"][session_column].dropna().unique())
        
        # 2ë‹¨ê³„: ë‰´ìŠ¤ í´ë¦­í•œ ì„¸ì…˜ ì¤‘ì—ì„œ Glossaryë„ í´ë¦­í•œ ì„¸ì…˜ ìˆ˜ (ìˆœì°¨ì )
        # êµì§‘í•©(&)ì„ ì‚¬ìš©í•˜ì—¬ ë‘ ì´ë²¤íŠ¸ë¥¼ ëª¨ë‘ ìˆ˜í–‰í•œ ì„¸ì…˜ë§Œ ê³„ì‚°
        glossary_after_news_sessions = len(news_sessions & glossary_sessions)  # êµì§‘í•©
        
        # 3ë‹¨ê³„: ë‰´ìŠ¤ í´ë¦­ AND Glossary í´ë¦­í•œ ì„¸ì…˜ ì¤‘ì—ì„œ ì±—ë´‡ë„ ì§ˆë¬¸í•œ ì„¸ì…˜ ìˆ˜ (ìˆœì°¨ì )
        # ì¦‰, ë‰´ìŠ¤ â†’ Glossary â†’ ì±—ë´‡ ìˆœì„œë¡œ ëª¨ë‘ ìˆ˜í–‰í•œ ì„¸ì…˜ ìˆ˜
        news_and_glossary_sessions = news_sessions & glossary_sessions
        chat_after_glossary_sessions = len(news_and_glossary_sessions & chat_sessions)  # 3ë‹¨ê³„ ëª¨ë‘ ìˆ˜í–‰í•œ ì„¸ì…˜
        
        # ì „í™˜ìœ¨ ê³„ì‚° (ì´ì „ ë‹¨ê³„ ëŒ€ë¹„)
        glossary_rate = (glossary_after_news_sessions / news_click_sessions * 100) if news_click_sessions > 0 else 0
        chat_rate = (chat_after_glossary_sessions / glossary_after_news_sessions * 100) if glossary_after_news_sessions > 0 else 0
        
        funnel_data = pd.DataFrame({
            "ë‹¨ê³„": ["ë‰´ìŠ¤ í´ë¦­", "Glossary í´ë¦­", "ì±—ë´‡ ì§ˆë¬¸"],
            "ì„¸ì…˜ ìˆ˜": [news_click_sessions, glossary_after_news_sessions, chat_after_glossary_sessions],
            "ì „í™˜ìœ¨ (%)": [100.0, glossary_rate, chat_rate]
        })
        
        st.dataframe(funnel_data, use_container_width=True)
        
        if px is not None:
            fig3 = px.funnel(
                funnel_data,
                x="ì„¸ì…˜ ìˆ˜",
                y="ë‹¨ê³„",
                title="ë‰´ìŠ¤ â†’ Glossary â†’ ì±—ë´‡ ì „í™˜ í¼ë„ (ì„¸ì…˜ ê¸°ë°˜, ìˆœì°¨ì )"
            )
            st.plotly_chart(fig3, use_container_width=True)
    
    # ì±—ë´‡ ê²€ìƒ‰ â†’ ë‰´ìŠ¤ í´ë¦­ ì „í™˜ í¼ë„ (ì„¸ì…˜ ê¸°ë°˜)
    # 1ë‹¨ê³„: ì±—ë´‡ ê²€ìƒ‰ ì‹¤í–‰í•œ ì„¸ì…˜ ìˆ˜
    search_sessions = df_view[df_view["event_name"] == "news_search_from_chat"][session_column].nunique() if session_column in df_view.columns else 0
    
    if search_sessions > 0:
        # 2ë‹¨ê³„: ê²€ìƒ‰ ì‹¤í–‰í•œ ì„¸ì…˜ ì¤‘ì—ì„œ ë‰´ìŠ¤ë„ í´ë¦­í•œ ì„¸ì…˜ ìˆ˜
        search_session_set = set(df_view[df_view["event_name"] == "news_search_from_chat"][session_column].dropna().unique())
        selected_session_set = set(df_view[df_view["event_name"] == "news_selected_from_chat"][session_column].dropna().unique())
        selected_after_search_sessions = len(search_session_set & selected_session_set)  # êµì§‘í•©
        
        # ì „í™˜ìœ¨ ê³„ì‚°
        selected_rate = (selected_after_search_sessions / search_sessions * 100) if search_sessions > 0 else 0
        
        chat_funnel_data = pd.DataFrame({
            "ë‹¨ê³„": ["ì±—ë´‡ ê²€ìƒ‰ ì‹¤í–‰", "ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ í´ë¦­"],
            "ì„¸ì…˜ ìˆ˜": [search_sessions, selected_after_search_sessions],
            "ì „í™˜ìœ¨ (%)": [100.0, selected_rate]
        })
        
        st.dataframe(chat_funnel_data, use_container_width=True)
        
        if px is not None:
            fig3b = px.funnel(
                chat_funnel_data,
                x="ì„¸ì…˜ ìˆ˜",
                y="ë‹¨ê³„",
                title="ì±—ë´‡ ê²€ìƒ‰ â†’ ë‰´ìŠ¤ í´ë¦­ ì „í™˜ í¼ë„ (ì„¸ì…˜ ê¸°ë°˜)"
            )
            st.plotly_chart(fig3b, use_container_width=True)
    
    st.markdown("---")
    
    # ========== D. ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„ ==========
    st.markdown("#### ğŸ” ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„")
    
    # ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ ë‰´ìŠ¤ì™€ ì‹¤ì œ í´ë¦­ëœ ë‰´ìŠ¤ ë¶„ì„
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"].copy()
    selected_events = df_view[df_view["event_name"] == "news_selected_from_chat"].copy()
    
    if len(search_events) > 0:
        # ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ ë‰´ìŠ¤ ID ìˆ˜ì§‘
        news_appearances = {}  # {news_id: {count: int, keywords: set}}
        
        for idx, row in search_events.iterrows():
            payload = _parse_payload(row.get("payload"))
            if payload and "article_ids" in payload:
                article_ids = payload.get("article_ids", [])
                keyword = payload.get("keyword", "")
                
                for news_id in article_ids:
                    if news_id:
                        news_id_str = str(news_id)
                        if news_id_str not in news_appearances:
                            news_appearances[news_id_str] = {"count": 0, "keywords": set()}
                        news_appearances[news_id_str]["count"] += 1
                        if keyword:
                            news_appearances[news_id_str]["keywords"].add(keyword)
        
        # ì‹¤ì œ í´ë¦­ëœ ë‰´ìŠ¤ ID ìˆ˜ì§‘
        news_clicks = {}  # {news_id: count}
        news_titles = {}  # {news_id: title}
        
        for idx, row in selected_events.iterrows():
            news_id = row.get("news_id")
            if news_id:
                news_id_str = str(news_id)
                news_clicks[news_id_str] = news_clicks.get(news_id_str, 0) + 1
                
                # ì œëª© ì •ë³´ ìˆ˜ì§‘
                payload = _parse_payload(row.get("payload"))
                if payload and "title" in payload and news_id_str not in news_titles:
                    news_titles[news_id_str] = payload.get("title", "")
        
        # ì¸ê¸° ë‰´ìŠ¤ ë¶„ì„ ë°ì´í„° ìƒì„±
        if news_appearances:
            popularity_data = []
            for news_id, data in news_appearances.items():
                appearance_count = data["count"]
                click_count = news_clicks.get(news_id, 0)
                click_rate = (click_count / appearance_count * 100) if appearance_count > 0 else 0
                title = news_titles.get(news_id, f"ë‰´ìŠ¤ ID: {news_id}")
                
                popularity_data.append({
                    "news_id": news_id,
                    "ì œëª©": title[:50] + "..." if len(title) > 50 else title,
                    "ê²€ìƒ‰ ê²°ê³¼ í¬í•¨": appearance_count,
                    "í´ë¦­ ìˆ˜": click_count,
                    "í´ë¦­ë¥  (%)": round(click_rate, 1)
                })
            
            if popularity_data:
                popularity_df = pd.DataFrame(popularity_data)
                popularity_df = popularity_df.sort_values("í´ë¦­ ìˆ˜", ascending=False).head(10)
                
                if px is not None and len(popularity_df) > 0:
                    fig_pop = px.bar(
                        popularity_df,
                        x="í´ë¦­ ìˆ˜",
                        y="ì œëª©",
                        orientation='h',
                        title="ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì¥ ë§ì´ í´ë¦­ëœ ë‰´ìŠ¤ Top 10",
                        labels={"í´ë¦­ ìˆ˜": "í´ë¦­ ìˆ˜", "ì œëª©": "ë‰´ìŠ¤ ì œëª©"},
                        hover_data=["ê²€ìƒ‰ ê²°ê³¼ í¬í•¨", "í´ë¦­ë¥  (%)"]
                    )
                    st.plotly_chart(fig_pop, use_container_width=True)
                
                st.dataframe(popularity_df, use_container_width=True, height=300)
    
    st.markdown("---")
    
    # ========== E. ìš©ì–´ë³„ ì¸ê¸° ë¶„ì„ (Bar Chart) ==========
    st.markdown("#### ğŸ“Š ìš©ì–´ë³„ ì¸ê¸° ë¶„ì„")
    
    glossary_clicks = df_view[df_view["event_name"] == "glossary_click"].copy()
    
    if len(glossary_clicks) > 0:
        # term ì¶”ì¶œ
        terms_list = []
        for idx, row in glossary_clicks.iterrows():
            term = _get_term_from_row(row)
            if term:
                terms_list.append(term)
        
        if terms_list:
            term_counts = pd.Series(terms_list).value_counts().head(10)
            term_df = pd.DataFrame({
                "ìš©ì–´": term_counts.index,
                "í´ë¦­ ìˆ˜": term_counts.values
            })
            
            if px is not None:
                fig4 = px.bar(
                    term_df,
                    x="í´ë¦­ ìˆ˜",
                    y="ìš©ì–´",
                    orientation='h',
                    title="Top 10 Glossary ìš©ì–´",
                    labels={"í´ë¦­ ìˆ˜": "í´ë¦­ ìˆ˜", "ìš©ì–´": "ìš©ì–´"}
                )
                st.plotly_chart(fig4, use_container_width=True)
            
            st.dataframe(term_df, use_container_width=True, height=300)
    
    st.markdown("---")
    
    # ========== E. ì„±ëŠ¥ ë¶„ì„ ==========
    st.markdown("#### âš¡ ì„±ëŠ¥ ë¶„ì„")
    
    # í•˜ì´ë¼ì´íŠ¸ latency ë°ì´í„° ì¶”ì¶œ (ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ êµ¬ë¶„)
    # Content Quality íƒ­ì˜ ì„±ëŠ¥ ë¶„ì„ì—ì„œ ì‚¬ìš©
    detail_events = df_view[df_view["event_name"] == "news_detail_open"].copy()
    highlight_latencies_cache_miss = []  # ìºì‹œ ë¯¸ìŠ¤ë§Œ (ì‹¤ì œ ì„±ëŠ¥ ë¬¸ì œ íŒŒì•…ìš©)
    highlight_latencies_cache_hit = []   # ìºì‹œ íˆíŠ¸ë§Œ (ì°¸ê³ ìš©)
    highlight_latencies = []  # ì „ì²´ (fallbackìš© - ìºì‹œ ì •ë³´ê°€ ì—†ì„ ë•Œ)
    
    for idx, row in detail_events.iterrows():
        perf_data = _extract_perf_data(row)
        if perf_data and isinstance(perf_data, dict):
            highlight_ms = perf_data.get("highlight_ms")
            cache_hit = perf_data.get("highlight_cache_hit", False)
            
            if highlight_ms is not None:
                try:
                    highlight_value = float(highlight_ms)
                    # ìœ íš¨í•œ ê°’ë§Œ ì¶”ê°€ (0ë³´ë‹¤ í¬ê³  í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´)
                    if highlight_value > 0 and highlight_value < 100000:  # 100ì´ˆ ì´ìƒì€ ì œì™¸
                        highlight_latencies.append(highlight_value)
                        # ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ êµ¬ë¶„
                        if cache_hit:
                            highlight_latencies_cache_hit.append(highlight_value)
                        else:
                            highlight_latencies_cache_miss.append(highlight_value)
                except (ValueError, TypeError):
                    pass
    
    # 1. í•˜ì´ë¼ì´íŠ¸ latency íˆìŠ¤í† ê·¸ë¨ (ìºì‹œ íˆíŠ¸/ë¯¸ìŠ¤ êµ¬ë¶„)
    # ìºì‹œ ë¯¸ìŠ¤ë§Œ í‘œì‹œ (ì‹¤ì œ ì„±ëŠ¥ ë¬¸ì œ íŒŒì•…ìš©)
    if highlight_latencies_cache_miss and px is not None:
        fig5 = px.histogram(
            x=highlight_latencies_cache_miss,
            nbins=30,
            title="í•˜ì´ë¼ì´íŠ¸ Latency ë¶„í¬ (ìºì‹œ ë¯¸ìŠ¤ë§Œ)",
            labels={"x": "Latency (ms)", "count": "ë¹ˆë„"}
        )
        # ê²½ê³  ì˜ì—­ í‘œì‹œ (2ì´ˆ ì´ìƒ)
        fig5.add_vline(x=2000, line_dash="dash", line_color="red", annotation_text="ê²½ê³  ì˜ì—­ (2ì´ˆ)")
        st.plotly_chart(fig5, use_container_width=True)
        
        # ìºì‹œ íˆíŠ¸ ë¶„í¬ë„ ë³„ë„ë¡œ í‘œì‹œ (ì°¸ê³ ìš©)
        if highlight_latencies_cache_hit and len(highlight_latencies_cache_hit) > 0:
            fig5b = px.histogram(
                x=highlight_latencies_cache_hit,
                nbins=30,
                title="í•˜ì´ë¼ì´íŠ¸ Latency ë¶„í¬ (ìºì‹œ íˆíŠ¸ë§Œ, ì°¸ê³ ìš©)",
                labels={"x": "Latency (ms)", "count": "ë¹ˆë„"}
            )
            st.plotly_chart(fig5b, use_container_width=True)
    elif highlight_latencies and px is not None:
        # ìºì‹œ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ì „ì²´ ë°ì´í„° í‘œì‹œ
        fig5 = px.histogram(
            x=highlight_latencies,
            nbins=30,
            title="í•˜ì´ë¼ì´íŠ¸ Latency ë¶„í¬",
            labels={"x": "Latency (ms)", "count": "ë¹ˆë„"}
        )
        # ê²½ê³  ì˜ì—­ í‘œì‹œ (2ì´ˆ ì´ìƒ)
        fig5.add_vline(x=2000, line_dash="dash", line_color="red", annotation_text="ê²½ê³  ì˜ì—­ (2ì´ˆ)")
        st.plotly_chart(fig5, use_container_width=True)
    
    # 2. ì¼ìë³„ í‰ê·  í•˜ì´ë¼ì´íŠ¸ latency
    if "date" in df_view.columns and highlight_latencies and len(detail_events) > 0:
        detail_events_with_date = detail_events.copy()
        if "date" not in detail_events_with_date.columns:
            detail_events_with_date["date"] = pd.to_datetime(detail_events_with_date["event_time"]).dt.date
        
        daily_highlight_latencies = []
        for date in detail_events_with_date["date"].unique():
            daily_events = detail_events_with_date[detail_events_with_date["date"] == date]
            daily_latencies = []
            for idx, row in daily_events.iterrows():
                perf_data = _extract_perf_data(row)
                if perf_data and isinstance(perf_data, dict):
                    highlight_ms = perf_data.get("highlight_ms")
                    if highlight_ms is not None:
                        try:
                            daily_latencies.append(float(highlight_ms))
                        except (ValueError, TypeError):
                            pass
            if daily_latencies:
                daily_highlight_latencies.append({
                    "date": date,
                    "í‰ê·  Latency (ms)": sum(daily_latencies) / len(daily_latencies)
                })
        
        if daily_highlight_latencies and px is not None:
            daily_latency_df = pd.DataFrame(daily_highlight_latencies)
            # ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ (ì‹œê°„ ì •ë³´ ì œê±°)
            daily_latency_df["date_dt"] = pd.to_datetime(daily_latency_df["date"])
            daily_latency_df = daily_latency_df.sort_values("date_dt")
            fig6 = px.line(
                daily_latency_df,
                x="date_dt",
                y="í‰ê·  Latency (ms)",
                title="ì¼ìë³„ í‰ê·  í•˜ì´ë¼ì´íŠ¸ Latency",
                labels={"date_dt": "ë‚ ì§œ", "í‰ê·  Latency (ms)": "Latency (ms)"}
            )
            # Xì¶• í¬ë§·ì„ ë‚ ì§œë§Œ í‘œì‹œí•˜ë„ë¡ ì„¤ì •
            fig6.update_xaxes(tickformat='%Y-%m-%d', dtick="D1")
            st.plotly_chart(fig6, use_container_width=True)
    
    # 3. ì±—ë´‡ ì‘ë‹µì†ë„ Box plot
    if chat_latencies and px is not None:
        fig7 = px.box(
            y=chat_latencies,
            title="ì±—ë´‡ ì‘ë‹µì†ë„ ë¶„í¬",
            labels={"y": "Latency (ms)"}
        )
        st.plotly_chart(fig7, use_container_width=True)
