"""
ì„œë²„ ì¤‘ì‹¬ ë¡œê·¸ ë·°ì–´
ì„œë²„ APIì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ í‘œì‹œí•©ë‹ˆë‹¤.
event_log ì¤‘ì‹¬ ëª¨ë“œì—ì„œëŠ” Supabaseì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
"""

from core.config import API_BASE_URL, API_ENABLE, SUPABASE_ENABLE
from core.logger import _get_user_id, _get_backend_session_id, _ensure_backend_session, get_supabase_client
import streamlit as st
import pandas as pd
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
import json
import importlib
import re
import os

# wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import matplotlib
    matplotlib.use('Agg')  # GUI ë°±ì—”ë“œ ì—†ì´ ì‚¬ìš©
    WORDCLOUD_AVAILABLE = True
except ImportError:
    WORDCLOUD_AVAILABLE = False
    if SUPABASE_ENABLE:
        try:
            st.warning("âš ï¸ wordcloud ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. pip install wordcloud matplotlibë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        except:
            pass

px = None
try:
    if importlib.util.find_spec("plotly.express"):
        px = importlib.import_module("plotly.express")
        try:
            import plotly.graph_objects as go
            from plotly.subplots import make_subplots
        except Exception:
            go = None
            make_subplots = None
    else:
        go = None
        make_subplots = None
except Exception:
    px = None
    go = None
    make_subplots = None

# requests ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import requests
    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False

# ============================================================================
# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ============================================================================

def _parse_payload(payload: Any) -> Dict[str, Any]:
    """payloadë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜"""
    if isinstance(payload, dict):
        return payload
    if isinstance(payload, str):
        try:
            return json.loads(payload) if payload else {}
        except:
            return {}
    return {}

def _extract_from_payload(payload: Any, key: str, default=None):
    """payloadì—ì„œ íŠ¹ì • í‚¤ ê°’ì„ ì¶”ì¶œ"""
    parsed = _parse_payload(payload)
    return parsed.get(key, default)

def _get_rag_chat_question_sessions(df_view: pd.DataFrame, session_column: str = "session_id") -> set:
    """
    chat_question ì´ë²¤íŠ¸ ì¤‘ RAG ì§ˆë¬¸ì¸ ì„¸ì…˜ë§Œ ë°˜í™˜
    
    chat_questionì€ ì„¸ ê°€ì§€ íƒ€ì…ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤:
    1. RAG ì§ˆë¬¸: ì´í›„ glossary_answer ì´ë²¤íŠ¸ ë°œìƒ
    2. ë§í¬: ì´í›„ news_url_added_from_chat ì´ë²¤íŠ¸ ë°œìƒ
    3. ì¼ë°˜ ì§ˆë¬¸: ì´í›„ chat_response ì´ë²¤íŠ¸ ë°œìƒ
    
    Returns:
        RAG ì§ˆë¬¸ì¸ chat_questionì´ ë°œìƒí•œ ì„¸ì…˜ ID ì§‘í•©
    """
    if df_view.empty or "event_name" not in df_view.columns or session_column not in df_view.columns:
        return set()
    
    chat_questions = df_view[df_view["event_name"] == "chat_question"].copy()
    if chat_questions.empty:
        return set()
    
    # ì„¸ì…˜ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ê° chat_question ì´í›„ ì´ë²¤íŠ¸ í™•ì¸
    rag_sessions = set()
    
    for session_id in chat_questions[session_column].dropna().unique():
        session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
        chat_question_indices = session_events[session_events["event_name"] == "chat_question"].index
        
        for chat_idx in chat_question_indices:
            # chat_question ì´í›„ì˜ ì´ë²¤íŠ¸ í™•ì¸
            after_chat = session_events.loc[session_events.index > chat_idx]
            
            # RAG ì§ˆë¬¸ì¸ì§€ í™•ì¸: glossary_answer ì´ë²¤íŠ¸ê°€ ë°œìƒí–ˆëŠ”ì§€
            has_glossary_answer = (after_chat["event_name"] == "glossary_answer").any()
            
            # ë§í¬ë‚˜ ê²€ìƒ‰ ìš”ì²­ì´ ì•„ë‹Œì§€ í™•ì¸ (ì´ ê²½ìš° ì œì™¸)
            has_url_added = (after_chat["event_name"] == "news_url_added_from_chat").any()
            has_search = (after_chat["event_name"] == "news_search_from_chat").any()
            
            # RAG ì§ˆë¬¸ì´ê³ , ë§í¬ë‚˜ ê²€ìƒ‰ ìš”ì²­ì´ ì•„ë‹Œ ê²½ìš°ë§Œ í¬í•¨
            if has_glossary_answer and not has_url_added and not has_search:
                rag_sessions.add(session_id)
                break  # í•œ ì„¸ì…˜ì— ì—¬ëŸ¬ RAG ì§ˆë¬¸ì´ ìˆì–´ë„ í•œ ë²ˆë§Œ ì¶”ê°€
    
    return rag_sessions

def _extract_perf_data(row: pd.Series) -> Optional[Dict[str, Any]]:
    """payloadì—ì„œ ì„±ëŠ¥ ë°ì´í„° ì¶”ì¶œ (event_name ê¸°ë°˜)"""
    try:
        payload_raw = row.get("payload")
        event_name = row.get("event_name")
        
        if not payload_raw:
            return None
        
        payload = _parse_payload(payload_raw)
        if not payload:
            return None
        
        # event_nameì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
        if event_name == "news_detail_open":
            perf_steps = payload.get("perf_steps", {})
            if isinstance(perf_steps, dict) and "highlight_ms" in perf_steps:
                return {
                    "highlight_ms": perf_steps.get("highlight_ms"),
                    "terms_filter_ms": perf_steps.get("terms_filter_ms"),
                    "total_ms": perf_steps.get("total_ms"),
                    "terms_count": perf_steps.get("terms_count"),
                    "content_length": perf_steps.get("content_length"),
                    "cache_hit": payload.get("cache_hit", False),
                    "highlight_cache_hit": payload.get("highlight_cache_hit", False),
                    "terms_cache_hit": payload.get("terms_cache_hit", False),
                }
        
        elif event_name == "news_click":
            click_process_ms = payload.get("click_process_ms")
            if click_process_ms is not None:
                return {
                    "click_process_ms": click_process_ms,
                    "content_length": payload.get("content_length"),
                }
        
        elif event_name in ("glossary_click", "glossary_answer"):
            perf_steps = payload.get("perf_steps", {})
            if isinstance(perf_steps, dict) and "explanation_ms" in perf_steps:
                return {
                    "explanation_ms": perf_steps.get("explanation_ms"),
                    "total_ms": perf_steps.get("total_ms"),
                    "answer_length": perf_steps.get("answer_length"),
                }
        
        # RAG ì‘ë‹µ ì‹œê°„ (latency_ms ì§ì ‘ ì‚¬ìš©)
        if event_name in ("chat_response", "glossary_answer"):
            latency_ms = payload.get("latency_ms") or row.get("latency_ms")
            if latency_ms is not None:
                return {
                    "latency_ms": latency_ms,
                    "answer_length": payload.get("answer_len") or payload.get("answer_length"),
                }
        
        return None
    except Exception:
        return None

def _get_news_id_from_row(row: pd.Series) -> Optional[str]:
    """rowì—ì„œ news_id ì¶”ì¶œ (ì—¬ëŸ¬ ì†ŒìŠ¤ í™•ì¸)"""
    news_id = row.get("news_id")
    if news_id and not pd.isna(news_id):
        return str(news_id)
    
    # payloadì—ì„œ ì¶”ì¶œ ì‹œë„
    payload = _parse_payload(row.get("payload"))
    if payload:
        news_id = payload.get("news_id") or payload.get("article_id")
        if news_id:
            return str(news_id)
    
    # ref_id í™•ì¸
    ref_id = row.get("ref_id")
    if ref_id and not pd.isna(ref_id):
        return str(ref_id)
    
    return None

def _format_news_id_display(news_id: Optional[str]) -> str:
    """news_idë¥¼ í‘œì‹œìš©ìœ¼ë¡œ í¬ë§·íŒ… (ìŒìˆ˜ë©´ ì„ì‹œ ë‰´ìŠ¤ë¡œ í‘œì‹œ)"""
    if not news_id:
        return "N/A"
    
    try:
        news_id_num = float(news_id)
        if news_id_num < 0:
            return f"ì„ì‹œ ë‰´ìŠ¤ ({news_id})"
        else:
            return str(int(news_id_num))
    except (ValueError, TypeError):
        return str(news_id)

def _get_term_from_row(row: pd.Series) -> Optional[str]:
    """rowì—ì„œ term ì¶”ì¶œ (ì—¬ëŸ¬ ì†ŒìŠ¤ í™•ì¸)"""
    term = row.get("term")
    if term and not pd.isna(term) and term != "":
        return str(term)
    
    # payloadì—ì„œ ì¶”ì¶œ
    payload = _parse_payload(row.get("payload"))
    if payload:
        term = payload.get("term")
        if term:
            return str(term)
    
    # term_from_payload ì»¬ëŸ¼ í™•ì¸
    term_from_payload = row.get("term_from_payload")
    if term_from_payload and not pd.isna(term_from_payload):
        return str(term_from_payload)
    
    return None

# ============================================================================
# ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ë“¤
# ============================================================================

def _fetch_news_from_supabase(limit: int = 1000) -> pd.DataFrame:
    """
    Supabaseì—ì„œ news í…Œì´ë¸” ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    
    ì •ë ¬ ê¸°ì¤€ (ìš°ì„ ìˆœìœ„ ìˆœ):
    1. published_at ìµœì‹ ìˆœ (ê°€ì¥ ì¤‘ìš” - ìµœì‹ ì„± í•„ìˆ˜)
    2. impact_score ë†’ì€ ìˆœ (ë‘ ë²ˆì§¸ - ìµœì‹  ë‰´ìŠ¤ ì¤‘ ì˜í–¥ë„ ë†’ì€ ê²ƒ)
    3. urgency_score ë†’ì€ ìˆœ (ì„¸ ë²ˆì§¸)
    4. credibility_score ë†’ì€ ìˆœ (ë„¤ ë²ˆì§¸)
    
    í•„í„°:
    - deleted_atì´ NULLì¸ ë‰´ìŠ¤ë§Œ (ì‚­ì œë˜ì§€ ì•Šì€ ë‰´ìŠ¤)
    """
    if not SUPABASE_ENABLE:
        return pd.DataFrame()
    
    supabase = get_supabase_client()
    if not supabase:
        return pd.DataFrame()
    
    try:
        # deleted_atì´ NULLì¸ ë‰´ìŠ¤ë§Œ ê°€ì ¸ì˜¤ê¸° (ì‚­ì œë˜ì§€ ì•Šì€ ë‰´ìŠ¤)
        # ì¶©ë¶„íˆ ë§ì´ ê°€ì ¸ì˜¨ í›„, Pythonì—ì„œ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬
        query = (
            supabase.table("news")
            .select("*")
            .is_("deleted_at", "null")
        )
        
        # limitì´ ë§¤ìš° í¬ë©´ ì œí•œ ì—†ì´ ê°€ì ¸ì˜¤ê¸° (ëª¨ë“  ë°ì´í„° ë¶„ì„)
        if limit < 999999:
            query = query.limit(limit * 10)  # ì¶©ë¶„íˆ ë§ì´ ê°€ì ¸ì˜¨ í›„ ì •ë ¬ (ë†’ì€ ì ìˆ˜ ë‰´ìŠ¤ í™•ë³´)
        
        response = query.execute()
        
        if response.data:
            df = pd.DataFrame(response.data)
            
            # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜
            date_columns = ["published_at", "created_at", "updated_at", "deleted_at"]
            for col in date_columns:
                if col in df.columns:
                    df[col] = pd.to_datetime(df[col], errors="coerce")
            
            # ì •ë ¬ ê¸°ì¤€: published_at > impact_score > urgency_score > credibility_score
            # ì ìˆ˜ê°€ NULLì¸ ê²½ìš° -1ë¡œ ë³€í™˜í•˜ì—¬ ë‚®ì€ ìš°ì„ ìˆœìœ„ë¡œ ì²˜ë¦¬
            sort_columns = []
            ascending_list = []
            
            # 1ìˆœìœ„: published_at ìµœì‹ ìˆœ (ê°€ì¥ ì¤‘ìš” - ìµœì‹ ì„± í•„ìˆ˜)
            if "published_at" in df.columns:
                sort_columns.append("published_at")
                ascending_list.append(False)
            
            # 2ìˆœìœ„: impact_score ë†’ì€ ìˆœ (ìµœì‹  ë‰´ìŠ¤ ì¤‘ ì˜í–¥ë„ ë†’ì€ ê²ƒ)
            if "impact_score" in df.columns:
                df["impact_score_sorted"] = df["impact_score"].fillna(-1)
                sort_columns.append("impact_score_sorted")
                ascending_list.append(False)
            
            # 3ìˆœìœ„: urgency_score ë†’ì€ ìˆœ
            if "urgency_score" in df.columns:
                df["urgency_score_sorted"] = df["urgency_score"].fillna(-1)
                sort_columns.append("urgency_score_sorted")
                ascending_list.append(False)
            
            # 4ìˆœìœ„: credibility_score ë†’ì€ ìˆœ
            if "credibility_score" in df.columns:
                df["credibility_score_sorted"] = df["credibility_score"].fillna(-1)
                sort_columns.append("credibility_score_sorted")
                ascending_list.append(False)
            
            # ì •ë ¬ ì‹¤í–‰
            if sort_columns:
                df = df.sort_values(sort_columns, ascending=ascending_list)
                # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
                temp_cols = [col for col in df.columns if col.endswith("_sorted")]
                df = df.drop(columns=temp_cols)
                # ìƒìœ„ limitê°œë§Œ ë°˜í™˜
                df = df.head(limit)
            else:
                # ì ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ published_at ê¸°ì¤€ìœ¼ë¡œë§Œ ì •ë ¬
                if "published_at" in df.columns:
                    df = df.sort_values("published_at", ascending=False).head(limit)
                else:
                    df = df.head(limit)
            
            return df
        return pd.DataFrame()
    except Exception as e:
        st.warning(f"âš ï¸ Supabaseì—ì„œ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def _fetch_event_logs_from_supabase(user_id: Optional[str] = None, limit: int = 1000) -> pd.DataFrame:
    """Supabaseì—ì„œ event_logs ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
    if not SUPABASE_ENABLE:
        return pd.DataFrame()
    
    supabase = get_supabase_client()
    if not supabase:
        return pd.DataFrame()
    
    try:
        query = supabase.table("event_logs").select("*")
        
        if user_id:
            query = query.eq("user_id", user_id)
        
        query = query.order("event_time", desc=True)
        # Supabaseì˜ ê¸°ë³¸ limitì´ 1000ì´ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ limit ì„¤ì •
        if limit > 0:
            query = query.limit(limit)
        
        response = query.execute()
        
        if response.data:
            df = pd.DataFrame(response.data)
            if "event_time" in df.columns:
                df["event_time"] = pd.to_datetime(df["event_time"], errors="coerce")
            if "payload" in df.columns:
                def _extract_from_payload(payload, key):
                    """payloadì—ì„œ íŠ¹ì • í‚¤ ê°’ì„ ì¶”ì¶œ"""
                    parsed = _parse_payload(payload)
                    return parsed.get(key)
                
                # term ì¶”ì¶œ
                df["term_from_payload"] = df["payload"].apply(lambda p: _extract_from_payload(p, "term"))
                
                # news_id ì¶”ì¶œ
                if "news_id" not in df.columns or df["news_id"].isna().all():
                    df["news_id_from_payload"] = df["payload"].apply(
                        lambda p: _extract_from_payload(p, "news_id") or _extract_from_payload(p, "article_id")
                    )
                    if "news_id" not in df.columns:
                        df["news_id"] = df["news_id_from_payload"]
                    else:
                        df["news_id"] = df["news_id"].fillna(df["news_id_from_payload"])
                
                # latency_ms ì¶”ì¶œ
                if "latency_ms" not in df.columns or df["latency_ms"].isna().all():
                    df["latency_ms_from_payload"] = df["payload"].apply(lambda p: _extract_from_payload(p, "latency_ms"))
                    if "latency_ms" not in df.columns:
                        df["latency_ms"] = df["latency_ms_from_payload"]
                    else:
                        df["latency_ms"] = df["latency_ms"].fillna(df["latency_ms_from_payload"])
            return df
        return pd.DataFrame()
    except Exception as e:
        st.warning(f"âš ï¸ Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return pd.DataFrame()

def _to_kst(series):
    """UTC ì‹œê°„ì„ KSTë¡œ ë³€í™˜"""
    dt = pd.to_datetime(series, errors="coerce", utc=True)
    return dt.dt.tz_convert("Asia/Seoul")

def _fill_sessions_from_time(
    df: pd.DataFrame,
    *,
    threshold_minutes: int = 30,
    time_column: str = "event_time",
    user_column: str = "user_id",
) -> pd.DataFrame:
    """
    ì„¸ì…˜ ID ê³„ì‚°: ì´ë²¤íŠ¸ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì„¸ì…˜ êµ¬ë¶„
    
    ë¡œì§:
    1. user_idë³„ë¡œ ì´ë²¤íŠ¸ë¥¼ ì‹œê°„ìˆœ ì •ë ¬
    2. ì´ì „ ì´ë²¤íŠ¸ì™€ì˜ ì‹œê°„ ì°¨ì´ê°€ threshold_minutes(ê¸°ë³¸ 30ë¶„)ë¥¼ ì´ˆê³¼í•˜ë©´ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ êµ¬ë¶„
    3. ì²« ì´ë²¤íŠ¸ëŠ” í•­ìƒ ìƒˆ ì„¸ì…˜ ì‹œì‘
    
    Args:
        df: ì´ë²¤íŠ¸ ë¡œê·¸ DataFrame
        threshold_minutes: ì„¸ì…˜ êµ¬ë¶„ ê¸°ì¤€ ì‹œê°„(ë¶„) - ê¸°ë³¸ê°’ 30ë¶„
        time_column: ì‹œê°„ ì»¬ëŸ¼ëª…
        user_column: ì‚¬ìš©ì ID ì»¬ëŸ¼ëª…
    
    Returns:
        session_id_resolved ì»¬ëŸ¼ì´ ì¶”ê°€ëœ DataFrame
    """
    """event_time ê¸°ë°˜ìœ¼ë¡œ ì„¸ì…˜ IDë¥¼ ì¶”ì‚°í•©ë‹ˆë‹¤."""
    if df.empty or time_column not in df.columns:
        result = df.copy()
        if "session_id" in result.columns:
            result["session_id_resolved"] = result["session_id"]
        return result

    work = df.copy()
    work[time_column] = pd.to_datetime(work[time_column], errors="coerce")

    has_user_column = user_column in work.columns
    if has_user_column:
        session_users = work[user_column].fillna("anonymous").astype(str)
        session_users = session_users.where(session_users.str.len() > 0, "anonymous")
    else:
        session_users = pd.Series(["anonymous"] * len(work), index=work.index)

    threshold = pd.Timedelta(minutes=threshold_minutes)
    order = work.index.to_series(name="_session_order")
    work = work.assign(_session_user=session_users, _session_order=order)
    work = work.sort_values(["_session_user", time_column, "_session_order"])

    gaps = work.groupby("_session_user")[time_column].diff()
    new_session_flags = gaps.isna() | (gaps > threshold) | work[time_column].isna()
    session_sequence = new_session_flags.astype(int).groupby(work["_session_user"]).cumsum()

    inferred_ids = work["_session_user"].astype(str) + "-" + session_sequence.astype(str)
    work["session_id_inferred"] = inferred_ids

    resolved = work.sort_values("_session_order")["session_id_inferred"]
    result = df.copy()
    result["session_id_inferred"] = resolved

    if "session_id" in result.columns:
        session_series = result["session_id"]
        session_series = session_series.where(session_series.notna(), None)
        if session_series.dtype != object:
            session_series = session_series.astype("object")
        missing_mask = session_series.isna() | (session_series.astype(str).str.len() == 0)
        session_series = session_series.astype("object")
        session_series.loc[missing_mask] = result.loc[missing_mask, "session_id_inferred"]
        result["session_id"] = session_series
    else:
        result["session_id"] = result["session_id_inferred"]

    result["session_id_resolved"] = result["session_id"]
    return result

# ============================================================================
# ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜
# ============================================================================

def render(show_mode: str = "dashboard"):
    """
    ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜: ì„œë²„ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ëŒ€ì‹œë³´ë“œ/ë¡œê·¸ ë·°ì–´ ë Œë”ë§
    
    ì „ì²´ íë¦„:
    1. Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸° (ìµœëŒ€ 2000ê±´)
    2. UTC ì‹œê°„ì„ KSTë¡œ ë³€í™˜
    3. ì„¸ì…˜ ê³„ì‚° (30ë¶„ ê°„ê²©)
    4. show_modeì— ë”°ë¼ ëŒ€ì‹œë³´ë“œ ë˜ëŠ” ë¡œê·¸ ë·°ì–´ í‘œì‹œ
    
    Args:
        show_mode: "dashboard" (ëŒ€ì‹œë³´ë“œ) ë˜ëŠ” "log_viewer" (ë¡œê·¸ ë·°ì–´)
    """
    from core.logger import _get_user_id
    
    # Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
    with st.spinner("ğŸ”„ Supabaseì—ì„œ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        # ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ limitì„ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •
        # ì „ì²´ ë°ì´í„°ê°€ 4000ê°œ ì •ë„ì´ë¯€ë¡œ limitì„ ë” í¬ê²Œ ì„¤ì •
        df = _fetch_event_logs_from_supabase(user_id=None, limit=10000)  # 5000 -> 10000ìœ¼ë¡œ ì¦ê°€

        if df.empty:
            st.info("ğŸ“­ ì•„ì§ ì´ë²¤íŠ¸ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì•±ì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ê°€ ìˆ˜ì§‘ë©ë‹ˆë‹¤.")
            return

        # ì‹œê°„ëŒ€ ë³€í™˜: UTC â†’ KST (í•œêµ­ í‘œì¤€ì‹œ)
        df["event_time"] = _to_kst(df["event_time"])
        df = df.sort_values("event_time")

        # ì„¸ì…˜ ê³„ì‚°: 30ë¶„ ê°„ê²©ìœ¼ë¡œ ì„¸ì…˜ êµ¬ë¶„ (ëª¨ë“  íƒ­ì—ì„œ ì‚¬ìš©)
        # user_idë³„ë¡œ ì´ë²¤íŠ¸ë¥¼ ì‹œê°„ìˆœ ì •ë ¬í•˜ê³ , 30ë¶„ ì´ìƒ ê°„ê²©ì´ ìˆìœ¼ë©´ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ êµ¬ë¶„
        session_gap_minutes = 30
        df = _fill_sessions_from_time(df, threshold_minutes=session_gap_minutes)
        session_column = "session_id_resolved" if "session_id_resolved" in df.columns else "session_id"

        # show_modeì— ë”°ë¼ ë‹¤ë¥¸ í˜ì´ì§€ í‘œì‹œ
        if show_mode == "dashboard":
            st.markdown("## ğŸ“Š ëŒ€ì‹œë³´ë“œ")
            
            # ìƒìœ„ ë ˆë²¨ íƒ­: 4ê°œ ì¹´í…Œê³ ë¦¬
            tab1, tab2, tab3, tab4 = st.tabs([
                "ğŸ“Š KPI Dashboard",      # í•µì‹¬ ì§€í‘œ ìš”ì•½ (DAU, WAU, ì„¸ì…˜ ê¸¸ì´ ë“±)
                "ğŸ”´ Service Health",     # ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë° ì•ˆì •ì„±
                "ğŸŸ¡ Content Quality",     # ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ
                "ğŸŸ¢ User Behavior"       # ì‚¬ìš©ì í–‰ë™ ë¶„ì„
            ])
            
            # íƒ­ 1: KPI Dashboard - í•µì‹¬ ì§€í‘œ ìš”ì•½
            with tab1:
                _render_kpi_dashboard(df, session_column)
            
            # íƒ­ 2: Service Health - ì„±ëŠ¥ ë©”íŠ¸ë¦­, RAG ì‘ë‹µ ì‹œê°„, URL íŒŒì‹± ë“±
            with tab2:
                _render_service_health_tab(df, session_column)
            
            # íƒ­ 3: Content Quality - ë‰´ìŠ¤ ì†ŒìŠ¤ ë¶„ì„, ë³¸ë¬¸ í’ˆì§ˆ, ì›Œë“œí´ë¼ìš°ë“œ ë“±
            with tab3:
                _render_content_quality_tab(df)
            
            # íƒ­ 4: User Behavior - í´ë¦­ë¥ , ì½ê¸° ì‹œê°„, ìš©ì–´ í´ë¦­ë¥  ë“±
            with tab4:
                _render_user_behavior_tab(df, session_column)
        
        elif show_mode == "log_viewer":
            st.markdown("## ğŸ“ ë¡œê·¸ ë·°ì–´")
            # ë¡œê·¸ ë·°ì–´: ê°œë³„ ì´ë²¤íŠ¸ ë¡œê·¸ë¥¼ í•„í„°ë§í•˜ì—¬ ìƒì„¸ í™•ì¸
            _render_log_viewer_tab(df, session_column)

# ============================================================================
# íƒ­ 1: ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (Service Health)
# ============================================================================

def _render_service_health_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° íƒ­: í•µì‹¬ 4ê°œ ì§€í‘œë§Œ í‘œì‹œ
    - ë‰´ìŠ¤ ìƒì„¸ ë¡œë”© ì‹œê°„ í‰ê· 
    - ì±—ë´‡ ì‘ë‹µ ì‹œê°„ í‰ê· 
    - URL íŒŒì‹± ì‹¤íŒ¨ìœ¨
    - ì „ì²´ ì˜¤ë¥˜ ë°œìƒë¥  (RAG ì—ëŸ¬ í¬í•¨)
    """
    st.markdown("### ğŸ”´ ì„œë¹„ìŠ¤ ì„±ëŠ¥ ë°ì´í„° (Service Health)")
    st.markdown("**ëª©í‘œ**: ì„œë¹„ìŠ¤ì˜ ê¸°ìˆ ì  ì•ˆì •ì„± ì¸¡ì • - í•µì‹¬ ì§€í‘œë§Œ í‘œì‹œ")
    
    # 1. ë‰´ìŠ¤ ìƒì„¸ ë¡œë”© ì‹œê°„ í‰ê· 
    detail_events = df_view[df_view["event_name"] == "news_detail_open"].copy()
    detail_latencies = []
    for idx, row in detail_events.iterrows():
        perf_data = _extract_perf_data(row)
        if perf_data and isinstance(perf_data, dict):
            total_ms = perf_data.get("total_ms")
            if total_ms is None:
                highlight_ms = perf_data.get("highlight_ms", 0)
                terms_filter_ms = perf_data.get("terms_filter_ms", 0)
                total_ms = highlight_ms + terms_filter_ms
            if total_ms and total_ms > 0:
                try:
                    detail_latencies.append(float(total_ms))
                except (ValueError, TypeError):
                    pass
    avg_detail_latency = sum(detail_latencies) / len(detail_latencies) if detail_latencies else None
    
    # 2. ì±—ë´‡ ì‘ë‹µ ì‹œê°„ í‰ê· 
    chat_response_events = df_view[df_view["event_name"].isin(["chat_response", "glossary_answer"])].copy()
    chat_latencies = []
    for idx, row in chat_response_events.iterrows():
        latency_ms = None
        perf_data = _extract_perf_data(row)
        if perf_data and isinstance(perf_data, dict):
            latency_ms = perf_data.get("latency_ms") or perf_data.get("total_ms")
        if latency_ms is None:
            payload = _parse_payload(row.get("payload"))
            if payload:
                latency_ms = payload.get("latency_ms") or payload.get("total_ms")
        if latency_ms is None:
            latency_ms = row.get("latency_ms")
        if latency_ms is not None:
            try:
                latency_value = float(latency_ms)
                if latency_value > 0 and latency_value < 1000000:
                    chat_latencies.append(latency_value)
            except (ValueError, TypeError):
                pass
    avg_chat_latency = sum(chat_latencies) / len(chat_latencies) if chat_latencies else None
    
    # 3. URL íŒŒì‹± ì‹¤íŒ¨ìœ¨
    url_events = df_view[df_view["event_name"].isin(["news_url_added_from_chat", "news_url_add_error"])]
    url_error_count = int((url_events["event_name"] == "news_url_add_error").sum()) if len(url_events) > 0 else 0
    url_total_count = len(url_events)
    url_error_rate = (url_error_count / url_total_count * 100) if url_total_count > 0 else 0
    
    # 4. ì „ì²´ ì˜¤ë¥˜ ë°œìƒë¥  (RAG ì—ëŸ¬ í¬í•¨)
    # RAG ì—ëŸ¬ëŠ” payloadì— errorê°€ ìˆëŠ” ê²½ìš°ë¡œ íŒë‹¨
    error_events = df_view[df_view["event_name"].isin([
        "news_url_add_error",
        "glossary_answer",
        "chat_response"
    ])]
    total_errors = url_error_count
    for idx, row in error_events.iterrows():
        if row["event_name"] in ["glossary_answer", "chat_response"]:
            payload = _parse_payload(row.get("payload"))
            if payload and payload.get("error"):
                total_errors += 1
    total_events = len(df_view)
    total_error_rate = (total_errors / total_events * 100) if total_events > 0 else 0
    
    # í•µì‹¬ 4ê°œ ì§€í‘œ í‘œì‹œ
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        if avg_detail_latency is not None:
            st.metric("ë‰´ìŠ¤ ìƒì„¸ ë¡œë”© ì‹œê°„", f"{avg_detail_latency:.0f}ms")
            st.caption(f"ì¸¡ì • ê±´ìˆ˜: {len(detail_latencies)}ê±´")
        else:
            st.metric("ë‰´ìŠ¤ ìƒì„¸ ë¡œë”© ì‹œê°„", "N/A")
    with col2:
        if avg_chat_latency is not None:
            st.metric("ì±—ë´‡ ì‘ë‹µ ì‹œê°„", f"{avg_chat_latency:.0f}ms")
            st.caption(f"ì¸¡ì • ê±´ìˆ˜: {len(chat_latencies)}ê±´")
        else:
            st.metric("ì±—ë´‡ ì‘ë‹µ ì‹œê°„", "N/A")
    with col3:
        st.metric("URL íŒŒì‹± ì‹¤íŒ¨ìœ¨", f"{url_error_rate:.1f}%")
        st.caption(f"ì‹¤íŒ¨: {url_error_count}ê±´ / ì „ì²´: {url_total_count}ê±´")
    with col4:
        st.metric("ì „ì²´ ì˜¤ë¥˜ ë°œìƒë¥ ", f"{total_error_rate:.2f}%")
        st.caption(f"ì˜¤ë¥˜: {total_errors}ê±´ / ì „ì²´: {total_events}ê±´")

def _render_detail_performance(perf_events_with_data: pd.DataFrame):
    """ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ì„±ëŠ¥ ë¶„ì„"""
    detail_events = perf_events_with_data[perf_events_with_data["event_name"] == "news_detail_open"]
    
    if detail_events.empty:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ë¡œë”© ì‹œê°„")
    
    perf_data_list = []
    for idx, row in detail_events.iterrows():
        perf = row["perf_data"]
        if perf and isinstance(perf, dict):
            news_id = _get_news_id_from_row(row)
            highlight_ms = perf.get("highlight_ms", 0)
            terms_filter_ms = perf.get("terms_filter_ms", 0)
            total_ms = perf.get("total_ms") or (highlight_ms + terms_filter_ms)
            
            cache_status = []
            if perf.get("highlight_cache_hit"):
                cache_status.append("í•˜ì´ë¼ì´íŠ¸âœ…")
            if perf.get("terms_cache_hit"):
                cache_status.append("ìš©ì–´âœ…")
            if not cache_status:
                cache_status.append("âŒ")
            
            perf_data_list.append({
                "event_time": row.get("event_time"),
                "news_id": _format_news_id_display(news_id),
                "í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)": highlight_ms,
                "ìš©ì–´ í•„í„°ë§ (ms)": terms_filter_ms,
                "ì „ì²´ ë Œë”ë§ (ms)": total_ms,
                "ë°œê²¬ëœ ìš©ì–´ ìˆ˜": perf.get("terms_count", 0),
                "ê¸°ì‚¬ ê¸¸ì´ (ì)": perf.get("content_length", 0),
                "ìºì‹œ íˆíŠ¸": " / ".join(cache_status),
            })
    
    if perf_data_list:
        perf_df = pd.DataFrame(perf_data_list)
        perf_df = perf_df.sort_values("event_time", ascending=False)
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### ğŸ“Š ì„±ëŠ¥ í†µê³„")
            avg_highlight = perf_df["í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)"].mean()
            avg_filter = perf_df["ìš©ì–´ í•„í„°ë§ (ms)"].mean()
            avg_total = perf_df["ì „ì²´ ë Œë”ë§ (ms)"].mean()
            cache_hit_rate = (perf_df["ìºì‹œ íˆíŠ¸"].str.contains("âœ…", na=False).sum() / len(perf_df) * 100) if len(perf_df) > 0 else 0
            
            st.metric("í‰ê·  í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬", f"{avg_highlight:.0f}ms")
            st.metric("í‰ê·  ìš©ì–´ í•„í„°ë§", f"{avg_filter:.0f}ms")
            st.metric("í‰ê·  ì „ì²´ ë Œë”ë§", f"{avg_total:.0f}ms")
            st.metric("ìºì‹œ íˆíŠ¸ìœ¨", f"{cache_hit_rate:.1f}%")
        
        # ì‹œê°í™”
        if px is not None and len(perf_df) > 0:
            fig = px.scatter(
                perf_df.head(100),
                x="í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ms)",
                y="ìš©ì–´ í•„í„°ë§ (ms)",
                size="ì „ì²´ ë Œë”ë§ (ms)",
                color="ìºì‹œ íˆíŠ¸",
                hover_data=["news_id", "ë°œê²¬ëœ ìš©ì–´ ìˆ˜", "ê¸°ì‚¬ ê¸¸ì´ (ì)"],
                title="ë‰´ìŠ¤ ìƒì„¸ ë³´ê¸° ì„±ëŠ¥ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(perf_df.head(20), use_container_width=True, height=300)

def _render_rag_performance(perf_events_with_data: pd.DataFrame):
    """RAG ì‘ë‹µ ì‹œê°„ ë¶„ì„"""
    rag_events = perf_events_with_data[perf_events_with_data["event_name"].isin(["glossary_answer", "chat_response"])]
    
    if rag_events.empty:
        return
    
    st.markdown("#### ğŸ¤– RAG ì‘ë‹µ ì‹œê°„")
    
    rag_data_list = []
    for idx, row in rag_events.iterrows():
        perf = row["perf_data"]
        if perf and isinstance(perf, dict):
            term = _get_term_from_row(row)
            latency_ms = perf.get("latency_ms") or perf.get("total_ms") or perf.get("explanation_ms")
            answer_length = perf.get("answer_length") or perf.get("answer_len", 0)
            
            if latency_ms is not None:
                rag_data_list.append({
                    "event_time": row.get("event_time"),
                    "event_name": row.get("event_name"),
                    "term": term or "",
                    "ì‘ë‹µ ì‹œê°„ (ms)": latency_ms,
                    "ë‹µë³€ ê¸¸ì´ (ì)": answer_length,
                })
    
    if rag_data_list:
        rag_df = pd.DataFrame(rag_data_list)
        rag_df = rag_df.sort_values("event_time", ascending=False)
        
        col1, col2 = st.columns(2)
        with col1:
            avg_latency = rag_df["ì‘ë‹µ ì‹œê°„ (ms)"].mean()
            st.metric("í‰ê·  RAG ì‘ë‹µ ì‹œê°„", f"{avg_latency:.0f}ms")
        with col2:
            avg_length = rag_df["ë‹µë³€ ê¸¸ì´ (ì)"].mean()
            st.metric("í‰ê·  ë‹µë³€ ê¸¸ì´", f"{avg_length:.0f}ì")
        
        # ì‹œê°í™”
        if px is not None and len(rag_df) > 0:
            fig = px.histogram(
                rag_df,
                x="ì‘ë‹µ ì‹œê°„ (ms)",
                nbins=30,
                title="RAG ì‘ë‹µ ì‹œê°„ ë¶„í¬",
                labels={"ì‘ë‹µ ì‹œê°„ (ms)": "ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)", "count": "ë¹ˆë„"}
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(rag_df.head(20), use_container_width=True, height=300)

def _render_search_performance(df_view: pd.DataFrame):
    """ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„"""
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"]
    
    if search_events.empty:
        return
    
    st.markdown("#### ğŸ” ìì—°ì–´ ê²€ìƒ‰ ì²˜ë¦¬ ì†ë„")
    
    search_data_list = []
    for idx, row in search_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        latency_ms = payload.get("latency_ms") or row.get("latency_ms")
        message = payload.get("message") or row.get("message", "")
        
        # latency_msë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê³  ìœ íš¨í•œ ê°’ë§Œ ì‚¬ìš©
        try:
            if latency_ms is not None:
                latency_ms = pd.to_numeric(latency_ms, errors='coerce')
                if pd.notna(latency_ms) and latency_ms > 0:
                    search_data_list.append({
                        "event_time": row.get("event_time"),
                        "ê²€ìƒ‰ì–´": message[:50] if message else "",
                        "ì²˜ë¦¬ ì‹œê°„ (ms)": float(latency_ms),
                    })
        except (ValueError, TypeError):
            continue
    
    if search_data_list:
        search_df = pd.DataFrame(search_data_list)
        search_df = search_df.sort_values("event_time", ascending=False)
        
        # ìœ íš¨í•œ ê°’ë§Œìœ¼ë¡œ í‰ê·  ê³„ì‚°
        valid_latencies = search_df["ì²˜ë¦¬ ì‹œê°„ (ms)"].dropna()
        if len(valid_latencies) > 0:
            avg_latency = valid_latencies.mean()
            st.metric("í‰ê·  ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„", f"{avg_latency:.0f}ms")
        else:
            st.metric("í‰ê·  ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„", "N/A")
        
        if px is not None and len(search_df) > 0 and len(valid_latencies) > 0:
            fig = px.box(
                search_df,
                y="ì²˜ë¦¬ ì‹œê°„ (ms)",
                title="ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„ ë¶„í¬"
            )
            st.plotly_chart(fig, use_container_width=True)
        
        st.dataframe(search_df.head(20), use_container_width=True, height=200)
    else:
        st.info("ğŸ“Š ê²€ìƒ‰ ì²˜ë¦¬ ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _render_url_parsing_quality(df_view: pd.DataFrame):
    """URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨ ì´ë²¤íŠ¸"""
    url_events = df_view[df_view["event_name"].isin(["news_url_added_from_chat", "news_url_add_error"])]
    
    if url_events.empty:
        return
    
    st.markdown("#### ğŸ”— URL íŒŒì‹± í’ˆì§ˆ")
    
    success_count = int((url_events["event_name"] == "news_url_added_from_chat").sum())
    error_count = int((url_events["event_name"] == "news_url_add_error").sum())
    total_count = success_count + error_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("URL íŒŒì‹± ì„±ê³µ", success_count)
    with col2:
        st.metric("URL íŒŒì‹± ì‹¤íŒ¨", error_count)
    with col3:
        if total_count > 0:
            success_rate = (success_count / total_count) * 100
            st.metric("ì„±ê³µë¥ ", f"{success_rate:.1f}%")
        else:
            st.metric("ì„±ê³µë¥ ", "N/A")
    
    if total_count > 0 and px is not None:
        fig = px.pie(
            values=[success_count, error_count],
            names=["ì„±ê³µ", "ì‹¤íŒ¨"],
            title="URL íŒŒì‹± ì„±ê³µ/ì‹¤íŒ¨ ë¹„ìœ¨"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_session_load(df_view: pd.DataFrame, session_column: str):
    """Streamlit ì„¸ì…˜ ìˆ˜ / ë™ì‹œ ì ‘ì† ë¶€í•˜"""
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### ğŸ‘¥ ì„¸ì…˜ ë¶€í•˜ ë¶„ì„")
    
    # ì‹œê°„ëŒ€ë³„ ì„¸ì…˜ ìˆ˜
    df_view_copy = df_view.copy()
    df_view_copy["hour"] = df_view_copy["event_time"].dt.floor("H")
    hourly_sessions = df_view_copy.groupby("hour")[session_column].nunique().reset_index()
    hourly_sessions.columns = ["ì‹œê°„", "ì„¸ì…˜ ìˆ˜"]
    
    col1, col2 = st.columns(2)
    with col1:
        max_sessions = hourly_sessions["ì„¸ì…˜ ìˆ˜"].max() if len(hourly_sessions) > 0 else 0
        st.metric("ìµœëŒ€ ë™ì‹œ ì„¸ì…˜ ìˆ˜", f"{max_sessions}ê°œ")
    with col2:
        avg_sessions = hourly_sessions["ì„¸ì…˜ ìˆ˜"].mean() if len(hourly_sessions) > 0 else 0
        st.metric("í‰ê·  ì„¸ì…˜ ìˆ˜", f"{avg_sessions:.1f}ê°œ")
    
    if px is not None and len(hourly_sessions) > 0:
        fig = px.line(
            hourly_sessions,
            x="ì‹œê°„",
            y="ì„¸ì…˜ ìˆ˜",
            title="ì‹œê°„ëŒ€ë³„ ì„¸ì…˜ ìˆ˜ ì¶”ì´"
        )
        st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# íƒ­ 2: ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (Content Quality)
# ============================================================================

def _render_content_quality_tab(df_view: pd.DataFrame):
    """
    ğŸŸ¡ ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° íƒ­: í•µì‹¬ ì§€í‘œë§Œ í‘œì‹œ
    - ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸
    - ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘
    - ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨
    - URL íŒŒì‹± ì‹¤íŒ¨ìœ¨
    - ì–¸ë¡ ì‚¬ ë¶„í¬
    - ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„í¬ (LLM ê²°ê³¼)
    - ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš©ì ì°¸ì—¬ë„
    """
    st.markdown("### ğŸŸ¡ ë‰´ìŠ¤ ì½˜í…ì¸  í’ˆì§ˆ ë°ì´í„° (Content Quality)")
    st.markdown("**ëª©í‘œ**: ë‰´ìŠ¤ ë°ì´í„°ê°€ ë§ê°€ì§€ì§€ ì•Šì•˜ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ëŠ” í’ˆì§ˆ ì²´í¬")
    
    # 1. URL íŒŒì‹± ì‹¤íŒ¨ìœ¨ (ì´ë²¤íŠ¸ ë¡œê·¸ ê¸°ë°˜)
    _render_url_parsing_quality_for_content(df_view)
    
    # Supabase news í…Œì´ë¸” ì—°ë™ ë¶„ì„
    with st.spinner("ğŸ”„ Supabaseì—ì„œ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
        # ëª¨ë“  ë°ì´í„° ë¶„ì„ (limit ì œê±°)
        news_df = _fetch_news_from_supabase(limit=999999)
        
        if news_df.empty:
            st.warning("âš ï¸ Supabase `news` í…Œì´ë¸”ì—ì„œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ğŸ’¡ Supabase ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‰´ìŠ¤ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            st.success(f"âœ… {len(news_df):,}ê°œì˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
            
            # 2. ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸
            _render_news_collection_trends(news_df)
            
            # 3. ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘
            _render_financial_news_ratio(news_df)
            
            # 4. ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨
            _render_content_missing_analysis(news_df)
            
            # 5. ì–¸ë¡ ì‚¬ ë¶„í¬
            _render_news_source_distribution(news_df)
            
            # 6. ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„í¬ (LLM ê²°ê³¼)
            st.markdown("---")
            st.markdown("### ğŸ“Š ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„í¬ (LLM ê²°ê³¼)")
            _render_category_distribution_for_prompt(news_df)
            
            # 7. ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš©ì ì°¸ì—¬ë„
            st.markdown("---")
            st.markdown("### ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš©ì ì°¸ì—¬ë„")
            _render_category_engagement_analysis(news_df, df_view)

# ============================================================================
# Supabase news í…Œì´ë¸” ê¸°ë°˜ ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„ í•¨ìˆ˜ë“¤
# ============================================================================

def _render_news_source_distribution(news_df: pd.DataFrame):
    """ë‰´ìŠ¤ ì¶œì²˜(ì–¸ë¡ ì‚¬) ë¶„í¬"""
    if "source" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“° ë‰´ìŠ¤ ì¶œì²˜(ì–¸ë¡ ì‚¬) ë¶„í¬")
    
    # sourceê°€ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_source = news_df[news_df["source"].notna() & (news_df["source"] != "")]
    
    if news_with_source.empty:
        st.info("ğŸ“Š ì¶œì²˜ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    source_counts = news_with_source["source"].value_counts()
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ê³ ìœ  ì¶œì²˜ ìˆ˜", len(source_counts))
    with col2:
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", len(news_with_source))
    
    # Top 10 ì¶œì²˜
    top_sources = source_counts.head(10).reset_index()
    top_sources.columns = ["ì¶œì²˜", "ê±´ìˆ˜"]
    
    if px is not None and len(top_sources) > 0:
        fig = px.bar(
            top_sources,
            x="ì¶œì²˜",
            y="ê±´ìˆ˜",
            title="ë‰´ìŠ¤ ì¶œì²˜ Top 10",
            labels={"ì¶œì²˜": "ì–¸ë¡ ì‚¬", "ê±´ìˆ˜": "ê¸°ì‚¬ ìˆ˜"},
            text="ê±´ìˆ˜"  # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
        )
        fig.update_traces(texttemplate='%{text}', textposition='outside')
        fig.update_xaxes(tickangle=-45)
        fig.update_layout(
            height=400,
            showlegend=False
        )
        st.plotly_chart(fig, use_container_width=True)
    
def _render_financial_news_ratio(news_df: pd.DataFrame):
    """ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘ (í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„)"""
    # title ë˜ëŠ” content ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¶„ì„ ë¶ˆê°€
    if "title" not in news_df.columns and "content" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ’° ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘ (í‚¤ì›Œë“œ ê¸°ë°˜)")
    
    # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ (ì œëª©/ë³¸ë¬¸ì—ì„œ ê²€ìƒ‰)
    financial_keywords = [
        "ê¸ˆìœµ", "ì€í–‰", "ì¦ê¶Œ", "ì£¼ì‹", "ì±„ê¶Œ", "ë¶€ë™ì‚°", "ê²½ì œ", "ì‹œì¥", "íˆ¬ì", "ìì‚°",
        "ì´ì", "ê¸ˆë¦¬", "í™˜ìœ¨", "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "ê²½ê¸°", "ê²½ì œì„±ì¥", "GDP",
        "ê¸°ì—…", "ìƒì¥", "IPO", "ë°°ë‹¹", "ìˆ˜ìµ", "ì†ì‹¤", "ì¬ë¬´", "íšŒê³„", "ì„¸ê¸ˆ", "ì •ì±…",
        "í•œêµ­ì€í–‰", "ê¸ˆìœµê°ë…ì›", "ê¸ˆìœµìœ„ì›íšŒ", "ì¦ì„ ìœ„", "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ë‚˜ìŠ¤ë‹¥",
        "ë¹„íŠ¸ì½”ì¸", "ì•”í˜¸í™”í", "ë¸”ë¡ì²´ì¸", "ê¸ˆìœµê¶Œ", "ê¸ˆìœµì‚¬", "ì€í–‰ê¶Œ"
    ]
    
    def is_financial(row):
        """ë‰´ìŠ¤ê°€ ê¸ˆìœµ ê´€ë ¨ì¸ì§€ íŒë‹¨"""
        title = str(row.get("title", "")).lower()
        content = str(row.get("content", "")).lower()
        
        text = title + " " + content[:500]  # ë³¸ë¬¸ ì•ë¶€ë¶„ 500ìë§Œ í™•ì¸
        
        for keyword in financial_keywords:
            if keyword in text:
                return True
        return False
    
    news_df_copy = news_df.copy()
    news_df_copy["is_financial"] = news_df_copy.apply(is_financial, axis=1)
    
    financial_count = news_df_copy["is_financial"].sum()
    non_financial_count = (~news_df_copy["is_financial"]).sum()
    total_count = len(news_df_copy)
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê¸ˆìœµ ê¸°ì‚¬", f"{financial_count:,}ê±´")
        if total_count > 0:
            financial_ratio = (financial_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {financial_ratio:.1f}%")
    with col2:
        st.metric("ë¹„ê¸ˆìœµ ê¸°ì‚¬", f"{non_financial_count:,}ê±´")
        if total_count > 0:
            non_financial_ratio = (non_financial_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {non_financial_ratio:.1f}%")
    with col3:
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_count:,}ê±´")
    
    if total_count > 0 and px is not None:
        ratio_df = pd.DataFrame({
            "ë¶„ë¥˜": ["ê¸ˆìœµ ê¸°ì‚¬", "ë¹„ê¸ˆìœµ ê¸°ì‚¬"],
            "ê±´ìˆ˜": [financial_count, non_financial_count]
        })
        # Donut chart (hole íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        fig = px.pie(
            ratio_df,
            values="ê±´ìˆ˜",
            names="ë¶„ë¥˜",
            title="ê¸ˆìœµ/ë¹„ê¸ˆìœµ ê¸°ì‚¬ ë¹„ì¤‘",
            hole=0.4  # ë„ë„› ì°¨íŠ¸ë¡œ ë§Œë“¤ê¸°
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_content_length_analysis(news_df: pd.DataFrame):
    """ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬"""
    # content ë˜ëŠ” raw_content_length ì»¬ëŸ¼ í™•ì¸
    content_col = None
    if "raw_content_length" in news_df.columns:
        content_col = "raw_content_length"
    elif "content" in news_df.columns:
        content_col = "content"
    else:
        return
    
    st.markdown("#### ğŸ“ ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬")
    
    if content_col == "raw_content_length":
        # raw_content_lengthê°€ ìˆ«ì ì»¬ëŸ¼ì¸ ê²½ìš°
        news_with_content = news_df[news_df[content_col].notna()].copy()
        news_with_content["content_length"] = pd.to_numeric(news_with_content[content_col], errors='coerce')
    else:
        # content ì»¬ëŸ¼ì—ì„œ ê¸¸ì´ ê³„ì‚°
        news_with_content = news_df[news_df[content_col].notna() & (news_df[content_col] != "")].copy()
        news_with_content["content_length"] = news_with_content[content_col].astype(str).str.len()
    
    news_with_content = news_with_content[news_with_content["content_length"].notna() & (news_with_content["content_length"] > 0)]
    
    if news_with_content.empty:
        st.info("ğŸ“Š ë³¸ë¬¸ ê¸¸ì´ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    col1, col2, col3 = st.columns(3)
    with col1:
        avg_length = news_with_content["content_length"].mean()
        st.metric("í‰ê·  ë³¸ë¬¸ ê¸¸ì´", f"{avg_length:.0f}ì")
    with col2:
        median_length = news_with_content["content_length"].median()
        st.metric("ì¤‘ê°„ê°’ ë³¸ë¬¸ ê¸¸ì´", f"{median_length:.0f}ì")
    with col3:
        min_length = news_with_content["content_length"].min()
        max_length = news_with_content["content_length"].max()
        st.metric("ìµœì†Œ/ìµœëŒ€ ê¸¸ì´", f"{min_length:.0f} / {max_length:.0f}ì")
    
    if px is not None and len(news_with_content) > 0:
        fig = px.histogram(
            news_with_content,
            x="content_length",
            nbins=50,
            title="ë³¸ë¬¸ ê¸¸ì´ ë¶„í¬",
            labels={"content_length": "ë³¸ë¬¸ ê¸¸ì´ (ì)", "count": "ë¹ˆë„"}
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_content_missing_analysis(news_df: pd.DataFrame):
    """ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨"""
    # content ë˜ëŠ” raw_content_length ì»¬ëŸ¼ í™•ì¸
    content_col = None
    if "content" in news_df.columns:
        content_col = "content"
    elif "raw_content_length" in news_df.columns:
        content_col = "raw_content_length"
    else:
        return
    
    st.markdown("#### ğŸ“ ë³¸ë¬¸ ëˆ„ë½ ë¹„ìœ¨")
    
    total_count = len(news_df)
    
    if content_col == "raw_content_length":
        # raw_content_lengthê°€ ìˆ«ì ì»¬ëŸ¼ì¸ ê²½ìš°
        missing_content = news_df[news_df[content_col].isna() | (pd.to_numeric(news_df[content_col], errors='coerce') == 0)]
        missing_count = len(missing_content)
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš°ë„ ëˆ„ë½ìœ¼ë¡œ ê°„ì£¼ (300ì ë¯¸ë§Œ - ì‚¬ìš© ë¶ˆê°€ ìˆ˜ì¤€)
        # 100ì ë¯¸ë§Œ: ë§¤ìš° ì§§ìŒ (ì‚¬ìš© ë¶ˆê°€)
        # 200ì ë¯¸ë§Œ: ì§§ìŒ (ì‚¬ìš© ì–´ë ¤ì›€)
        # 300ì ë¯¸ë§Œ: ê²½ê³  (ë„ˆë¬´ ì§§ìŒ)
        very_short_content = news_df[
            news_df[content_col].notna() &
            (pd.to_numeric(news_df[content_col], errors='coerce') < 100)
        ]
        short_content = news_df[
            news_df[content_col].notna() &
            (pd.to_numeric(news_df[content_col], errors='coerce') >= 100) &
            (pd.to_numeric(news_df[content_col], errors='coerce') < 300)
        ]
        very_short_count = len(very_short_content)
        short_count = len(short_content)
    else:
        # content ì»¬ëŸ¼ì¸ ê²½ìš°
        missing_content = news_df[news_df[content_col].isna() | (news_df[content_col] == "")]
        missing_count = len(missing_content)
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ì€ ê²½ìš°ë„ ëˆ„ë½ìœ¼ë¡œ ê°„ì£¼ (300ì ë¯¸ë§Œ - ì‚¬ìš© ë¶ˆê°€ ìˆ˜ì¤€)
        # 100ì ë¯¸ë§Œ: ë§¤ìš° ì§§ìŒ (ì‚¬ìš© ë¶ˆê°€)
        # 200ì ë¯¸ë§Œ: ì§§ìŒ (ì‚¬ìš© ì–´ë ¤ì›€)
        # 300ì ë¯¸ë§Œ: ê²½ê³  (ë„ˆë¬´ ì§§ìŒ)
        very_short_content = news_df[
            news_df[content_col].notna() & 
            (news_df[content_col] != "") &
            (news_df[content_col].astype(str).str.len() < 100)
        ]
        short_content = news_df[
            news_df[content_col].notna() & 
            (news_df[content_col] != "") &
            (news_df[content_col].astype(str).str.len() >= 100) &
            (news_df[content_col].astype(str).str.len() < 300)
        ]
        very_short_count = len(very_short_content)
        short_count = len(short_content)
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ë³¸ë¬¸ ì™„ì „ ëˆ„ë½", f"{missing_count:,}ê±´")
        if total_count > 0:
            missing_rate = (missing_count / total_count) * 100
            st.caption(f"ëˆ„ë½ë¥ : {missing_rate:.1f}%")
    with col2:
        st.metric("ë§¤ìš° ì§§ìŒ (<100ì)", f"{very_short_count:,}ê±´")
        if total_count > 0:
            very_short_rate = (very_short_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {very_short_rate:.1f}%")
    with col3:
        st.metric("ì§§ìŒ (100-300ì)", f"{short_count:,}ê±´")
        if total_count > 0:
            short_rate = (short_count / total_count) * 100
            st.caption(f"ë¹„ìœ¨: {short_rate:.1f}%")
    with col4:
        total_issue = missing_count + very_short_count + short_count
        st.metric("ì´ ë¬¸ì œ ê¸°ì‚¬", f"{total_issue:,}ê±´")
        if total_count > 0:
            issue_rate = (total_issue / total_count) * 100
            st.caption(f"ë¬¸ì œ ë¹„ìœ¨: {issue_rate:.1f}%")
    
    if total_count > 0 and px is not None:
        quality_df = pd.DataFrame({
            "ìƒíƒœ": ["ì •ìƒ", "ëˆ„ë½", "ë§¤ìš° ì§§ìŒ (<100ì)", "ì§§ìŒ (100-300ì)"],
            "ê±´ìˆ˜": [total_count - total_issue, missing_count, very_short_count, short_count]
        })
        fig = px.pie(
            quality_df,
            values="ê±´ìˆ˜",
            names="ìƒíƒœ",
            title="ë³¸ë¬¸ í’ˆì§ˆ ìƒíƒœ",
            color_discrete_map={
                "ì •ìƒ": "#10b981",
                "ëˆ„ë½": "#ef4444",
                "ë§¤ìš° ì§§ìŒ (<100ì)": "#dc2626",
                "ì§§ìŒ (100-300ì)": "#b91c1c"
            }
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_title_content_duplication(news_df: pd.DataFrame):
    """ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µë¥ """
    if "title" not in news_df.columns or "content" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ”„ ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µë¥ ")
    
    # titleê³¼ contentê°€ ëª¨ë‘ ìˆëŠ” ë‰´ìŠ¤ë§Œ ë¶„ì„
    valid_news = news_df[
        news_df["title"].notna() & 
        (news_df["title"] != "") &
        news_df["content"].notna() & 
        (news_df["content"] != "")
    ].copy()
    
    if valid_news.empty:
        st.info("ğŸ“Š ì œëª©ê³¼ ë³¸ë¬¸ì´ ëª¨ë‘ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì œëª©ì´ ë³¸ë¬¸ ì•ë¶€ë¶„ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    def check_duplication(row):
        title = str(row["title"]).strip()
        content = str(row["content"]).strip()
        
        if not title or not content:
            return False
        
        # ë³¸ë¬¸ ì•ë¶€ë¶„(ì œëª© ê¸¸ì´ì˜ 2ë°°)ì—ì„œ ì œëª© í¬í•¨ ì—¬ë¶€ í™•ì¸
        content_preview = content[:len(title) * 2]
        return title in content_preview
    
    valid_news["has_duplication"] = valid_news.apply(check_duplication, axis=1)
    
    duplication_count = valid_news["has_duplication"].sum()
    total_count = len(valid_news)
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì¤‘ë³µ ë°œê²¬", f"{duplication_count:,}ê±´")
    with col2:
        if total_count > 0:
            dup_rate = (duplication_count / total_count) * 100
            st.metric("ì¤‘ë³µë¥ ", f"{dup_rate:.1f}%")
        else:
            st.metric("ì¤‘ë³µë¥ ", "N/A")
    
    if total_count > 0 and px is not None:
        dup_df = pd.DataFrame({
            "ìƒíƒœ": ["ì¤‘ë³µ ì—†ìŒ", "ì¤‘ë³µ ìˆìŒ"],
            "ê±´ìˆ˜": [total_count - duplication_count, duplication_count]
        })
        fig = px.pie(
            dup_df,
            values="ê±´ìˆ˜",
            names="ìƒíƒœ",
            title="ì œëª©Â·ë³¸ë¬¸ ì¤‘ë³µ ë¶„í¬"
        )
        st.plotly_chart(fig, use_container_width=True)

def _render_data_quality_consistency(news_df: pd.DataFrame):
    """
    ë°ì´í„° í’ˆì§ˆ ì¼ì¹˜ ì—¬ë¶€ ë¶„ì„: ì œëª©-URL-content-summary ì¼ì¹˜ë„
    
    ë¶„ì„ ê¸°ì¤€:
    1. ì œëª©-content ì¼ì¹˜ë„: ì œëª©ì˜ í•µì‹¬ í‚¤ì›Œë“œê°€ contentì— í¬í•¨ë˜ëŠ”ì§€ (ê°€ì¤‘ì¹˜: 70%)
    2. URL-content ì¼ì¹˜ë„: URL ë„ë©”ì¸/ê²½ë¡œì™€ contentì˜ ê´€ë ¨ì„± (ê°€ì¤‘ì¹˜: 10%)
    3. summary-content ì¼ì¹˜ë„: summaryê°€ contentë¥¼ ì •í™•íˆ ìš”ì•½í•˜ëŠ”ì§€ (ê°€ì¤‘ì¹˜: 20%)
    4. ì¢…í•© í’ˆì§ˆ ì ìˆ˜: ìœ„ 3ê°€ì§€ ê¸°ì¤€ì˜ ê°€ì¤‘ í‰ê· 
    """
    required_columns = ["title", "content"]
    missing_columns = [col for col in required_columns if col not in news_df.columns]
    
    if missing_columns:
        return
    
    st.markdown("#### ğŸ” ë°ì´í„° í’ˆì§ˆ ì¼ì¹˜ ì—¬ë¶€ ë¶„ì„ (ì œëª©-URL-content-summary)")
    st.markdown("**ëª©ì **: ì œëª©, URL, content, summary ê°„ì˜ ì¼ì¹˜ ì—¬ë¶€ë¥¼ í™•ì¸í•˜ì—¬ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ë°œê²¬")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ ë¶„ì„
    valid_news = news_df[
        news_df["title"].notna() & 
        (news_df["title"] != "") &
        news_df["content"].notna() & 
        (news_df["content"] != "")
    ].copy()
    
    if valid_news.empty:
        st.info("ğŸ“Š ì œëª©ê³¼ ë³¸ë¬¸ì´ ëª¨ë‘ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # news_idê°€ ì—†ìœ¼ë©´ display_id ì¶”ê°€ (í‘œì‹œìš©)
    if "news_id" not in valid_news.columns:
        valid_news["display_id"] = valid_news.index
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ)
    def extract_keywords(text, min_length=2):
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not text or pd.isna(text):
            return set()
        text_str = str(text).strip()
        if not text_str:
            return set()
        
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        import re
        keywords = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text_str)
        # ìµœì†Œ ê¸¸ì´ ì´ìƒì´ê³ , ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
        stopwords = {'ê·¸', 'ì´', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ë“±', 'ë°', 'ë˜', 'ë˜í•œ', 'ê·¸ë¦¬ê³ ', 
                     'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had'}
        keywords = [kw for kw in keywords if len(kw) >= min_length and kw.lower() not in stopwords]
        return set(keywords)
    
    # ì œëª©-content ì¼ì¹˜ë„ ê³„ì‚° (ë¶ˆëŸ‰ ë‰´ìŠ¤ í•„í„°ë§ì„ ìœ„í•´ ì—„ê²©í•œ ê¸°ì¤€)
    def calculate_title_content_match(row):
        """ì œëª©ê³¼ contentì˜ ì¼ì¹˜ë„ ê³„ì‚° (0-100) - ì—„ê²©í•œ ê¸°ì¤€"""
        title = str(row.get("title", "")).strip()
        content = str(row.get("content", "")).strip()
        
        if not title or not content:
            return 30  # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë‚®ì€ ì ìˆ˜
        
        # content ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ìœ¼ë©´ ê°ì )
        content_length = len(content)
        length_penalty = 0
        if content_length < 100:  # 100ì ë¯¸ë§Œ
            length_penalty = -30  # 30ì  ê°ì 
        elif content_length < 200:  # 200ì ë¯¸ë§Œ
            length_penalty = -20  # 20ì  ê°ì 
        elif content_length < 300:  # 300ì ë¯¸ë§Œ
            length_penalty = -10  # 10ì  ê°ì 
        
        # ì œëª©ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        title_keywords = extract_keywords(title, min_length=2)
        if not title_keywords:
            return 30  # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë‚®ì€ ì ìˆ˜
        
        # contentì˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ì•ë¶€ë¶„ 2000ì)
        content_preview = content[:2000]
        content_keywords = extract_keywords(content_preview, min_length=2)
        
        # ì œëª© í‚¤ì›Œë“œê°€ contentì— í¬í•¨ëœ ë¹„ìœ¨
        matched_keywords = title_keywords & content_keywords
        match_ratio = len(matched_keywords) / len(title_keywords) if title_keywords else 0
        
        # ì œëª© ìì²´ê°€ content ì•ë¶€ë¶„ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸ (ë” ì—„ê²©í•˜ê²Œ)
        title_in_content = title in content[:300]  # ë²”ìœ„ ì¶•ì†Œ
        title_match_bonus = 20 if title_in_content else 0
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ì´ í•µì‹¬ (ë” ì—„ê²©í•œ ê¸°ì¤€)
        # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ ë‚®ì€ ì ìˆ˜
        if match_ratio < 0.3:  # 30% ë¯¸ë§Œ ë§¤ì¹­
            base_score = 20
        elif match_ratio < 0.5:  # 50% ë¯¸ë§Œ ë§¤ì¹­
            base_score = 35
        elif match_ratio < 0.7:  # 70% ë¯¸ë§Œ ë§¤ì¹­
            base_score = 50
        else:  # 70% ì´ìƒ ë§¤ì¹­
            base_score = 65
        
        # ìµœì¢… ì ìˆ˜: ê¸°ë³¸ ì ìˆ˜ + í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨(30%) + ì œëª© í¬í•¨ ì—¬ë¶€(ë³´ë„ˆìŠ¤) + ê¸¸ì´ ê°ì 
        score = base_score + (match_ratio * 30) + title_match_bonus + length_penalty
        return min(100, max(0, int(score)))  # ìµœì†Œ ì ìˆ˜ ë³´ì¥ ì œê±°
    
    # URL-content ì¼ì¹˜ë„ ê³„ì‚°
    def calculate_url_content_match(row):
        """URLê³¼ contentì˜ ì¼ì¹˜ë„ ê³„ì‚° (0-100) - ì™„í™”ëœ ê¸°ì¤€"""
        url = str(row.get("url", "")).strip() if pd.notna(row.get("url")) else ""
        content = str(row.get("content", "")).strip()
        
        if not url or not content:
            return 60  # URLì´ ì—†ì–´ë„ ê¸°ë³¸ ì ìˆ˜ (ì™„í™”)
        
        # URLì—ì„œ ë„ë©”ì¸ê³¼ ê²½ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            domain = parsed.netloc.replace("www.", "")
            path = parsed.path
            
            # ë„ë©”ì¸ê³¼ ê²½ë¡œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
            url_keywords = extract_keywords(domain + " " + path, min_length=2)
            
            # contentì—ì„œ URL í‚¤ì›Œë“œê°€ í¬í•¨ë˜ëŠ”ì§€ í™•ì¸ (ë²”ìœ„ í™•ëŒ€)
            content_keywords = extract_keywords(content[:1000], min_length=2)
            matched = url_keywords & content_keywords
            
            # URLì´ contentì— ì§ì ‘ ì–¸ê¸‰ë˜ëŠ”ì§€ í™•ì¸
            url_mentioned = domain in content or url in content
            
            # ì ìˆ˜ ê³„ì‚° (ì™„í™”ëœ ê¸°ì¤€)
            if url_mentioned:
                return 100
            elif matched:
                match_ratio = len(matched) / len(url_keywords) if url_keywords else 0
                # ê¸°ë³¸ ì ìˆ˜ 50ì  + ë§¤ì¹­ ë¹„ìœ¨ì— ë”°ë¥¸ ì¶”ê°€ ì ìˆ˜
                return min(100, int(50 + match_ratio * 50))
            else:
                # URLì´ ìˆê³  contentë„ ìˆìœ¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ê´€ë ¨ ìˆë‹¤ê³  ê°€ì • (ì™„í™”)
                return 50  # ê´€ë ¨ì„± ë‚®ì•„ë„ ê¸°ë³¸ ì ìˆ˜
        except:
            return 60  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì ìˆ˜ (ì™„í™”)
    
    # summary-content ì¼ì¹˜ë„ ê³„ì‚°
    def calculate_summary_content_match(row):
        """summaryì™€ contentì˜ ì¼ì¹˜ë„ ê³„ì‚° (0-100) - ì™„í™”ëœ ê¸°ì¤€"""
        summary = str(row.get("summary", "")).strip() if pd.notna(row.get("summary")) else ""
        content = str(row.get("content", "")).strip()
        
        if not summary or not content:
            return 60  # summaryê°€ ì—†ì–´ë„ ê¸°ë³¸ ì ìˆ˜ (ì™„í™”)
        
        # summaryì˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        summary_keywords = extract_keywords(summary, min_length=2)
        if not summary_keywords:
            return 60  # í‚¤ì›Œë“œê°€ ì—†ì–´ë„ ê¸°ë³¸ ì ìˆ˜ (ì™„í™”)
        
        # contentì˜ í‚¤ì›Œë“œ ì¶”ì¶œ (ì•ë¶€ë¶„ 3000ìë¡œ í™•ëŒ€)
        content_preview = content[:3000]
        content_keywords = extract_keywords(content_preview, min_length=2)
        
        # summary í‚¤ì›Œë“œê°€ contentì— í¬í•¨ëœ ë¹„ìœ¨
        matched_keywords = summary_keywords & content_keywords
        match_ratio = len(matched_keywords) / len(summary_keywords) if summary_keywords else 0
        
        # summaryê°€ contentì˜ ì•ë¶€ë¶„ê³¼ ìœ ì‚¬í•œì§€ í™•ì¸
        content_start_keywords = extract_keywords(content[:1000], min_length=2)
        start_match_ratio = len(summary_keywords & content_start_keywords) / len(summary_keywords) if summary_keywords else 0
        
        # ìµœì†Œí•œ 1ê°œ ì´ìƒ í‚¤ì›Œë“œê°€ ë§¤ì¹­ë˜ë©´ ê¸°ë³¸ ì ìˆ˜ ë¶€ì—¬ (ì™„í™”)
        base_score = 50 if len(matched_keywords) > 0 else 45
        
        # ìµœì¢… ì ìˆ˜: ê¸°ë³¸ ì ìˆ˜ + í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨(40%) + ì•ë¶€ë¶„ ì¼ì¹˜(10%)
        score = base_score + (match_ratio * 40) + (start_match_ratio * 10)
        return min(100, max(45, int(score)))  # ìµœì†Œ 45ì  ë³´ì¥
    
    # ê° ë‰´ìŠ¤ì— ëŒ€í•´ ì¼ì¹˜ë„ ê³„ì‚°
    valid_news["title_content_match"] = valid_news.apply(calculate_title_content_match, axis=1)
    valid_news["url_content_match"] = valid_news.apply(calculate_url_content_match, axis=1)
    valid_news["summary_content_match"] = valid_news.apply(calculate_summary_content_match, axis=1)
    
    # ì¢…í•© í’ˆì§ˆ ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
    # ì œëª©-content: 70% (ê°€ì¤‘ì¹˜ í¬ê²Œ), URL-content: 10%, summary-content: 20%
    valid_news["quality_score"] = (
        valid_news["title_content_match"] * 0.7 +
        valid_news["url_content_match"] * 0.1 +
        valid_news["summary_content_match"] * 0.2
    ).round(1)
    
    # í’ˆì§ˆ ë“±ê¸‰ ë¶„ë¥˜ (ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„ê°€ ë‚®ìœ¼ë©´ ë“±ê¸‰ ê°•ë“±, ë³´í†µ ì¹´í…Œê³ ë¦¬ ì œê±°)
    def get_quality_grade(row):
        score = row["quality_score"]
        title_match = row["title_content_match"]
        
        # 40ì  ì´ìƒì´ë©´ ë¬´ë‚œí•œ í’ˆì§ˆë¡œ ê°„ì£¼ (ë¶ˆëŸ‰ ì œì™¸)
        if score >= 40:
            # ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„ê°€ 50ì  ë¯¸ë§Œì´ë©´ ë“±ê¸‰ì„ í•œ ë‹¨ê³„ ë‚®ì¶¤
            if title_match < 50:
                if score >= 70:
                    return "ì–‘í˜¸"  # ìš°ìˆ˜ â†’ ì–‘í˜¸
                elif score >= 50:
                    return "ì–‘í˜¸"  # ì–‘í˜¸ ìœ ì§€
                else:  # 40-50ì 
                    return "ì–‘í˜¸"  # ë¬´ë‚œí•œ í’ˆì§ˆë¡œ ì–‘í˜¸ ì²˜ë¦¬
            
            # ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„ê°€ 50-60ì ì´ë©´ ë“±ê¸‰ì„ í•œ ë‹¨ê³„ ë‚®ì¶¤
            if title_match < 60:
                if score >= 70:
                    return "ì–‘í˜¸"  # ìš°ìˆ˜ â†’ ì–‘í˜¸
                elif score >= 50:
                    return "ì–‘í˜¸"  # ì–‘í˜¸ ìœ ì§€
                else:  # 40-50ì 
                    return "ì–‘í˜¸"  # ë¬´ë‚œí•œ í’ˆì§ˆë¡œ ì–‘í˜¸ ì²˜ë¦¬
            
            # ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„ê°€ ì •ìƒì´ë©´ ì¼ë°˜ ê¸°ì¤€ ì ìš©
            if score >= 70:
                return "ìš°ìˆ˜"
            elif score >= 50:
                return "ì–‘í˜¸"
            else:  # 40-50ì 
                return "ì–‘í˜¸"  # ë¬´ë‚œí•œ í’ˆì§ˆë¡œ ì–‘í˜¸ ì²˜ë¦¬
        
        # 40ì  ë¯¸ë§Œë§Œ ë¶ˆëŸ‰ìœ¼ë¡œ ë¶„ë¥˜
        return "ë¶ˆëŸ‰"
    
    valid_news["quality_grade"] = valid_news.apply(get_quality_grade, axis=1)
    
    # í†µê³„ ìš”ì•½
    total_count = len(valid_news)
    
    # íˆìŠ¤í† ê·¸ë¨ í‰ê· ì„  í‘œì‹œìš© í‰ê·  ê³„ì‚° (ì•ˆì „í•˜ê²Œ ì²˜ë¦¬)
    avg_quality_score = 0.0
    if not valid_news.empty and "quality_score" in valid_news.columns:
        avg_quality_score = valid_news["quality_score"].mean()
        if pd.isna(avg_quality_score):
            avg_quality_score = 0.0
    
    # í’ˆì§ˆ ë“±ê¸‰ë³„ ê¸°ìˆ í†µê³„
    st.markdown("##### ğŸ“Š í’ˆì§ˆ ë“±ê¸‰ë³„ ì ìˆ˜ ê¸°ìˆ í†µê³„")
    
    grade_stats_list = []
    for grade in ["ìš°ìˆ˜", "ì–‘í˜¸", "ë¶ˆëŸ‰"]:
        grade_data = valid_news[valid_news["quality_grade"] == grade]
        if not grade_data.empty:
            stats = {
                "ë“±ê¸‰": grade,
                "ê°œìˆ˜": len(grade_data),
                "í‰ê· ": grade_data["quality_score"].mean(),
                "í‘œì¤€í¸ì°¨": grade_data["quality_score"].std(),
                "ìµœì†Œê°’": grade_data["quality_score"].min(),
                "ìµœëŒ€ê°’": grade_data["quality_score"].max(),
                "ì¤‘ê°„ê°’": grade_data["quality_score"].median()
            }
            grade_stats_list.append(stats)
    
    if grade_stats_list:
        grade_stats_df = pd.DataFrame(grade_stats_list)
        # ì†Œìˆ˜ì  2ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼
        grade_stats_df["í‰ê· "] = grade_stats_df["í‰ê· "].round(2)
        grade_stats_df["í‘œì¤€í¸ì°¨"] = grade_stats_df["í‘œì¤€í¸ì°¨"].round(2)
        grade_stats_df["ìµœì†Œê°’"] = grade_stats_df["ìµœì†Œê°’"].round(2)
        grade_stats_df["ìµœëŒ€ê°’"] = grade_stats_df["ìµœëŒ€ê°’"].round(2)
        grade_stats_df["ì¤‘ê°„ê°’"] = grade_stats_df["ì¤‘ê°„ê°’"].round(2)
        
        # í‘œì¤€í¸ì°¨ê°€ NaNì¸ ê²½ìš° 0ìœ¼ë¡œ ì²˜ë¦¬
        grade_stats_df["í‘œì¤€í¸ì°¨"] = grade_stats_df["í‘œì¤€í¸ì°¨"].fillna(0)
        
        st.dataframe(grade_stats_df, use_container_width=True, height=200)
    
    # í’ˆì§ˆ ë“±ê¸‰ ë¶„í¬ (ë³´í†µ ì œê±°)
    grade_counts = valid_news["quality_grade"].value_counts()
    grade_order = ["ìš°ìˆ˜", "ì–‘í˜¸", "ë¶ˆëŸ‰"]
    grade_counts = grade_counts.reindex(grade_order, fill_value=0)
    
    if px is not None and len(grade_counts) > 0:
        grade_df = pd.DataFrame({
            "ë“±ê¸‰": grade_counts.index,
            "ê±´ìˆ˜": grade_counts.values
        })
        fig = px.pie(
            grade_df,
            values="ê±´ìˆ˜",
            names="ë“±ê¸‰",
            title="ë°ì´í„° í’ˆì§ˆ ë“±ê¸‰ ë¶„í¬",
            color="ë“±ê¸‰",
            color_discrete_map={"ìš°ìˆ˜": "#10b981", "ì–‘í˜¸": "#3b82f6", "ë¶ˆëŸ‰": "#ef4444"}
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # í’ˆì§ˆ ì ìˆ˜ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
    if px is not None:
        fig = px.histogram(
            valid_news,
            x="quality_score",
            nbins=20,
            title="ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ë¶„í¬",
            labels={"quality_score": "í’ˆì§ˆ ì ìˆ˜", "count": "ë‰´ìŠ¤ ìˆ˜"},
            color_discrete_sequence=["#3b82f6"]
        )
        fig.add_vline(x=avg_quality_score, line_dash="dash", line_color="red", 
                     annotation_text=f"í‰ê· : {avg_quality_score:.1f}ì ")
        st.plotly_chart(fig, use_container_width=True)
    
    # ë¶ˆëŸ‰ ë°ì´í„° ìƒì„¸ ëª©ë¡ (ë¶ˆëŸ‰ ë“±ê¸‰ë§Œ í•„í„°ë§)
    st.markdown("#### âš ï¸ ë¶ˆëŸ‰ ë‰´ìŠ¤ (ì „ì²´ ëª©ë¡)")
    
    # ë¶ˆëŸ‰ ë“±ê¸‰ì¸ ë‰´ìŠ¤ë§Œ í•„í„°ë§
    bad_quality_news = valid_news[valid_news["quality_grade"] == "ë¶ˆëŸ‰"].copy()
    
    if bad_quality_news.empty:
        st.info("âœ… ë¶ˆëŸ‰ ë“±ê¸‰ì¸ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # news_id ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í¬í•¨, ì—†ìœ¼ë©´ display_id ì‚¬ìš©
        display_columns = []
        if "news_id" in bad_quality_news.columns:
            display_columns.append("news_id")
        elif "display_id" in bad_quality_news.columns:
            display_columns.append("display_id")
        
        display_columns.extend(["title", "url", "content", "quality_score", "title_content_match", 
                               "url_content_match", "summary_content_match", "quality_grade"])
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        available_columns = [col for col in display_columns if col in bad_quality_news.columns]
        # í’ˆì§ˆ ì ìˆ˜ ë‚®ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        bad_quality = bad_quality_news.nsmallest(len(bad_quality_news), "quality_score")[available_columns].copy()
        
        # ë¶ˆëŸ‰ ë‰´ìŠ¤ ê°œìˆ˜ í‘œì‹œ
        st.markdown(f"**ì´ {len(bad_quality):,}ê±´ì˜ ë¶ˆëŸ‰ ë‰´ìŠ¤ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.**")
        
        # ë¶ˆëŸ‰ ë‰´ìŠ¤ì˜ ë‰´ìŠ¤ì²˜(ì–¸ë¡ ì‚¬) ë¹„ìœ¨ íŒŒì´ê·¸ë˜í”„
        if "source" in bad_quality_news.columns:
            source_counts = bad_quality_news["source"].value_counts()
            if len(source_counts) > 0:
                st.markdown("##### ğŸ“Š ë¶ˆëŸ‰ ë‰´ìŠ¤ ë‰´ìŠ¤ì²˜(ì–¸ë¡ ì‚¬) ë¹„ìœ¨")
                
                # ë‰´ìŠ¤ì²˜ë³„ ê°œìˆ˜ì™€ ë¹„ìœ¨ ê³„ì‚°
                source_df = pd.DataFrame({
                    "ë‰´ìŠ¤ì²˜": source_counts.index,
                    "ê±´ìˆ˜": source_counts.values
                })
                source_df["ë¹„ìœ¨ (%)"] = (source_df["ê±´ìˆ˜"] / len(bad_quality_news) * 100).round(1)
                
                # íŒŒì´ê·¸ë˜í”„ ìƒì„±
                if px is not None:
                    fig_source = px.pie(
                        source_df,
                        values="ê±´ìˆ˜",
                        names="ë‰´ìŠ¤ì²˜",
                        title="ë¶ˆëŸ‰ ë‰´ìŠ¤ ë‰´ìŠ¤ì²˜(ì–¸ë¡ ì‚¬) ë¹„ìœ¨",
                        hover_data=["ë¹„ìœ¨ (%)"]
                    )
                    fig_source.update_layout(
                        height=400,
                        showlegend=True
                    )
                    st.plotly_chart(fig_source, use_container_width=True)
    
        # ì»¬ëŸ¼ëª… í•œê¸€í™”
        column_mapping = {
            "news_id": "ë‰´ìŠ¤ ID",
            "display_id": "ë‰´ìŠ¤ ID",
            "title": "ì œëª©",
            "url": "URL",
            "content": "ë³¸ë¬¸ ë‚´ìš©",
            "quality_score": "ì¢…í•© ì ìˆ˜",
            "title_content_match": "ì œëª©-ë³¸ë¬¸ ì¼ì¹˜ë„",
            "url_content_match": "URL-ë³¸ë¬¸ ì¼ì¹˜ë„",
            "summary_content_match": "ìš”ì•½-ë³¸ë¬¸ ì¼ì¹˜ë„",
            "quality_grade": "ë“±ê¸‰"
        }
        bad_quality.columns = [column_mapping.get(col, col) for col in bad_quality.columns]
        
        # ì œëª©, URL, ë³¸ë¬¸ ë‚´ìš© ê¸¸ì´ ì œí•œ (í‘œì‹œìš©)
        if "ì œëª©" in bad_quality.columns:
            bad_quality["ì œëª©"] = bad_quality["ì œëª©"].apply(lambda x: str(x)[:50] + "..." if len(str(x)) > 50 else str(x))
        if "URL" in bad_quality.columns:
            bad_quality["URL"] = bad_quality["URL"].apply(lambda x: str(x)[:50] + "..." if len(str(x)) > 50 else str(x))
        if "ë³¸ë¬¸ ë‚´ìš©" in bad_quality.columns:
            # ë³¸ë¬¸ ë‚´ìš©ì€ 200ìë¡œ ì œí•œ (ë” ê¸¸ê²Œ í‘œì‹œ)
            bad_quality["ë³¸ë¬¸ ë‚´ìš©"] = bad_quality["ë³¸ë¬¸ ë‚´ìš©"].apply(
                lambda x: str(x)[:200] + "..." if len(str(x)) > 200 else str(x) if pd.notna(x) and str(x).strip() else "(ë‚´ìš© ì—†ìŒ)"
            )
        
        st.dataframe(bad_quality, use_container_width=True, height=600)
    

def _render_duplicate_news_analysis(news_df: pd.DataFrame):
    """ì¤‘ë³µ ê¸°ì‚¬ ë¹„ìœ¨"""
    if "url" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ” ì¤‘ë³µ ê¸°ì‚¬ ë¹„ìœ¨")
    
    # urlì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_url = news_df[news_df["url"].notna() & (news_df["url"] != "")]
    
    if news_with_url.empty:
        st.info("ğŸ“Š URLì´ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ í™•ì¸
    url_counts = news_with_url["url"].value_counts()
    duplicate_urls = url_counts[url_counts > 1]
    
    total_news = len(news_with_url)
    unique_urls = len(url_counts)
    duplicate_count = len(duplicate_urls)
    total_duplicate_instances = duplicate_urls.sum() - duplicate_count  # ì¤‘ë³µ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê³ ìœ  URL ìˆ˜", f"{unique_urls:,}ê°œ")
    with col2:
        st.metric("ì¤‘ë³µ URL ìˆ˜", f"{duplicate_count:,}ê°œ")
    with col3:
        if total_news > 0:
            dup_rate = (total_duplicate_instances / total_news) * 100
            st.metric("ì¤‘ë³µ ë¹„ìœ¨", f"{dup_rate:.1f}%")
        else:
            st.metric("ì¤‘ë³µ ë¹„ìœ¨", "N/A")
    
    # ì¤‘ë³µì´ ë§ì€ URL Top 10
    if len(duplicate_urls) > 0:
        top_duplicates = duplicate_urls.head(10).reset_index()
        top_duplicates.columns = ["URL", "ì¤‘ë³µ íšŸìˆ˜"]
        st.dataframe(top_duplicates, use_container_width=True, height=200)

def _render_news_collection_trends(news_df: pd.DataFrame):
    """RSS/í¬ë¡¤ë§ë³„ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸"""
    if "published_at" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“ˆ ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì„¸")
    
    # published_atì´ ìˆëŠ” ë‰´ìŠ¤ë§Œ í•„í„°ë§
    news_with_date = news_df[news_df["published_at"].notna()].copy()
    
    if news_with_date.empty:
        st.info("ğŸ“Š ë°œí–‰ì¼ ì •ë³´ê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì¼ë³„ ìˆ˜ì§‘ëŸ‰
    news_with_date["date"] = news_with_date["published_at"].dt.date
    daily_counts = news_with_date.groupby("date").size().reset_index(name="ê±´ìˆ˜")
    daily_counts = daily_counts.sort_values("date")
    
    col1, col2 = st.columns(2)
    with col1:
        total_news = len(news_with_date)
        st.metric("ì´ ë‰´ìŠ¤ ìˆ˜", f"{total_news:,}ê±´")
    with col2:
        if len(daily_counts) > 0:
            avg_daily = daily_counts["ê±´ìˆ˜"].mean()
            st.metric("ì¼í‰ê·  ìˆ˜ì§‘ëŸ‰", f"{avg_daily:.1f}ê±´")
    
    if px is not None and len(daily_counts) > 0:
        fig = px.line(
            daily_counts,
            x="date",
            y="ê±´ìˆ˜",
            title="ì¼ë³„ ë‰´ìŠ¤ ìˆ˜ì§‘ëŸ‰ ì¶”ì´",
            labels={"date": "ë‚ ì§œ", "ê±´ìˆ˜": "ìˆ˜ì§‘ ê±´ìˆ˜"}
        )
        st.plotly_chart(fig, use_container_width=True)
    
    # sourceë³„ ìˆ˜ì§‘ëŸ‰ (sourceê°€ ìˆëŠ” ê²½ìš°)
    if "source" in news_df.columns:
        news_with_source = news_with_date[news_with_date["source"].notna() & (news_with_date["source"] != "")]
        if not news_with_source.empty:
            source_counts = news_with_source["source"].value_counts().head(10)
            
            if px is not None and len(source_counts) > 0:
                source_df = source_counts.reset_index()
                source_df.columns = ["ì¶œì²˜", "ê±´ìˆ˜"]
                fig2 = px.bar(
                    source_df,
                    x="ì¶œì²˜",
                    y="ê±´ìˆ˜",
                    title="ì¶œì²˜ë³„ ìˆ˜ì§‘ëŸ‰ Top 10",
                    labels={"ì¶œì²˜": "ì–¸ë¡ ì‚¬/ì†ŒìŠ¤", "ê±´ìˆ˜": "ìˆ˜ì§‘ ê±´ìˆ˜"}
                )
                fig2.update_xaxes(tickangle=-45)
                st.plotly_chart(fig2, use_container_width=True)

def _get_korean_font_path():
    """í•œê¸€ í°íŠ¸ ê²½ë¡œ ì°¾ê¸°"""
    # Windows í°íŠ¸ ê²½ë¡œë“¤
    windows_font_paths = [
        "C:/Windows/Fonts/NanumGothic.ttf",
        "C:/Windows/Fonts/NanumBarunGothic.ttf",
        "C:/Windows/Fonts/malgun.ttf",  # ë§‘ì€ ê³ ë”•
        "C:/Windows/Fonts/gulim.ttc",  # êµ´ë¦¼
    ]
    
    # Linux/Mac í°íŠ¸ ê²½ë¡œë“¤
    linux_font_paths = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf",
        "/System/Library/Fonts/AppleGothic.ttf",  # Mac
    ]
    
    # Windows ê²½ë¡œ í™•ì¸
    for font_path in windows_font_paths:
        if os.path.exists(font_path):
            return font_path
    
    # Linux/Mac ê²½ë¡œ í™•ì¸
    for font_path in linux_font_paths:
        if os.path.exists(font_path):
            return font_path
    
    return None

def _extract_korean_words(text: str) -> List[str]:
    """í•œêµ­ì–´ ë‹¨ì–´ ì¶”ì¶œ (ì¡°ì‚¬/ë¶ˆìš©ì–´ ì œê±°)"""
    if not text:
        return []
    
    # í•œêµ­ì–´ ë¶ˆìš©ì–´ ë° ì¡°ì‚¬ (ë‰´ìŠ¤ íŠ¹í™”)
    stopwords = {
        # ì¡°ì‚¬
        "ì€", "ëŠ”", "ì´", "ê°€", "ì„", "ë¥¼", "ì˜", "ì—", "ì™€", "ê³¼", "ë„", "ë¡œ", "ìœ¼ë¡œ",
        "ì—ì„œ", "ì—ê²Œ", "í•œí…Œ", "ê»˜", "ë¶€í„°", "ê¹Œì§€", "ë§Œ", "ì¡°ì°¨", "ë§ˆì €", "ë¿",
        
        # ëŒ€ëª…ì‚¬
        "ê·¸", "ê·¸ê²ƒ", "ì´ê²ƒ", "ì €ê²ƒ", "ê·¸ë“¤", "ì´ë“¤", "ì €ë“¤", "ê·¸ë…€", "ê·¸ë¶„",
        
        # ì ‘ì†ì‚¬/ë¶€ì‚¬
        "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ê·¸ëŸ¬ë‚˜", "í•˜ì§€ë§Œ", "ê·¸ëŸ°ë°", "ê·¸ë˜ì„œ", "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ",
        "ë•Œë¬¸ì—", "ìœ„í•´", "ëŒ€í•´", "ê´€ë ¨", "ë”", "ë˜", "ë˜ëŠ”", "ë°",
        
        # ë‰´ìŠ¤ ê´€ë ¨ ìš©ì–´
        "ê¸°ì", "ì—°í•©ë‰´ìŠ¤", "ë‰´ìŠ¤", "ê¸°ì‚¬", "ë³´ë„", "ì·¨ì¬", "ì „ë¬¸", "ì¸í„°ë·°",
        "ë°œí‘œ", "ë°œìƒ", "í™•ì¸", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "í–ˆë‹¤", "ìˆë‹¤", "ì—†ë‹¤",
        "ì „ë§", "ì˜ˆìƒ", "ì˜ˆì¸¡", "ì¶”ì •", "ë¶„ì„", "ì¡°ì‚¬", "ê²°ê³¼", "ë°œê²¬",
        "ì´ë²ˆ", "ì´ë‚ ", "ì˜¤ëŠ˜", "ì–´ì œ", "ë‚´ì¼", "ì§€ë‚œ", "ì˜¬í•´", "ì‘ë…„", "ë‚´ë…„",
        "ì§€ë‚œí•´", "ì˜¬", "ì‘", "ë‚´",
        
        # ì‹œê°„ ê´€ë ¨
        "ì›”", "ì¼", "ë…„", "ì‹œ", "ë¶„", "ì´ˆ", "ìš”ì¼", "ì£¼", "ê°œì›”", "ë…„ë„",
        "ì˜¤ì „", "ì˜¤í›„", "ìƒˆë²½", "ë°¤", "ë‚®", "ì €ë…", "ë‚ ì§œ",
        
        # ìˆ˜ëŸ‰/ë‹¨ìœ„ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ë§Œ", "ì–µ", "ì¡°", "ì›", "ë‹¬ëŸ¬", "í¼ì„¼íŠ¸", "í”„ë¡œ",
        "ê°œ", "ëª…", "ê±´", "ê³³", "ì°¨ë¡€", "ë²ˆ", "íšŒ", "ì°¨",
        
        # ì¼ë°˜ ë™ì‚¬/í˜•ìš©ì‚¬ (ë„ˆë¬´ ë¹ˆë²ˆí•œ ê²ƒë“¤)
        "ìˆë‹¤", "ì—†ë‹¤", "ë˜ë‹¤", "í•˜ë‹¤", "ì´ë‹¤", "ì•Šë‹¤", "ì•„ë‹ˆë‹¤",
        "ê°™ë‹¤", "ë‹¤ë¥´ë‹¤", "í¬ë‹¤", "ì‘ë‹¤", "ë§ë‹¤", "ì ë‹¤", "ë†’ë‹¤", "ë‚®ë‹¤",
        "ì¢‹ë‹¤", "ë‚˜ì˜ë‹¤", "ìƒˆë¡­ë‹¤", "ì˜¤ë˜ë˜ë‹¤", "ìµœê·¼", "ìµœì‹ ",
        "ìˆëŠ”", "ì—†ëŠ”", "ë˜ëŠ”", "í•˜ëŠ”", "ê²ƒìœ¼ë¡œ", "ê²ƒì´ë‹¤", "ê²ƒì´", "ê²ƒì„", "ê²ƒì„",
        
        # ì¼ë°˜ì ì¸ í˜•ìš©ì‚¬/ë¶€ì‚¬
        "ë§¤ìš°", "ì•„ì£¼", "ë„ˆë¬´", "ì •ë§", "ì§„ì§œ", "ì™„ì „", "ì „í˜€", "ë³„ë¡œ",
        "ê°€ì¥", "ìµœê³ ", "ìµœëŒ€", "ìµœì†Œ",
        
        # ë‰´ìŠ¤ ë¬¸ì²´ íŠ¹í™”
        "ë”°ë¥´ë©´", "ì— ë”°ë¥´ë©´", "ë°í˜”ë‹¤", "ë§í–ˆë‹¤", "ì „í–ˆë‹¤", "ì•Œë ¤ì¡Œë‹¤",
        "í™•ì¸ëë‹¤", "ë°œìƒí–ˆë‹¤", "ë‚˜íƒ€ë‚¬ë‹¤", "ì§€ì í–ˆë‹¤", "ê°•ì¡°í–ˆë‹¤",
        "ì£¼ì¥í–ˆë‹¤", "ì œê¸°í–ˆë‹¤", "ìš”êµ¬í–ˆë‹¤", "ì´‰êµ¬í–ˆë‹¤", "ì œì•ˆí–ˆë‹¤",
        "ë°œí‘œí–ˆë‹¤", "ê³µê°œí–ˆë‹¤", "ë°œí‘œëë‹¤", "ê³µê°œëë‹¤",
        
        # ê¸°ê´€/ì§ì±… (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ì •ë¶€", "êµ­ê°€", "ê¸°ê´€", "ë‹¨ì²´", "ì¡°ì§", "íšŒì‚¬", "ê¸°ì—…", "ë²•ì¸",
        "ëŒ€í‘œ", "ì‚¬ì¥", "íšŒì¥", "ì´ì‚¬", "ì§ì›", "ê´€ê³„ì", "ë‹¹êµ­",
        
        # ì¼ë°˜ì ì¸ ëª…ì‚¬ (ë‰´ìŠ¤ì—ì„œ ë„ˆë¬´ ìì£¼ ë‚˜ì˜¤ëŠ” ê²ƒë“¤)
        "ì‚¬ì‹¤", "ë‚´ìš©", "ìƒí™©", "ë¬¸ì œ", "ì´ìŠˆ", "ì‚¬ê±´", "ì‚¬ê³ ", "ì‚¬ë¡€",
        "ê²½ìš°", "ë•Œë¬¸", "ì´ìœ ", "ì›ì¸", "ê²°ê³¼", "ì˜í–¥", "íš¨ê³¼",
        "ë°©ë²•", "ë°©ì•ˆ", "ëŒ€ì±…", "ì •ì±…", "ì œë„", "ì‹œìŠ¤í…œ",
        "ê³¼ì •", "ì ˆì°¨", "ë‹¨ê³„", "ìˆ˜ì¤€", "ì •ë„", "ë²”ìœ„",
        "êµ­ë‚´", "ëŒ€ë¹„",
        
        # ìœ„ì¹˜/ë°©í–¥ (ë„ˆë¬´ ì¼ë°˜ì ì¸ ê²ƒë“¤)
        "ìœ„", "ì•„ë˜", "ì•", "ë’¤", "ì™¼ìª½", "ì˜¤ë¥¸ìª½", "ì¤‘ì•™", "ì¤‘ì‹¬",
        "ë‚´ë¶€", "ì™¸ë¶€", "ì•ìª½", "ë’¤ìª½", "ì–‘ìª½",
        
        # ê¸°íƒ€ ë¹ˆë²ˆí•œ ë‹¨ì–´
        "ë“±", "ë°", "ë˜", "ê·¸ë¦¬ê³ ", "ê·¸ëŸ¬ë‚˜",
        "ì´ë¯¸", "ì•„ì§", "ë²Œì¨", "ê³§", "ê³§ë°”ë¡œ", "ì¦‰ì‹œ", "ë°”ë¡œ",
    }
    
    # ì˜ë¯¸ì—†ëŠ” ìˆ«ì íŒ¨í„´ ì œê±° (1-31ì¼, 1-12ì›” ë“±)
    # ë‹¨ì–´ ì¶”ì¶œ í›„ ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±°
    
    # í•œê¸€ë§Œ ì¶”ì¶œ (í•œê¸€, ê³µë°±, ìˆ«ì)
    korean_text = re.sub(r'[^ê°€-í£\s0-9]', ' ', text)
    
    # ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
    words = korean_text.split()
    
    # í•„í„°ë§ í•¨ìˆ˜
    def should_keep_word(word):
        word = word.strip()
        
        # 2ê¸€ì ë¯¸ë§Œ ì œê±°
        if len(word) < 2:
            return False
        
        # ë¶ˆìš©ì–´ ì œê±°
        if word in stopwords:
            return False
        
        # ì˜ë¯¸ì—†ëŠ” ìˆ«ì íŒ¨í„´ ì œê±° (1-31ì¼, 1-12ì›” ë“±)
        # ìˆ«ìë§Œ ìˆê±°ë‚˜ ìˆ«ì+ì¼/ì›”/ë…„/ì‹œ/ë¶„ ë“±ìœ¼ë¡œ ëë‚˜ëŠ” ë‹¨ì–´
        if re.match(r'^\d+[ì¼ì›”ë…„ì‹œë¶„ì´ˆ]?$', word):
            # 1-31ì¼, 1-12ì›” ê°™ì€ ì˜ë¯¸ì—†ëŠ” ìˆ«ìëŠ” ì œê±°
            num_match = re.match(r'^(\d+)([ì¼ì›”ë…„ì‹œë¶„ì´ˆ]?)$', word)
            if num_match:
                num = int(num_match.group(1))
                suffix = num_match.group(2)
                # 1-31ì¼, 1-12ì›” ê°™ì€ íŒ¨í„´ì€ ì œê±°
                if suffix in ['ì¼', 'ì›”'] and 1 <= num <= 31:
                    return False
                # 1-24ì‹œ ê°™ì€ íŒ¨í„´ë„ ì œê±°
                if suffix == 'ì‹œ' and 1 <= num <= 24:
                    return False
                # 1-60ë¶„ ê°™ì€ íŒ¨í„´ë„ ì œê±°
                if suffix == 'ë¶„' and 1 <= num <= 60:
                    return False
                # ìˆ«ìë§Œ ìˆëŠ” ê²½ìš° (ë„ˆë¬´ ì§§ì€ ìˆ«ì)
                if not suffix and num < 100:
                    return False
        
        return True
    
    # í•„í„°ë§ ì ìš©
    filtered_words = [
        word.strip() 
        for word in words 
        if should_keep_word(word.strip())
    ]
    
    return filtered_words

def _calculate_news_scores(news: Dict[str, Any]) -> Dict[str, float]:
    """
    ë‰´ìŠ¤ì˜ 5ê°€ì§€ ì§€í‘œë¥¼ ê³„ì‚°í•˜ì—¬ ë¼ì´ë‹¤ ì°¨íŠ¸ì— ì‚¬ìš©í•  ì ìˆ˜ë¥¼ ë°˜í™˜
    
    ê³„ì‚° ì§€í‘œ:
    1. ì‹œì¥ ì˜í–¥ë„: ê¸ˆìœµ ì‹œì¥ ê´€ë ¨ í‚¤ì›Œë“œ ê¸°ë°˜ (0-100)
    2. ì •ë³´ ë°€ë„: ë³¸ë¬¸ ê¸¸ì´ ë° ìˆ«ì í¬í•¨ ì—¬ë¶€ (0-100)
    3. ì´ˆë³´ì ë‚œì´ë„: RAG ìš©ì–´ ì‚¬ì „ì— ìˆëŠ” ì „ë¬¸ ìš©ì–´ê°€ ë§ì„ìˆ˜ë¡ ê°ì  (0-100, ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)
    4. í•™ìŠµ ê°€ì¹˜: êµìœ¡ì  í‚¤ì›Œë“œ ê¸°ë°˜ (0-100)
    5. ì‹¤í–‰ ê°€ì¹˜: ì‹¤ìš©ì  ì¡°ì–¸ í‚¤ì›Œë“œ ê¸°ë°˜ (0-100)
    
    Args:
        news: ë‰´ìŠ¤ ë”•ì…”ë„ˆë¦¬ (title, content ë“± í¬í•¨)
    
    Returns:
        5ê°€ì§€ ì§€í‘œ ì ìˆ˜ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬
    """
    title = str(news.get("title", "")).strip()
    content = str(news.get("content", "")).strip()
    
    # ê¸°ë³¸ ì ìˆ˜: ëª¨ë“  ì§€í‘œëŠ” 50ì ì—ì„œ ì‹œì‘ (ì´ˆë³´ì ë‚œì´ë„ëŠ” 100ì ì—ì„œ ì‹œì‘)
    scores = {
        "market_impact": 50.0,      # ì‹œì¥ ì˜í–¥ë„: ê¸°ë³¸ 50ì 
        "info_density": 50.0,       # ì •ë³´ ë°€ë„: ê¸°ë³¸ 50ì  (ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¼ ì¬ì„¤ì •ë¨)
        "beginner_friendly": 100.0, # ì´ˆë³´ì ë‚œì´ë„: ê¸°ë³¸ 100ì  (ì „ë¬¸ ìš©ì–´ì— ë”°ë¼ ê°ì )
        "learning_value": 50.0,     # í•™ìŠµ ê°€ì¹˜: ê¸°ë³¸ 50ì 
        "action_value": 50.0        # ì‹¤í–‰ ê°€ì¹˜: ê¸°ë³¸ 50ì 
    }
    
    # í…ìŠ¤íŠ¸ í•©ì¹˜ê¸° (ì œëª© + ë³¸ë¬¸, ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ í‚¤ì›Œë“œ ë§¤ì¹­)
    text = f"{title} {content}".lower()
    content_len = len(content)
    title_len = len(title)
    
    # ========== 1. ì‹œì¥ ì˜í–¥ë„ (Market Impact) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ ê¸ˆìœµ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì˜ í¬ê¸°ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ + ìˆ«ì í¬í•¨ ì—¬ë¶€
    
    # ê¸ˆìœµ ê´€ë ¨ í‚¤ì›Œë“œ ëª©ë¡ (16ê°œ)
    market_keywords = [
        "ê¸ˆë¦¬", "ê¸ˆìœµ", "ì¦ê¶Œ", "ì£¼ì‹", "ì‹œì¥", "ê²½ì œ", "ì •ì±…", "í•œêµ­ì€í–‰",
        "ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ì¸í”Œë ˆì´ì…˜", "ë””í”Œë ˆì´ì…˜", "í™˜ìœ¨", "ë¶€ë™ì‚°", "íˆ¬ì", "ìì‚°"
    ]
    market_count = sum(1 for keyword in market_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì  (ìµœëŒ€ 50ì  ì¶”ê°€ ê°€ëŠ¥)
    scores["market_impact"] = min(100, 50 + market_count * 5)
    
    # ìˆ«ì í¬í•¨ ì—¬ë¶€ (ê¸ˆë¦¬, í¼ì„¼íŠ¸, ê¸ˆì•¡ ë“±)
    # íŒ¨í„´: ìˆ«ì + ë‹¨ìœ„(%, ì›, ì–µ, ë§Œ, ì¡°)
    numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', text))
    # ìˆ«ì 1ê°œë‹¹ +2ì  (ìµœëŒ€ 20ì  ì¶”ê°€ ê°€ëŠ¥)
    scores["market_impact"] = min(100, scores["market_impact"] + min(numbers * 2, 20))
    
    # ========== 2. ì •ë³´ ë°€ë„ (Info Density) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ì— í¬í•¨ëœ ì •ë³´ì˜ ì–‘ê³¼ ì§ˆì„ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: ë³¸ë¬¸ ê¸¸ì´ + ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€
    
    if content_len > 0:
        # ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¥¸ ê¸°ë³¸ ì ìˆ˜ ì„¤ì •
        if content_len >= 2000:
            scores["info_density"] = 80  # ë§¤ìš° ê¸´ ê¸°ì‚¬: ì •ë³´ê°€ í’ë¶€í•¨
        elif content_len >= 1000:
            scores["info_density"] = 70  # ê¸´ ê¸°ì‚¬: ì •ë³´ê°€ ë§ìŒ
        elif content_len >= 500:
            scores["info_density"] = 60  # ì¤‘ê°„ ê¸¸ì´: ì ë‹¹í•œ ì •ë³´
        else:
            scores["info_density"] = 40  # ì§§ì€ ê¸°ì‚¬: ì •ë³´ê°€ ë¶€ì¡±í•¨
        
        # ìˆ«ì/í†µê³„ í¬í•¨ ì—¬ë¶€ (ë³¸ë¬¸ ë‚´ì—ì„œë§Œ ê²€ìƒ‰)
        numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°]?', content))
        # ìˆ«ì 1ê°œë‹¹ +1ì  (ìµœëŒ€ 20ì  ì¶”ê°€ ê°€ëŠ¥)
        scores["info_density"] = min(100, scores["info_density"] + min(numbers, 20))
    else:
        # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì •ë³´ ë°€ë„ëŠ” 0ì 
        scores["info_density"] = 0
    
    # ========== 3. ì´ˆë³´ì ë‚œì´ë„ (Beginner Friendly) ê³„ì‚° ==========
    # ëª©ì : ì´ˆë³´ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì •ë„ë¥¼ ì¸¡ì • (ë†’ì„ìˆ˜ë¡ ì‰¬ì›€)
    # ê³„ì‚° ë°©ë²•: RAG ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ì˜ ìš©ì–´ ê°ì  + ë³¸ë¬¸ ê¸¸ì´ ê°ì 
    
    # RAG ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ì—ì„œ ìš©ì–´ ê°€ì ¸ì˜¤ê¸°
    try:
        from rag.glossary import ensure_financial_terms, DEFAULT_TERMS
        ensure_financial_terms()  # RAG ìš©ì–´ ì‚¬ì „ ì´ˆê¸°í™”
        financial_terms = st.session_state.get("financial_terms", DEFAULT_TERMS)
        expert_terms = list(financial_terms.keys()) if financial_terms else []
    except Exception:
        # RAG ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì „ë¬¸ ìš©ì–´ ëª©ë¡ ì‚¬ìš© (fallback)
        expert_terms = [
            "íŒŒìƒìƒí’ˆ", "ì˜µì…˜", "ì„ ë¬¼", "ìŠ¤ì™‘", "í—¤ì§€", "ë ˆë²„ë¦¬ì§€", "ë§ˆì§„ì½œ", "ì¦ê±°ê¸ˆ",
            "M&A", "IPO", "ê³µëª¨ì£¼", "ë°°ë‹¹ë½ì¼", "ì•¡ë©´ë¶„í• ", "ìœ ìƒì¦ì"
        ]
    
    # RAG ìš©ì–´ ì‚¬ì „ì— ìˆëŠ” ìš©ì–´ê°€ ë‰´ìŠ¤ì— í¬í•¨ëœ ê°œìˆ˜ ê³„ì‚°
    # ê¸´ ìš©ì–´ë¶€í„° ë§¤ì¹­í•˜ì—¬ ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€ (ì˜ˆ: "ê¸°ì¤€ê¸ˆë¦¬"ê°€ "ê¸ˆë¦¬"ë³´ë‹¤ ë¨¼ì € ë§¤ì¹­)
    expert_terms_sorted = sorted(expert_terms, key=len, reverse=True)
    matched_terms = []
    text_lower = text.lower()
    
    for term in expert_terms_sorted:
        if term.lower() in text_lower:
            matched_terms.append(term)
            # ì´ë¯¸ ë§¤ì¹­ëœ ë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€ (ê°„ë‹¨í•œ ë°©ë²•)
            # ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë§¤ì¹­ì´ í•„ìš”í•  ìˆ˜ ìˆì§€ë§Œ, ì„±ëŠ¥ì„ ìœ„í•´ ë‹¨ìˆœí™”
    
    expert_count = len(matched_terms)
    
    # RAG ìš©ì–´ê°€ ë§ì„ìˆ˜ë¡ ê°ì 
    # RAG ìš©ì–´ ì‚¬ì „ í¬ê¸°ì— ë”°ë¼ ê°ì  ë¹„ìœ¨ ì¡°ì •
    # - ìš©ì–´ ì‚¬ì „ì´ ì‘ìœ¼ë©´(50ê°œ ì´í•˜): ìš©ì–´ 1ê°œë‹¹ -2ì 
    # - ìš©ì–´ ì‚¬ì „ì´ ì¤‘ê°„(51-200ê°œ): ìš©ì–´ 1ê°œë‹¹ -1ì 
    # - ìš©ì–´ ì‚¬ì „ì´ í¬ë©´(200ê°œ ì´ìƒ): ìš©ì–´ 1ê°œë‹¹ -0.5ì 
    if len(expert_terms) <= 50:
        penalty_per_term = 2.0
    elif len(expert_terms) <= 200:
        penalty_per_term = 1.0
    else:
        penalty_per_term = 0.5
    
    total_penalty = min(50, expert_count * penalty_per_term)  # ìµœëŒ€ 50ì  ê°ì 
    scores["beginner_friendly"] = max(0, 100 - total_penalty)
    
    # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì´í•´í•˜ê¸° ì–´ë ¤ì›€
    if content_len < 300:
        scores["beginner_friendly"] = max(0, scores["beginner_friendly"] - 20)
    
    # ========== 4. í•™ìŠµ ê°€ì¹˜ (Learning Value) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ êµìœ¡ì  ê°€ì¹˜ë¥¼ ì œê³µí•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: êµìœ¡ì  í‚¤ì›Œë“œ + ë³¸ë¬¸ ê¸¸ì´ ë³´ë„ˆìŠ¤
    
    # êµìœ¡ì  í‚¤ì›Œë“œ ëª©ë¡ (14ê°œ)
    learning_keywords = [
        "ì„¤ëª…", "ì´ìœ ", "ë°°ê²½", "ê³¼ì •", "ë°©ë²•", "ì›ë¦¬", "ê°œë…", "ì˜ë¯¸",
        "ì˜í–¥", "íš¨ê³¼", "ê²°ê³¼", "ë¶„ì„", "ì „ë§", "ì˜ˆìƒ"
    ]
    learning_count = sum(1 for keyword in learning_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
    scores["learning_value"] = min(100, 50 + learning_count * 5)
    
    # ë³¸ë¬¸ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ë” ë§ì€ ë°°ê²½ ì •ë³´ì™€ ì„¤ëª…ì„ í¬í•¨í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    if content_len >= 1500:
        scores["learning_value"] = min(100, scores["learning_value"] + 20)  # ë§¤ìš° ê¸´ ê¸°ì‚¬: +20ì 
    elif content_len >= 800:
        scores["learning_value"] = min(100, scores["learning_value"] + 10)   # ê¸´ ê¸°ì‚¬: +10ì 
    
    # ========== 5. ì‹¤í–‰ ê°€ì¹˜ (Action Value) ê³„ì‚° ==========
    # ëª©ì : ë‰´ìŠ¤ê°€ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì´ë‚˜ í–‰ë™ ì§€ì¹¨ì„ ì œê³µí•˜ëŠ” ì •ë„ë¥¼ ì¸¡ì •
    # ê³„ì‚° ë°©ë²•: í–‰ë™ ì§€ì¹¨ í‚¤ì›Œë“œ + êµ¬ì²´ì  ìˆ«ì/ê¸°ê°„ ë³´ë„ˆìŠ¤
    
    # í–‰ë™ ì§€ì¹¨ í‚¤ì›Œë“œ ëª©ë¡ (14ê°œ)
    action_keywords = [
        "ê¶Œì¥", "ì œì•ˆ", "ì¡°ì–¸", "ë°©ì•ˆ", "ëŒ€ì±…", "ì „ëµ", "ê³„íš", "ë°©ë²•",
        "í•´ì•¼", "í•„ìš”", "ì¤‘ìš”", "ì£¼ì˜", "ê²½ê³ ", "ì‹œì‚¬ì "
    ]
    action_count = sum(1 for keyword in action_keywords if keyword in text)
    # í‚¤ì›Œë“œ 1ê°œë‹¹ +5ì 
    scores["action_value"] = min(100, 50 + action_count * 5)
    
    # êµ¬ì²´ì ì¸ ìˆ«ìë‚˜ ê¸°ê°„ì´ ìˆìœ¼ë©´ ì‹¤í–‰ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
    # íŒ¨í„´: ìˆ«ì + ë‹¨ìœ„(%, ì›, ì–µ, ë§Œ, ì¡°, ì¼, ì›”, ë…„)
    specific_numbers = len(re.findall(r'\d+[%ì›ì–µë§Œì¡°ì¼ì›”ë…„]', text))
    if specific_numbers >= 3:
        scores["action_value"] = min(100, scores["action_value"] + 15)  # êµ¬ì²´ì  ì •ë³´ ë³´ë„ˆìŠ¤
    
    # ========== ìµœì¢… ì ìˆ˜ ì •ê·œí™” ==========
    # ëª¨ë“  ì ìˆ˜ë¥¼ 0-100 ë²”ìœ„ë¡œ ì œí•œ (ì•ˆì „ì¥ì¹˜)
    for key in scores:
        scores[key] = max(0, min(100, scores[key]))
    
    return scores

def _render_search_result_news_popularity(df_view: pd.DataFrame):
    """ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„"""
    search_events = df_view[df_view["event_name"] == "news_search_from_chat"].copy()
    selected_events = df_view[df_view["event_name"] == "news_selected_from_chat"].copy()
    
    if search_events.empty:
        return
    
    st.markdown("#### ğŸ” ê²€ìƒ‰ ê²°ê³¼ ë‰´ìŠ¤ ì¸ê¸° ë¶„ì„")
    st.markdown("**ëª©ì **: ì±—ë´‡ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì–´ë–¤ ë‰´ìŠ¤ê°€ ê°€ì¥ ì¸ê¸° ìˆëŠ”ì§€ ë¶„ì„")
    
    # ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ ë‰´ìŠ¤ ID ìˆ˜ì§‘
    news_appearances = {}  # {news_id: {count: int, keywords: set}}
    
    for idx, row in search_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        # article_ids ì™¸ì— ë‹¤ë¥¸ í•„ë“œëª…ë„ í™•ì¸ (supabase_results ë“±)
        article_ids = []
        if payload:
            # ì—¬ëŸ¬ ê°€ëŠ¥í•œ í•„ë“œëª… í™•ì¸
            if "article_ids" in payload:
                article_ids = payload.get("article_ids", [])
            elif "supabase_results" in payload:
                # supabase_resultsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°
                results = payload.get("supabase_results", [])
                if isinstance(results, list):
                    article_ids = [item.get("id") or item.get("news_id") for item in results if item]
            elif "results" in payload:
                results = payload.get("results", [])
                if isinstance(results, list):
                    article_ids = [item.get("id") or item.get("news_id") for item in results if item]
            
            keyword = payload.get("keyword", "")
            
            for news_id in article_ids:
                if news_id:
                    news_id_str = str(news_id)
                    if news_id_str not in news_appearances:
                        news_appearances[news_id_str] = {"count": 0, "keywords": set()}
                    news_appearances[news_id_str]["count"] += 1
                    if keyword:
                        news_appearances[news_id_str]["keywords"].add(keyword)
    
    # ì‹¤ì œ í´ë¦­ëœ ë‰´ìŠ¤ ID ìˆ˜ì§‘
    news_clicks = {}  # {news_id: count}
    news_titles = {}  # {news_id: title}
    
    for idx, row in selected_events.iterrows():
        news_id = row.get("news_id")
        if news_id:
            news_id_str = str(news_id)
            news_clicks[news_id_str] = news_clicks.get(news_id_str, 0) + 1
            
            # ì œëª© ì •ë³´ ìˆ˜ì§‘ (payloadì—ì„œ ë¨¼ì € ì‹œë„)
            payload = _parse_payload(row.get("payload"))
            if payload and "title" in payload and news_id_str not in news_titles:
                news_titles[news_id_str] = payload.get("title", "")
    
    # Supabase news í…Œì´ë¸”ì—ì„œ ì œëª© ê°€ì ¸ì˜¤ê¸° (payloadì— ì—†ëŠ” ê²½ìš°)
    all_news_ids = set(news_appearances.keys()) | set(news_clicks.keys())
    missing_title_ids = [nid for nid in all_news_ids if nid not in news_titles]
    
    if missing_title_ids:
        try:
            # Supabaseì—ì„œ ë‰´ìŠ¤ ì œëª© ê°€ì ¸ì˜¤ê¸°
            news_df = _fetch_news_from_supabase(limit=10000)  # ì¶©ë¶„íˆ ë§ì´ ê°€ì ¸ì˜¤ê¸°
            if not news_df.empty and "news_id" in news_df.columns and "title" in news_df.columns:
                # news_idë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­ (ì–‘ë°©í–¥ ë³€í™˜ ì‹œë„)
                news_df["news_id_str"] = news_df["news_id"].astype(str)
                # ì •ìˆ˜ë¡œë„ ë³€í™˜ ì‹œë„ (news_idê°€ ì •ìˆ˜ì¸ ê²½ìš°)
                try:
                    news_df["news_id_int"] = news_df["news_id"].astype(int)
                except:
                    news_df["news_id_int"] = None
                
                for news_id_str in missing_title_ids:
                    matched_news = None
                    
                    # ë¬¸ìì—´ë¡œ ë¨¼ì € ë§¤ì¹­ ì‹œë„
                    matched_news = news_df[news_df["news_id_str"] == news_id_str]
                    
                    # ë¬¸ìì—´ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­ ì‹œë„
                    if matched_news.empty:
                        try:
                            news_id_int = int(news_id_str)
                            if "news_id_int" in news_df.columns:
                                matched_news = news_df[news_df["news_id_int"] == news_id_int]
                        except (ValueError, TypeError):
                            pass
                    
                    if not matched_news.empty:
                        title = matched_news.iloc[0].get("title", "")
                        if title and pd.notna(title) and str(title).strip():
                            news_titles[news_id_str] = str(title).strip()
        except Exception as e:
            # Supabase ì¡°íšŒ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë¡œê¹… (ë””ë²„ê¹…ìš©)
            import traceback
            print(f"âš ï¸ Supabaseì—ì„œ ë‰´ìŠ¤ ì œëª© ì¡°íšŒ ì‹¤íŒ¨: {e}")
            print(f"ì—ëŸ¬ ìƒì„¸: {traceback.format_exc()}")
            # ì—ëŸ¬ê°€ ë°œìƒí•´ë„ ê³„ì† ì§„í–‰ (payloadì—ì„œ ê°€ì ¸ì˜¨ ì œëª©ë§Œ ì‚¬ìš©)
    
    # ì¸ê¸° ë‰´ìŠ¤ ë¶„ì„ ë°ì´í„° ìƒì„±
    # í´ë¦­ëœ ë‰´ìŠ¤ë„ í¬í•¨ (ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ë”ë¼ë„)
    all_news_ids_for_analysis = set(news_appearances.keys()) | set(news_clicks.keys())
    
    if all_news_ids_for_analysis:
        popularity_data = []
        for news_id in all_news_ids_for_analysis:
            appearance_count = news_appearances.get(news_id, {}).get("count", 0)
            click_count = news_clicks.get(news_id, 0)
            # appearance_countê°€ 0ì´ë©´ í´ë¦­ë¥  ê³„ì‚° ë¶ˆê°€ (N/A ë˜ëŠ” 0ìœ¼ë¡œ í‘œì‹œ)
            # ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ì§€ë§Œ í´ë¦­ëœ ê²½ìš°ì¼ ìˆ˜ ìˆìŒ
            if appearance_count > 0:
                click_rate = (click_count / appearance_count * 100)
            else:
                # ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•˜ì§€ë§Œ í´ë¦­ëœ ê²½ìš°
                click_rate = 0  # ë˜ëŠ” N/Aë¡œ í‘œì‹œí•  ìˆ˜ë„ ìˆìŒ
            title = news_titles.get(news_id, f"ë‰´ìŠ¤ ID: {news_id}")
            
            popularity_data.append({
                "news_id": news_id,
                "ì œëª©": title[:50] + "..." if len(title) > 50 else title,
                "ê²€ìƒ‰ ê²°ê³¼ í¬í•¨": appearance_count if appearance_count > 0 else 0,
                "í´ë¦­ ìˆ˜": click_count,
                "í´ë¦­ë¥  (%)": round(click_rate, 1) if appearance_count > 0 else 0.0
            })
        
        if popularity_data:
            popularity_df = pd.DataFrame(popularity_data)
            popularity_df = popularity_df.sort_values("í´ë¦­ ìˆ˜", ascending=False)
            
            # ì£¼ìš” ë©”íŠ¸ë¦­
            col1, col2, col3 = st.columns(3)
            with col1:
                total_searches = len(search_events)
                st.metric("ì´ ê²€ìƒ‰ ì‹¤í–‰", f"{total_searches:,}ê±´")
            with col2:
                unique_news = len(news_appearances)
                st.metric("ê²€ìƒ‰ ê²°ê³¼ì— í¬í•¨ëœ ë‰´ìŠ¤", f"{unique_news:,}ê°œ")
            with col3:
                total_clicks = sum(news_clicks.values())
                st.metric("ì´ í´ë¦­ ìˆ˜", f"{total_clicks:,}ê±´")
            
            # Top 10 ì¸ê¸° ë‰´ìŠ¤ (í´ë¦­ ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬)
            top_10_df = popularity_df.head(10).copy()
            # ì°¨íŠ¸ì—ì„œë„ í´ë¦­ ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ í‘œì‹œë˜ë„ë¡ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
            top_10_df = top_10_df.sort_values("í´ë¦­ ìˆ˜", ascending=True)  # ì°¨íŠ¸ëŠ” ì•„ë˜ì—ì„œ ìœ„ë¡œ ì˜¬ë¼ê°€ë¯€ë¡œ ì˜¤ë¦„ì°¨ìˆœ
            
            if px is not None and len(top_10_df) > 0:
                fig = px.bar(
                    top_10_df,
                    x="í´ë¦­ ìˆ˜",
                    y="ì œëª©",
                    orientation='h',
                    title="ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì¥ ë§ì´ í´ë¦­ëœ ë‰´ìŠ¤ Top 10",
                    labels={"í´ë¦­ ìˆ˜": "í´ë¦­ ìˆ˜", "ì œëª©": "ë‰´ìŠ¤ ì œëª©"},
                    hover_data=["ê²€ìƒ‰ ê²°ê³¼ í¬í•¨", "í´ë¦­ë¥  (%)"],
                    text="í´ë¦­ ìˆ˜"  # ë§‰ëŒ€ ì˜†ì— ìˆ«ì í‘œì‹œ
                )
                fig.update_traces(texttemplate='%{text}ê±´', textposition='outside')
                fig.update_layout(
                    height=500,
                    showlegend=False
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # ì „ì²´ ë°ì´í„°ëŠ” expanderë¡œ ìˆ¨ê¹€ ì²˜ë¦¬ (í•„ìš”ì‹œ í¼ì³ì„œ í™•ì¸ ê°€ëŠ¥)
            with st.expander("ğŸ“‹ ì „ì²´ ë‰´ìŠ¤ í´ë¦­ ë°ì´í„° ë³´ê¸°", expanded=False):
                st.dataframe(popularity_df, use_container_width=True, height=400)
        else:
            st.info("ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def _render_url_parsing_quality_for_content(df_view: pd.DataFrame):
    """ì½˜í…ì¸  í’ˆì§ˆ íƒ­ìš© URL íŒŒì‹± í’ˆì§ˆ"""
    url_events = df_view[df_view["event_name"].isin(["news_url_added_from_chat", "news_url_add_error"])]
    
    if url_events.empty:
        st.info("ğŸ“Š URL íŒŒì‹± ì´ë²¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.markdown("#### ğŸ“° URL íŒŒì‹± í’ˆì§ˆ (í¬ë¡¤ë§/íŒŒì‹± ì‹¤íŒ¨ìœ¨)")
    
    # ì´ë²¤íŠ¸ ê±´ìˆ˜ (ì¤‘ë³µ í¬í•¨)
    success_events = url_events[url_events["event_name"] == "news_url_added_from_chat"]
    error_events = url_events[url_events["event_name"] == "news_url_add_error"]
    success_count = len(success_events)
    error_count = len(error_events)
    total_count = success_count + error_count
    
    # ê³ ìœ  URL ê¸°ì¤€ìœ¼ë¡œ ì¹´ìš´íŠ¸ (ì¤‘ë³µ ì œê±°) - payloadì—ì„œ URL ì¶”ì¶œ ì‹œë„
    unique_urls_success = set()
    unique_urls_error = set()
    
    for idx, row in success_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        if payload:
            url = payload.get("url") or payload.get("link") or payload.get("news_url")
            if url:
                unique_urls_success.add(str(url).strip())
    
    for idx, row in error_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        if payload:
            url = payload.get("url") or payload.get("link") or payload.get("news_url") or payload.get("error_url")
            if url:
                unique_urls_error.add(str(url).strip())
    
    unique_success_count = len(unique_urls_success) if unique_urls_success else success_count
    unique_error_count = len(unique_urls_error) if unique_urls_error else error_count
    unique_total_count = unique_success_count + unique_error_count
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("íŒŒì‹± ì„±ê³µ", f"{unique_success_count:,}ê±´")
        if success_count > unique_success_count:
            st.caption(f"ì´ë²¤íŠ¸ ê±´ìˆ˜: {success_count:,}ê±´")
    with col2:
        st.metric("íŒŒì‹± ì‹¤íŒ¨", f"{unique_error_count:,}ê±´")
        if error_count > unique_error_count:
            st.caption(f"ì´ë²¤íŠ¸ ê±´ìˆ˜: {error_count:,}ê±´")
    with col3:
        if unique_total_count > 0:
            failure_rate = (unique_error_count / unique_total_count) * 100
            st.metric("ì‹¤íŒ¨ìœ¨", f"{failure_rate:.1f}%")
        else:
            st.metric("ì‹¤íŒ¨ìœ¨", "N/A")
    
    # ì°¸ê³  ì •ë³´ í‘œì‹œ
    if total_count > unique_total_count:
        st.info(f"ğŸ’¡ **ì°¸ê³ **: ê³ ìœ  URL {unique_total_count:,}ê°œì— ëŒ€í•´ ì´ {total_count:,}ê°œì˜ íŒŒì‹± ì´ë²¤íŠ¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (ê°™ì€ URLì„ ì—¬ëŸ¬ ë²ˆ íŒŒì‹±í•œ ê²½ìš° í¬í•¨)")
    
    # ì‹œê°„ëŒ€ë³„ ì‹¤íŒ¨ìœ¨ ì¶”ì´
    if total_count > 0:
        url_events_copy = url_events.copy()
        url_events_copy["hour"] = url_events_copy["event_time"].dt.floor("H")
        
        # ì„±ê³µ/ì‹¤íŒ¨ë³„ë¡œ ê·¸ë£¹í™”
        success_by_hour = url_events_copy[url_events_copy["event_name"] == "news_url_added_from_chat"].groupby("hour").size().reset_index(name="ì„±ê³µ")
        error_by_hour = url_events_copy[url_events_copy["event_name"] == "news_url_add_error"].groupby("hour").size().reset_index(name="ì‹¤íŒ¨")
        
        # ë³‘í•©
        hourly_stats = success_by_hour.merge(error_by_hour, on="hour", how="outer").fillna(0)
        
        if len(hourly_stats) > 0 and px is not None:
            hourly_stats["ì‹¤íŒ¨ìœ¨"] = (hourly_stats["ì‹¤íŒ¨"] / (hourly_stats["ì„±ê³µ"] + hourly_stats["ì‹¤íŒ¨"]) * 100).fillna(0)
            fig = px.line(
                hourly_stats,
                x="hour",
                y="ì‹¤íŒ¨ìœ¨",
                title="ì‹œê°„ëŒ€ë³„ URL íŒŒì‹± ì‹¤íŒ¨ìœ¨ ì¶”ì´"
            )
            st.plotly_chart(fig, use_container_width=True)

# ============================================================================
# íƒ­ 3: ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (User Behavior)
# ============================================================================

def _render_user_behavior_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° íƒ­: í•µì‹¬ ì§€í‘œë§Œ í‘œì‹œ
    - ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)
    - ê¸°ì‚¬ ì½ê¸° ì‹œê°„ (Dwell Time, í‰ê· )
    - Glossary ì‚¬ìš©ë¥ 
    - ì§ˆë¬¸(RAG) ì‚¬ìš©ë¥ 
    - ìš©ì–´ í´ë¦­ë¥ 
    - ë‰´ìŠ¤ ê²€ìƒ‰ ì‚¬ìš©ë¥  / ê²€ìƒ‰ ì„±ê³µë¥  (ê°„ë‹¨ ë²„ì „)
    - ì¬ë°©ë¬¸ ì„¸ì…˜ ë¹„ìœ¨ (ê°„ë‹¨)
    """
    st.markdown("### ğŸŸ¢ ì‚¬ìš©ì í–‰ë™ ë°ì´í„° (User Behavior)")
    st.markdown("**ëª©í‘œ**: ì‚¬ìš©ìì˜ í•µì‹¬ í–‰ë™ íŒ¨í„´ì„ í•œëˆˆì— íŒŒì•…")
    
    # 1. ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)
    _render_news_ctr(df_view)
    
    # 2. ê¸°ì‚¬ ì½ê¸° ì‹œê°„ (Dwell Time, í‰ê· )
    _render_dwell_time(df_view)
    
    # 3. Glossary ì‚¬ìš©ë¥ 
    _render_glossary_usage_rate(df_view, session_column)
    
    # 4. ì§ˆë¬¸(RAG) ì‚¬ìš©ë¥ 
    _render_rag_question_usage_rate(df_view, session_column)
    
    # 5. ìš©ì–´ í´ë¦­ë¥ 
    _render_term_clicks(df_view)
    
    # 6. ë‰´ìŠ¤ ê²€ìƒ‰ ì‚¬ìš©ë¥  / ê²€ìƒ‰ ì„±ê³µë¥  (ê°„ë‹¨ ë²„ì „)
    _render_search_usage_simple(df_view, session_column)
    
    # 7. ì¬ë°©ë¬¸ ì„¸ì…˜ ë¹„ìœ¨ (ê°„ë‹¨)
    _render_returning_sessions_simple(df_view, session_column)

def _render_news_ctr(df_view: pd.DataFrame):
    """ë‰´ìŠ¤ í´ë¦­ë¥  ë¶„ì„"""
    st.markdown("#### ğŸ“Š ë‰´ìŠ¤ í´ë¦­ë¥  (CTR)")
    
    clicks = df_view[df_view["event_name"] == "news_click"]
    views = df_view[df_view["event_name"].isin(["news_click", "news_detail_open"])]
    
    if len(views) > 0:
        ctr = (len(clicks) / len(views)) * 100
        st.metric("í´ë¦­ë¥ ", f"{ctr:.1f}%")
        
        # ì‹œê°„ëŒ€ë³„ CTR
        if len(clicks) > 0:
            clicks["hour"] = clicks["event_time"].dt.floor("H")
            hourly_clicks = clicks.groupby("hour").size().reset_index(name="í´ë¦­ ìˆ˜")
            
            if px is not None and len(hourly_clicks) > 0:
                fig = px.bar(
                    hourly_clicks,
                    x="hour",
                    y="í´ë¦­ ìˆ˜",
                    title="ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ í´ë¦­ ìˆ˜"
                )
                st.plotly_chart(fig, use_container_width=True)

def _render_dwell_time(df_view: pd.DataFrame):
    """ê¸°ì‚¬ ì½ê¸° ì‹œê°„ ë¶„ì„ (í‰ê· ë§Œ í‘œì‹œ)"""
    st.markdown("#### â±ï¸ ê¸°ì‚¬ ì½ê¸° ì‹œê°„ (Dwell Time)")
    
    duration_data = []
    
    # ë°©ë²• 1: view_duration ì´ë²¤íŠ¸ì—ì„œ duration_sec ì¶”ì¶œ
    view_duration_events = df_view[df_view["event_name"] == "view_duration"]
    for idx, row in view_duration_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        duration_sec = payload.get("duration_sec")
        if duration_sec is not None:
            try:
                duration_data.append(float(duration_sec))
            except (ValueError, TypeError):
                pass
    
    # ë°©ë²• 2: news_detail_back ì´ë²¤íŠ¸ì˜ payloadì—ì„œ duration_sec ì¶”ì¶œ (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•)
    detail_back_events = df_view[df_view["event_name"] == "news_detail_back"]
    for idx, row in detail_back_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        if payload:
            duration_sec = payload.get("duration_sec")
            if duration_sec is not None:
                try:
                    duration_data.append(float(duration_sec))
                except (ValueError, TypeError):
                    pass
    
    # ë°©ë²• 3: news_detail_open ì´ë²¤íŠ¸ì˜ payloadì—ì„œ duration_sec ì¶”ì¶œ (í˜¹ì‹œ ìˆì„ ê²½ìš°)
    detail_open_events = df_view[df_view["event_name"] == "news_detail_open"]
    for idx, row in detail_open_events.iterrows():
        payload = _parse_payload(row.get("payload"))
        if payload:
            duration_sec = payload.get("duration_sec")
            if duration_sec is not None:
                try:
                    duration_data.append(float(duration_sec))
                except (ValueError, TypeError):
                    pass
    
    # ë°©ë²• 4: ì„¸ì…˜ ë‚´ì—ì„œ news_detail_openê³¼ news_detail_back ê°„ì˜ ì‹œê°„ ì°¨ì´ ê³„ì‚°
    if "session_id" in df_view.columns and "event_time" in df_view.columns:
        for session_id in df_view["session_id"].dropna().unique():
            session_events = df_view[df_view["session_id"] == session_id].sort_values("event_time")
            detail_open_events = session_events[session_events["event_name"] == "news_detail_open"]
            detail_back_events = session_events[session_events["event_name"] == "news_detail_back"]
            
            for open_idx in detail_open_events.index:
                open_time = session_events.loc[open_idx, "event_time"]
                open_news_id = session_events.loc[open_idx, "news_id"] if "news_id" in session_events.columns else None
                
                # ê°™ì€ news_idë¥¼ ê°€ì§„ news_detail_back ì´ë²¤íŠ¸ ì°¾ê¸°
                after_open = session_events.loc[session_events.index > open_idx]
                matching_back = after_open[
                    (after_open["event_name"] == "news_detail_back") &
                    (after_open["news_id"] == open_news_id if open_news_id else True)
                ]
                
                if not matching_back.empty:
                    back_time = matching_back.iloc[0]["event_time"]
                    duration = (back_time - open_time).total_seconds()
                    if duration > 0 and duration < 3600:  # 1ì‹œê°„ ì´ë‚´ë§Œ ìœ íš¨
                        duration_data.append(duration)
    
    if duration_data:
        avg_duration = sum(duration_data) / len(duration_data)
        st.metric("í‰ê·  ì½ê¸° ì‹œê°„", f"{avg_duration:.1f}ì´ˆ")
        st.caption(f"ğŸ“Š ë¶„ì„ëœ ì½ê¸° ì‹œê°„ ë°ì´í„°: {len(duration_data)}ê±´")
    else:
        st.info("ğŸ“Š ì½ê¸° ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.caption("ğŸ’¡ ì½ê¸° ì‹œê°„ì€ ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤:")
        st.caption("   1. view_duration ì´ë²¤íŠ¸ì˜ duration_sec")
        st.caption("   2. news_detail_back ì´ë²¤íŠ¸ì˜ payload duration_sec (ì£¼ìš” ë°©ë²•)")
        st.caption("   3. news_detail_openê³¼ news_detail_back ê°„ì˜ ì‹œê°„ ì°¨ì´")

def _render_summary_clicks(df_view: pd.DataFrame):
    """ìš”ì•½ í´ë¦­ë¥  ë¶„ì„"""
    # ìš”ì•½ ê´€ë ¨ ì´ë²¤íŠ¸ê°€ ìˆìœ¼ë©´ ë¶„ì„
    summary_events = df_view[df_view["event_name"].str.contains("summary", case=False, na=False)]
    
    if summary_events.empty:
        return
    
    st.markdown("#### ğŸ“ ìš”ì•½ í´ë¦­ë¥ ")
    summary_clicks = len(summary_events)
    st.metric("ìš”ì•½ í´ë¦­", summary_clicks)

def _render_term_clicks(df_view: pd.DataFrame):
    """ìš©ì–´ í´ë¦­ë¥ """
    # Glossary í´ë¦­ë§Œ ì§‘ê³„ (ìš©ì–´ í´ë¦­ë¥ )
    glossary_clicks = df_view[df_view["event_name"] == "glossary_click"].copy()
    
    if glossary_clicks.empty:
        st.markdown("#### ğŸ’¡ ìš©ì–´ í´ë¦­ë¥ ")
        st.info("ğŸ“Š Glossary í´ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.markdown("#### ğŸ’¡ ìš©ì–´ í´ë¦­ë¥ ")
    
    # ìš©ì–´ í´ë¦­ë¥  ê³„ì‚°
    total_news_clicks = len(df_view[df_view["event_name"] == "news_click"])
    glossary_click_count = len(glossary_clicks)
    term_ctr = (glossary_click_count / total_news_clicks * 100) if total_news_clicks > 0 else 0
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Glossary í´ë¦­ ìˆ˜", glossary_click_count)
    with col2:
        st.metric("ìš©ì–´ í´ë¦­ë¥ ", f"{term_ctr:.1f}%")

def _render_glossary_usage_rate(df_view: pd.DataFrame, session_column: str):
    """Glossary ì‚¬ìš©ë¥ """
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### ğŸ“š Glossary ì‚¬ìš©ë¥ ")
    
    glossary_sessions = set(df_view[df_view["event_name"] == "glossary_click"][session_column].dropna().unique())
    total_sessions = df_view[session_column].nunique() if session_column in df_view.columns else 1
    glossary_usage_count = len(glossary_sessions)
    glossary_usage_rate = (glossary_usage_count / total_sessions * 100) if total_sessions > 0 else 0
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("Glossary ì‚¬ìš© ì„¸ì…˜", glossary_usage_count)
    with col2:
        st.metric("Glossary ì‚¬ìš©ë¥ ", f"{glossary_usage_rate:.1f}%")

def _render_rag_question_usage_rate(df_view: pd.DataFrame, session_column: str):
    """ì§ˆë¬¸(RAG) ì‚¬ìš©ë¥ """
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### â“ ì§ˆë¬¸(RAG) ì‚¬ìš©ë¥ ")
    
    rag_chat_question_sessions = _get_rag_chat_question_sessions(df_view, session_column)
    total_sessions = df_view[session_column].nunique() if session_column in df_view.columns else 1
    question_usage_count = len(rag_chat_question_sessions)
    question_usage_rate = (question_usage_count / total_sessions * 100) if total_sessions > 0 else 0
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì§ˆë¬¸(RAG) ì‚¬ìš© ì„¸ì…˜", question_usage_count)
    with col2:
        st.metric("ì§ˆë¬¸(RAG) ì‚¬ìš©ë¥ ", f"{question_usage_rate:.1f}%")

def _render_search_usage_simple(df_view: pd.DataFrame, session_column: str):
    """ë‰´ìŠ¤ ê²€ìƒ‰ ì‚¬ìš©ë¥  / ê²€ìƒ‰ ì„±ê³µë¥  (ê°„ë‹¨ ë²„ì „)"""
    if session_column not in df_view.columns:
        return
    
    st.markdown("#### ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‚¬ìš©ë¥  / ê²€ìƒ‰ ì„±ê³µë¥ ")
    
    # ê²€ìƒ‰ ì‚¬ìš© ì„¸ì…˜
    search_sessions = set(df_view[df_view["event_name"] == "news_search_from_chat"][session_column].dropna().unique())
    total_sessions = df_view[session_column].nunique() if session_column in df_view.columns else 1
    search_usage_count = len(search_sessions)
    search_usage_rate = (search_usage_count / total_sessions * 100) if total_sessions > 0 else 0
    
    # ê²€ìƒ‰ ì„±ê³µë¥ 
    search_success = df_view[df_view["event_name"] == "news_search_from_chat"]
    search_failed = df_view[df_view["event_name"] == "news_search_failed"]
    success_count = len(search_success)
    failed_count = len(search_failed)
    total_search_attempts = success_count + failed_count
    success_rate = (success_count / total_search_attempts * 100) if total_search_attempts > 0 else 0
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ê²€ìƒ‰ ì‚¬ìš© ì„¸ì…˜", search_usage_count)
        st.caption(f"ì‚¬ìš©ë¥ : {search_usage_rate:.1f}%")
    with col2:
        st.metric("ê²€ìƒ‰ ì„±ê³µ", success_count)
    with col3:
        st.metric("ê²€ìƒ‰ ì„±ê³µë¥ ", f"{success_rate:.1f}%")

def _render_returning_sessions_simple(df_view: pd.DataFrame, session_column: str):
    """ì¬ë°©ë¬¸ ì„¸ì…˜ ë¹„ìœ¨ (ê°„ë‹¨ ë²„ì „)"""
    if session_column not in df_view.columns or "user_id" not in df_view.columns:
        return
    
    st.markdown("#### ğŸ”„ ì¬ë°©ë¬¸ ì„¸ì…˜ ë¹„ìœ¨")
    
    user_sessions = df_view.groupby("user_id")[session_column].nunique().reset_index()
    user_sessions.columns = ["user_id", "ì„¸ì…˜ ìˆ˜"]
    
    total_users = len(user_sessions)
    returning_users = len(user_sessions[user_sessions["ì„¸ì…˜ ìˆ˜"] > 1])
    return_rate = (returning_users / total_users * 100) if total_users > 0 else 0
    
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ì´ ì‚¬ìš©ì", total_users)
    with col2:
        st.metric("ì¬ë°©ë¬¸ ì‚¬ìš©ì", returning_users)
        st.caption(f"ì¬ë°©ë¬¸ë¥ : {return_rate:.1f}%")


# ============================================================================
# Log Viewer íƒ­
# ============================================================================

def _render_log_viewer_tab(df_view: pd.DataFrame, session_column: str):
    """
    ğŸ“ Log Viewer íƒ­
    â†’ ìƒì„¸ ì´ë²¤íŠ¸ ë¡œê·¸ ë·°ì–´
    """
    st.markdown("### ğŸ“ Log Viewer")
    st.markdown("**ëª©ì **: ìƒì„¸ ì´ë²¤íŠ¸ ë¡œê·¸ í™•ì¸ ë° ë¶„ì„")
    
    # í•„í„° ì˜µì…˜
    col1, col2, col3 = st.columns(3)
    with col1:
        event_filter = st.multiselect(
            "ì´ë²¤íŠ¸ í•„í„°",
            options=sorted(df_view["event_name"].unique()) if "event_name" in df_view.columns else [],
            default=[],
            key="log_viewer_event_filter"
        )
    with col2:
        user_filter = st.text_input(
            "ì‚¬ìš©ì ID í•„í„°",
            value="",
            key="log_viewer_user_filter"
        )
    with col3:
        limit = st.number_input(
            "í‘œì‹œí•  ë¡œê·¸ ìˆ˜",
            min_value=10,
            max_value=1000,
            value=100,
            step=10,
            key="log_viewer_limit"
        )
    
    # í•„í„° ì ìš©
    filtered_df = df_view.copy()
    
    if event_filter:
        filtered_df = filtered_df[filtered_df["event_name"].isin(event_filter)]
    
    if user_filter:
        filtered_df = filtered_df[filtered_df["user_id"].astype(str).str.contains(user_filter, case=False, na=False)]
    
    # ìµœì‹ ìˆœ ì •ë ¬
    filtered_df = filtered_df.sort_values("event_time", ascending=False).head(limit)
    
    # í†µê³„
    st.markdown("#### ğŸ“Š ë¡œê·¸ í†µê³„")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ì´ ë¡œê·¸ ìˆ˜", f"{len(df_view):,}ê±´")
    with col2:
        st.metric("í•„í„°ëœ ë¡œê·¸", f"{len(filtered_df):,}ê±´")
    with col3:
        unique_events = filtered_df["event_name"].nunique() if "event_name" in filtered_df.columns else 0
        st.metric("ê³ ìœ  ì´ë²¤íŠ¸", f"{unique_events}ê°œ")
    with col4:
        unique_users = filtered_df["user_id"].nunique() if "user_id" in filtered_df.columns else 0
        st.metric("ê³ ìœ  ì‚¬ìš©ì", f"{unique_users}ëª…")
    
    st.markdown("---")
    
    # ë¡œê·¸ í…Œì´ë¸”
    st.markdown("#### ğŸ“‹ ì´ë²¤íŠ¸ ë¡œê·¸")
    
    # í‘œì‹œí•  ì»¬ëŸ¼ ì„ íƒ
    display_cols = ["event_time", "event_name", "user_id", session_column]
    if "news_id" in filtered_df.columns:
        display_cols.append("news_id")
    if "term" in filtered_df.columns:
        display_cols.append("term")
    if "message" in filtered_df.columns:
        display_cols.append("message")
    if "latency_ms" in filtered_df.columns:
        display_cols.append("latency_ms")
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
    available_cols = [col for col in display_cols if col in filtered_df.columns]
    
    if len(filtered_df) > 0:
        st.dataframe(
            filtered_df[available_cols],
            use_container_width=True,
            height=600
        )
        
        # Payload ìƒì„¸ ë³´ê¸° (ì„ íƒì )
        if st.checkbox("Payload ìƒì„¸ ë³´ê¸°", key="log_viewer_show_payload"):
            st.markdown("#### ğŸ” Payload ìƒì„¸")
            selected_index = st.selectbox(
                "ë¡œê·¸ ì„ íƒ",
                options=range(len(filtered_df)),
                format_func=lambda x: f"{filtered_df.iloc[x]['event_time']} - {filtered_df.iloc[x]['event_name']}" if x < len(filtered_df) else ""
            )
            
            if selected_index < len(filtered_df):
                selected_row = filtered_df.iloc[selected_index]
                payload = _parse_payload(selected_row.get("payload"))
                if payload:
                    st.json(payload)
                else:
                    st.info("Payloadê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info("ğŸ“­ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ============================================================================
# KPI ëŒ€ì‹œë³´ë“œ
# ============================================================================

def _render_kpi_dashboard(df_view: pd.DataFrame, session_column: str):
    """
    ğŸ“Š KPI ëŒ€ì‹œë³´ë“œ ë©”ì¸ í˜ì´ì§€
    â†’ "ê¸ˆìœµ ì´ˆë³´ìì˜ ë‰´ìŠ¤ ì´í•´"ë¥¼ ë•ëŠ” ì„œë¹„ìŠ¤ì˜ í•µì‹¬ ì§€í‘œ ëª¨ë‹ˆí„°ë§
    """
    st.markdown("#### ğŸ“Š KPI ëŒ€ì‹œë³´ë“œ ìš”ì•½")
    st.markdown("**í•µì‹¬ ì§ˆë¬¸**: ì‚¬ìš©ìëŠ” ì‹¤ì œë¡œ ë‰´ìŠ¤ë¥¼ ì½ê³  ìˆëŠ”ê°€? ìš©ì–´/Glossary ê¸°ëŠ¥ì€ ì´í•´ë¥¼ ë•ê³  ìˆëŠ”ê°€? ì±—ë´‡ì€ ì§„ì§œ ì‚¬ìš©ë˜ëŠ”ê°€? ì„±ëŠ¥ì€ UXë¥¼ ë§ì¹˜ì§€ ì•ŠëŠ”ê°€?")
    
    # ë‚ ì§œ í•„í„°
    selected_start_date = None
    selected_end_date = None
    date_range_days = None
    
    if "event_time" in df_view.columns and not df_view.empty:
        df_view = df_view.copy()
        df_view["date"] = pd.to_datetime(df_view["event_time"]).dt.date
        df_view["datetime"] = pd.to_datetime(df_view["event_time"])
        df_view["hour"] = df_view["datetime"].dt.hour
        
        min_date = df_view["date"].min()
        max_date = df_view["date"].max()
        
        if min_date and max_date:
            date_range = st.date_input(
                "ê¸°ê°„ ì„ íƒ",
                value=(min_date, max_date),
                min_value=min_date,
                max_value=max_date,
                key="kpi_date_range"
            )
            
            if isinstance(date_range, tuple) and len(date_range) == 2:
                selected_start_date, selected_end_date = date_range
                df_view = df_view[(df_view["date"] >= selected_start_date) & (df_view["date"] <= selected_end_date)]
                date_range_days = (selected_end_date - selected_start_date).days + 1
    
    # ========== A. ìƒë‹¨ Summary (ë©”íŠ¸ë¦­ ì¹´ë“œ 9ê°œ) ==========
    st.markdown("#### ğŸ“ˆ í•µì‹¬ ì§€í‘œ ìš”ì•½")
    
    # 1. DAU / WAU ê³„ì‚° (í•œêµ­ ì‹œê°„ ê¸°ì¤€)
    # DAU: ì˜¤ëŠ˜ ë‚ ì§œì— ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œí‚¨ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
    # WAU: ìµœê·¼ 7ì¼ê°„ ì´ë²¤íŠ¸ë¥¼ ë°œìƒì‹œí‚¨ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
    # ì£¼ì˜: ë‚ ì§œ í•„í„°ê°€ ì ìš©ëœ ê²½ìš°ì—ë„ ì „ì²´ ê¸°ê°„ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
    if "user_id" in df_view.columns and "date" in df_view.columns:
        # ì›ë³¸ ë°ì´í„°ì—ì„œ ë‚ ì§œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸ (í•„í„°ë§ ì „)
        # í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì˜¤ëŠ˜ ë‚ ì§œ ê³„ì‚°
        today_kst = pd.Timestamp.now(tz="Asia/Seoul").date()
        week_ago = today_kst - pd.Timedelta(days=7)
        
        # DAU: ì˜¤ëŠ˜ ë‚ ì§œì˜ ê³ ìœ  ì‚¬ìš©ì ìˆ˜ (í•„í„°ë§ëœ df_view ê¸°ì¤€)
        dau = df_view[df_view["date"] == today_kst]["user_id"].nunique()
        
        # WAU: ìµœê·¼ 7ì¼ê°„ ê³ ìœ  ì‚¬ìš©ì ìˆ˜
        # ë‚ ì§œ í•„í„°ê°€ ì ìš©ëœ ê²½ìš°, í•„í„° ë²”ìœ„ ë‚´ì—ì„œë§Œ ê³„ì‚°
        # í•˜ì§€ë§Œ ì›ë³¸ ë°ì´í„° ì „ì²´ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ render í•¨ìˆ˜ì—ì„œ ì „ì²´ ë°ì´í„°ë¥¼ ì „ë‹¬í•´ì•¼ í•¨
        wau_df = df_view[df_view["date"] >= week_ago]
        wau = wau_df["user_id"].nunique()
        
        # ë””ë²„ê¹… ì •ë³´ (í™•ì¥ ê°€ëŠ¥í•œ ì„¹ì…˜ì— í‘œì‹œ)
        with st.expander("ğŸ” DAU/WAU ê³„ì‚° ìƒì„¸ ì •ë³´", expanded=False):
            st.markdown(f"**ê³„ì‚° ê¸°ì¤€ ë‚ ì§œ**: {today_kst}")
            st.markdown(f"**7ì¼ ì „ ë‚ ì§œ**: {week_ago}")
            st.markdown(f"**ë°ì´í„° ë²”ìœ„**: {df_view['date'].min()} ~ {df_view['date'].max()}")
            st.markdown(f"**í•„í„°ë§ëœ ë°ì´í„° ê±´ìˆ˜**: {len(df_view):,}ê±´")
            st.markdown(f"**ìµœê·¼ 7ì¼ ë°ì´í„° ê±´ìˆ˜**: {len(wau_df):,}ê±´")
            st.markdown(f"**ì „ì²´ ê³ ìœ  user_id ìˆ˜**: {df_view['user_id'].nunique()}ëª…")
            st.markdown(f"**ìµœê·¼ 7ì¼ ê³ ìœ  user_id ìˆ˜**: {wau}ëª…")
            
            # user_id ëª©ë¡ í‘œì‹œ (ìƒìœ„ 20ê°œ)
            if wau > 0:
                st.markdown("**ìµœê·¼ 7ì¼ í™œë™ user_id ëª©ë¡ (ìƒìœ„ 20ê°œ)**:")
                user_counts = wau_df.groupby("user_id").size().sort_values(ascending=False).head(20)
                st.dataframe(user_counts.reset_index().rename(columns={0: "ì´ë²¤íŠ¸ ìˆ˜", "user_id": "User ID"}), use_container_width=True)
                
                st.info("ğŸ’¡ **ì°¸ê³ **: user_idëŠ” ë¸Œë¼ìš°ì € localStorageì— ì €ì¥ë©ë‹ˆë‹¤. ë¡œì»¬(`localhost`)ê³¼ ë°°í¬ ì‚¬ì´íŠ¸ëŠ” ë‹¤ë¥¸ ë„ë©”ì¸ì´ë¯€ë¡œ ê°ê° ë‹¤ë¥¸ user_idê°€ ìƒì„±ë©ë‹ˆë‹¤. ê°™ì€ ì‚¬ìš©ìê°€ ë¡œì»¬ê³¼ ë°°í¬ ì‚¬ì´íŠ¸ì—ì„œ ì ‘ì†í•˜ë©´ 2ê°œì˜ user_idë¡œ ì§‘ê³„ë©ë‹ˆë‹¤.")
    else:
        dau = 0
        wau = 0
    
    # 2. í‰ê·  ì„¸ì…˜ ê¸¸ì´ ê³„ì‚°
    # ê° ì„¸ì…˜ì˜ ì²« ì´ë²¤íŠ¸ì™€ ë§ˆì§€ë§‰ ì´ë²¤íŠ¸ ì‹œê°„ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ì—¬ í‰ê· 
    if session_column in df_view.columns and "event_time" in df_view.columns:
        session_durations = []
        for session_id in df_view[session_column].dropna().unique():
            session_events = df_view[df_view[session_column] == session_id]
            if len(session_events) > 1:
                session_start = session_events["event_time"].min()
                session_end = session_events["event_time"].max()
                duration = (session_end - session_start).total_seconds() / 60  # ë¶„ ë‹¨ìœ„
                if duration > 0:
                    session_durations.append(duration)
        avg_session_length = sum(session_durations) / len(session_durations) if session_durations else 0
    else:
        avg_session_length = 0
    
    # 3. ì„¸ì…˜ë‹¹ ë‰´ìŠ¤ í´ë¦­ ìˆ˜
    news_clicks = int((df_view["event_name"] == "news_click").sum())
    total_sessions = df_view[session_column].nunique() if session_column in df_view.columns else 1
    news_clicks_per_session = news_clicks / total_sessions if total_sessions > 0 else 0
    
    # 4. Glossary ì‚¬ìš©ë¥  (Glossary í´ë¦­ë§Œ)
    glossary_sessions = set(df_view[df_view["event_name"] == "glossary_click"][session_column].dropna().unique()) if session_column in df_view.columns else set()
    glossary_usage_count = len(glossary_sessions)
    glossary_usage_rate = (glossary_usage_count / total_sessions * 100) if total_sessions > 0 else 0
    
    # 5. ì§ˆë¬¸ ì‚¬ìš©ë¥  (RAG ì§ˆë¬¸ë§Œ)
    rag_chat_question_sessions = _get_rag_chat_question_sessions(df_view, session_column)
    question_usage_count = len(rag_chat_question_sessions)
    question_usage_rate = (question_usage_count / total_sessions * 100) if total_sessions > 0 else 0
    
    # 6. ì‹ ê·œ/ì¬ë°©ë¬¸ ë¹„ìœ¨
    if "user_id" in df_view.columns and session_column in df_view.columns:
        # ê° ì‚¬ìš©ìì˜ ì²« ì„¸ì…˜ ì°¾ê¸°
        user_first_sessions = {}
        for idx, row in df_view.iterrows():
            user_id = row.get("user_id")
            session_id = row.get(session_column)
            if user_id and session_id:
                if user_id not in user_first_sessions:
                    user_first_sessions[user_id] = session_id
        
        # ì‹ ê·œ ì‚¬ìš©ì: ì²« ì„¸ì…˜ì´ í˜„ì¬ ê¸°ê°„ ë‚´ì— ìˆëŠ” ì‚¬ìš©ì
        new_users = set(user_first_sessions.keys())
        new_user_sessions = set(user_first_sessions.values())
        new_session_count = len(new_user_sessions & set(df_view[session_column].dropna().unique()))
        
        # ì¬ë°©ë¬¸ ì‚¬ìš©ì: ì²« ì„¸ì…˜ì´ í˜„ì¬ ê¸°ê°„ ì´ì „ì— ìˆëŠ” ì‚¬ìš©ì
        returning_session_count = total_sessions - new_session_count
        new_return_rate = (new_session_count / total_sessions * 100) if total_sessions > 0 else 0
        returning_rate = (returning_session_count / total_sessions * 100) if total_sessions > 0 else 0
    else:
        new_session_count = 0
        returning_session_count = 0
        new_return_rate = 0
        returning_rate = 0
    
    # ë©”íŠ¸ë¦­ ì¹´ë“œ í‘œì‹œ (í•µì‹¬ 15ê°œ ì§€í‘œ ì¤‘ KPI/Usage ë¶€ë¶„)
    st.markdown("##### ğŸ”µ KPI / Usage")
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("DAU", f"{dau}ëª…")
        st.metric("WAU", f"{wau}ëª…")
    with col2:
        st.metric("ì‹ ê·œ ì„¸ì…˜", f"{new_session_count}ê°œ")
        st.caption(f"ì‹ ê·œ ë¹„ìœ¨: {new_return_rate:.1f}%")
        st.metric("ì¬ë°©ë¬¸ ì„¸ì…˜", f"{returning_session_count}ê°œ")
        st.caption(f"ì¬ë°©ë¬¸ ë¹„ìœ¨: {returning_rate:.1f}%")
    with col3:
        st.metric("ì„¸ì…˜ë‹¹ ë‰´ìŠ¤ í´ë¦­", f"{news_clicks_per_session:.1f}ê±´")
        st.metric("Glossary ì‚¬ìš©ë¥ ", f"{glossary_usage_rate:.1f}%")
    with col4:
        st.metric("ì§ˆë¬¸ ì‚¬ìš©ë¥ ", f"{question_usage_rate:.1f}%")
    
    st.markdown("---")
    
    # ========== B. ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´ ==========
    st.markdown("#### ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´")
    
    if "date" in df_view.columns and "hour" in df_view.columns and px is not None:
        # ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´
        hourly_news_clicks = df_view[df_view["event_name"] == "news_click"].groupby("hour").size().reset_index(name="í´ë¦­ ìˆ˜")
        
        if len(hourly_news_clicks) > 0:
            hourly_news_clicks = hourly_news_clicks.sort_values("hour")
            
            fig1 = px.line(
                hourly_news_clicks,
                x="hour",
                y="í´ë¦­ ìˆ˜",
                title="ì‹œê°„ëŒ€ë³„ ë‰´ìŠ¤ í´ë¦­ ì¶”ì´",
                labels={"hour": "ì‹œê°„ (ì‹œ)", "í´ë¦­ ìˆ˜": "í´ë¦­ ìˆ˜"}
            )
            fig1.update_xaxes(tickmode='linear', tick0=0, dtick=3, tickformat='%Hì‹œ')
            st.plotly_chart(fig1, use_container_width=True)
    
    st.markdown("---")
    
    # ========== C. í–‰ë™ íë¦„ (Funnel Chart) ==========
    st.markdown("#### ğŸ”½ í–‰ë™ íë¦„ ë¶„ì„")
    
    # ========== í¼ë„ 1: ë‰´ìŠ¤ ê¸°ë°˜ ì´í•´ í¼ë„ (News-based Understanding Funnel) ==========
    # ëª©ì : ì´ˆë³´ìê°€ ë‰´ìŠ¤ë¥¼ ì–¼ë§ˆë‚˜ 'ì´í•´'í–ˆëŠ”ê°€ ì¸¡ì •
    # ë‰´ìŠ¤ í´ë¦­ â†’ Glossary í´ë¦­ â†’ ì§ˆë¬¸ â†’ ì¬íƒìƒ‰
    st.markdown("##### 1ï¸âƒ£ ë‰´ìŠ¤ ê¸°ë°˜ ì´í•´ í¼ë„ (ë‰´ìŠ¤ í´ë¦­ â†’ Glossary í´ë¦­ â†’ ì§ˆë¬¸ â†’ ì¬íƒìƒ‰)")
    st.caption("**ëª©ì **: í™ˆ í™”ë©´ì—ì„œ ì¶”ì²œ ë‰´ìŠ¤ë¥¼ ë³¸ ì‚¬ìš©ìì˜ í•™ìŠµ íë¦„ ë¶„ì„")
    st.caption("**ì°¸ê³ **: Glossary í´ë¦­(ìš©ì–´ í•™ìŠµ)ê³¼ ì§ˆë¬¸(ë‰´ìŠ¤ ì´í•´)ì„ ë¶„ë¦¬í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # í™ˆì—ì„œ ë‰´ìŠ¤ í´ë¦­í•œ ì„¸ì…˜ (ê²€ìƒ‰ì´ ì•„ë‹Œ ì§ì ‘ í´ë¦­)
    # sourceê°€ "list" ë˜ëŠ” "home"ì¸ news_clickë§Œ í¬í•¨ (ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì˜¨ ê²ƒì€ ì œì™¸)
    home_news_clicks = df_view[
        (df_view["event_name"] == "news_click") &
        (df_view["source"].isin(["list", "home", ""]) | df_view["source"].isna())
    ]
    news_click_sessions = home_news_clicks[session_column].nunique() if session_column in home_news_clicks.columns and not home_news_clicks.empty else 0
    
    if news_click_sessions > 0:
        # ê° ì´ë²¤íŠ¸ë³„ ì„¸ì…˜ ì§‘í•© ìƒì„±
        news_sessions = set(home_news_clicks[session_column].dropna().unique())
        
        # 2ë‹¨ê³„: Glossary í´ë¦­ (ìš©ì–´ í•™ìŠµ)
        glossary_sessions = set(df_view[df_view["event_name"] == "glossary_click"][session_column].dropna().unique())
        glossary_after_news = len(news_sessions & glossary_sessions)
        
        # 3ë‹¨ê³„: ì§ˆë¬¸ (ë‰´ìŠ¤ ì´í•´ ì§ˆë¬¸)
        # ë‰´ìŠ¤ í´ë¦­ ì´í›„ ë°œìƒí•œ chat_questionë§Œ í¬í•¨ (RAG ì—¬ë¶€, ì¶œì²˜ ë¬´ê´€)
        # ì–´ëŠ ë‰´ìŠ¤ ë’¤ë¼ë„ ì§ˆë¬¸ì„ í•œ ì„¸ì…˜ì„ ì¡ê¸° ìœ„í•´ ì²« ë²ˆì§¸ ë‰´ìŠ¤ ì´í›„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í™•ì¸
        question_sessions = set()
        for session_id in news_sessions:
            session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
            # ë‰´ìŠ¤ í´ë¦­ ì´í›„ ì§ˆë¬¸ í™•ì¸ (ì²« ë²ˆì§¸ ë‰´ìŠ¤ í´ë¦­ ì´í›„ ë°œìƒí•œ chat_questionë§Œ)
            news_click_indices = session_events[session_events["event_name"] == "news_click"].index
            if len(news_click_indices) > 0:
                first_news_idx = news_click_indices[0]
                after_first_news = session_events.loc[session_events.index > first_news_idx]
                # ì²« ë²ˆì§¸ ë‰´ìŠ¤ í´ë¦­ ì´í›„ chat_question ë°œìƒ í™•ì¸ (RAG ì—¬ë¶€ ë¬´ê´€)
                if (after_first_news["event_name"] == "chat_question").any():
                    question_sessions.add(session_id)
        question_count = len(question_sessions)
        
        # 4ë‹¨ê³„: ì¬íƒìƒ‰ (ì§ˆë¬¸ ë˜ëŠ” Glossary í´ë¦­ ì´í›„ ë‹¤ì‹œ ë‰´ìŠ¤ í´ë¦­)
        re_explore_sessions = set()
        for session_id in news_sessions:
            session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
            # Glossary í´ë¦­ ë˜ëŠ” ì§ˆë¬¸ ì´í›„ ì¬íƒìƒ‰ í™•ì¸
            glossary_indices = session_events[session_events["event_name"] == "glossary_click"].index
            question_indices = session_events[session_events["event_name"] == "chat_question"].index
            
            # ë§ˆì§€ë§‰ Glossary í´ë¦­ ë˜ëŠ” ì§ˆë¬¸ ì°¾ê¸°
            last_interaction_idx = None
            if len(glossary_indices) > 0:
                last_interaction_idx = max(glossary_indices.tolist())
            if len(question_indices) > 0:
                last_question_idx = max(question_indices.tolist())
                if last_interaction_idx is None or last_question_idx > last_interaction_idx:
                    last_interaction_idx = last_question_idx
            
            if last_interaction_idx is not None:
                after_interaction = session_events.loc[session_events.index > last_interaction_idx]
                if len(after_interaction) > 0:
                    has_re_explore = (
                        (after_interaction["event_name"] == "news_click").any() or
                        (after_interaction["event_name"] == "news_search_from_chat").any()
                    )
                    if has_re_explore:
                        re_explore_sessions.add(session_id)
        re_explore_count = len(re_explore_sessions)
        
        # ì „í™˜ìœ¨ ê³„ì‚°
        glossary_rate = (glossary_after_news / news_click_sessions * 100) if news_click_sessions > 0 else 0
        question_rate = (question_count / news_click_sessions * 100) if news_click_sessions > 0 else 0
        re_explore_rate = (re_explore_count / news_click_sessions * 100) if news_click_sessions > 0 else 0
        
        funnel1_data = pd.DataFrame({
            "ë‹¨ê³„": ["ë‰´ìŠ¤ í´ë¦­", "Glossary í´ë¦­", "ì§ˆë¬¸", "ì¬íƒìƒ‰"],
            "ì„¸ì…˜ ìˆ˜": [news_click_sessions, glossary_after_news, question_count, re_explore_count],
            "ì „í™˜ìœ¨ (%)": [100.0, glossary_rate, question_rate, re_explore_rate]
        })
        
        st.dataframe(funnel1_data, use_container_width=True)
        
        if px is not None:
            fig1 = px.funnel(
                funnel1_data,
                x="ì„¸ì…˜ ìˆ˜",
                y="ë‹¨ê³„",
                title="ë‰´ìŠ¤ ê¸°ë°˜ ì´í•´ í¼ë„ (ë‰´ìŠ¤ â†’ ìš©ì–´ â†’ ì§ˆë¬¸ â†’ ì¬íƒìƒ‰)"
            )
            st.plotly_chart(fig1, use_container_width=True)
        
        # í•µì‹¬ ì§€í‘œ ê°•ì¡°
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Glossary ì‚¬ìš©ë¥ ", f"{glossary_rate:.1f}%", 
                     help="ë‰´ìŠ¤ í´ë¦­ í›„ ìš©ì–´ í•™ìŠµ ë¹„ìœ¨")
        with col2:
            st.metric("ì§ˆë¬¸ ë¹„ìœ¨", f"{question_rate:.1f}%",
                     help="ë‰´ìŠ¤ í´ë¦­ í›„ ì§ˆë¬¸ ë¹„ìœ¨")
        with col3:
            st.metric("ì¬íƒìƒ‰ìœ¨", f"{re_explore_rate:.1f}%",
                     help="í•™ìŠµ í›„ ì¶”ê°€ íƒìƒ‰ìœ¼ë¡œ ì´ì–´ì¡ŒëŠ”ì§€")
    
    # ========== í¼ë„ 2: ì±—ë´‡ ê¸°ë°˜ íƒìƒ‰ í¼ë„ (Chatbot-based Exploration Funnel) ==========
    # ëª©ì : ì±—ë´‡ì„ í†µí•œ íƒìƒ‰ ê¸°ëŠ¥ì´ ì–¼ë§ˆë‚˜ ë„ì›€ì´ ë˜ëŠ”ì§€ ì¸¡ì •
    # ì§ˆë¬¸ â†’ ì±—ë´‡ ì‘ë‹µ â†’ ì¶”ì²œ ë‰´ìŠ¤ í´ë¦­ â†’ Glossary í´ë¦­ â†’ ì¶”ê°€ ì§ˆë¬¸
    st.markdown("---")
    st.markdown("##### 2ï¸âƒ£ ì±—ë´‡ ê¸°ë°˜ íƒìƒ‰ í¼ë„ (ì§ˆë¬¸ â†’ ì±—ë´‡ ì‘ë‹µ â†’ ì¶”ì²œ ë‰´ìŠ¤ í´ë¦­ â†’ Glossary í´ë¦­ â†’ ì¶”ê°€ ì§ˆë¬¸)")
    st.caption("**ëª©ì **: ì±—ë´‡ì„ í†µí•œ íƒìƒ‰ ê¸°ëŠ¥ì˜ íš¨ê³¼ì„± ë° ì¶”ì²œ ë‰´ìŠ¤ í’ˆì§ˆ í‰ê°€")
    st.caption("**ì°¸ê³ **: ì±—ë´‡ì´ ì¶”ì²œí•œ ë‰´ìŠ¤ë¥¼ í´ë¦­í•œ í›„ì˜ í•™ìŠµ íë¦„ì„ ë¶„ì„í•©ë‹ˆë‹¤.")
    
    # 1ë‹¨ê³„: ì§ˆë¬¸ ì‹œì‘ (chat_question)
    # chat_question ì¤‘ì—ì„œ ì´í›„ chat_responseê°€ ë°œìƒí•œ ê²ƒë§Œ (RAG ì§ˆë¬¸ë§Œ ì œì™¸)
    # ê°™ì€ ì„¸ì…˜ ë‚´ì— ì¼ë°˜ ì§ˆë¬¸ê³¼ RAG ì§ˆë¬¸ì´ ì„ì—¬ ìˆì–´ë„ ì¼ë°˜ ì§ˆë¬¸ì´ ìˆìœ¼ë©´ í¬í•¨
    all_chat_questions = df_view[df_view["event_name"] == "chat_question"].copy()
    question_sessions = set()
    
    for session_id in all_chat_questions[session_column].dropna().unique():
        session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
        chat_question_indices = list(session_events[session_events["event_name"] == "chat_question"].index)
        
        for i, chat_idx in enumerate(chat_question_indices):
            # ì´ ì§ˆë¬¸ì˜ 'ë'ì„ ë‹¤ìŒ ì§ˆë¬¸ ì§ì „ê¹Œì§€ë¡œ ì„¤ì • (í•´ë‹¹ ì§ˆë¬¸ë§Œì˜ ìœˆë„ìš°)
            if i < len(chat_question_indices) - 1:
                end_idx = chat_question_indices[i + 1]
                window = session_events.loc[(session_events.index > chat_idx) & (session_events.index < end_idx)]
            else:
                # ë§ˆì§€ë§‰ ì§ˆë¬¸ì¸ ê²½ìš° ì„¸ì…˜ ëê¹Œì§€
                window = session_events.loc[session_events.index > chat_idx]
            
            # RAG ì§ˆë¬¸ì¸ì§€ í™•ì¸ (í•´ë‹¹ ì§ˆë¬¸ ìœˆë„ìš° ë‚´ì—ì„œ glossary_answer ë°œìƒ)
            has_glossary_answer = (window["event_name"] == "glossary_answer").any()
            # ì¼ë°˜ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (í•´ë‹¹ ì§ˆë¬¸ ìœˆë„ìš° ë‚´ì—ì„œ chat_response ë°œìƒ)
            has_chat_response = (window["event_name"] == "chat_response").any()
            
            # ì¼ë°˜ ì§ˆë¬¸ì´ê³ , RAG ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
            if has_chat_response and not has_glossary_answer:
                question_sessions.add(session_id)
                break  # í•œ ì„¸ì…˜ì— ì—¬ëŸ¬ ì§ˆë¬¸ì´ ìˆì–´ë„ í•œ ë²ˆë§Œ ì¶”ê°€
    
    question_count = len(question_sessions)
    
    if question_count > 0:
        # 2ë‹¨ê³„: ì±—ë´‡ ì‘ë‹µ (chat_response)
        chat_response_sessions = set()
        for session_id in question_sessions:
            session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
            chat_question_indices = session_events[session_events["event_name"] == "chat_question"].index
            
            for chat_idx in chat_question_indices:
                after_chat = session_events.loc[session_events.index > chat_idx]
                chat_responses = after_chat[after_chat["event_name"] == "chat_response"]
                
                if len(chat_responses) > 0:
                    chat_response_sessions.add(session_id)
                    break
        
        chat_response_count = len(chat_response_sessions)
        
        # 3ë‹¨ê³„: ì¶”ì²œ ë‰´ìŠ¤ í´ë¦­ (news_click source="chat" ë˜ëŠ” news_selected_from_chat)
        # chat_response ì´í›„ì— ì±—ë´‡ì´ ì¶”ì²œí•œ ë‰´ìŠ¤ë¥¼ í´ë¦­í•œ ì„¸ì…˜
        news_click_sessions = set()
        for session_id in chat_response_sessions:
            session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
            chat_response_indices = session_events[session_events["event_name"] == "chat_response"].index
            
            for resp_idx in chat_response_indices:
                after_response = session_events.loc[session_events.index > resp_idx]
                # news_click (source="chat") ë˜ëŠ” news_selected_from_chat ë°œìƒ
                has_news_click = (
                    ((after_response["event_name"] == "news_click") & (after_response["source"] == "chat")).any() or
                    (after_response["event_name"] == "news_selected_from_chat").any()
                )
                if has_news_click:
                    news_click_sessions.add(session_id)
                    break
        
        news_click_count = len(news_click_sessions)
        
        # 4ë‹¨ê³„: Glossary í´ë¦­
        glossary_sessions = set(df_view[df_view["event_name"] == "glossary_click"][session_column].dropna().unique())
        glossary_after_news = len(news_click_sessions & glossary_sessions)
        
        # 5ë‹¨ê³„: ì¶”ê°€ ì§ˆë¬¸ (chat_question - ë‰´ìŠ¤ í´ë¦­ ì´í›„ ì§ˆë¬¸)
        # ë‰´ìŠ¤ í´ë¦­ ì´í›„ì— ì§ˆë¬¸ì´ ë°œìƒí•œ ì„¸ì…˜ (RAG ì§ˆë¬¸ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
        additional_question_sessions = set()
        for session_id in news_click_sessions:
            session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
            # ë‰´ìŠ¤ í´ë¦­ ì´í›„ ì§ˆë¬¸ í™•ì¸
            news_click_indices = session_events[
                ((session_events["event_name"] == "news_click") & (session_events["source"] == "chat")) |
                (session_events["event_name"] == "news_selected_from_chat")
            ].index
            if len(news_click_indices) > 0:
                last_news_idx = news_click_indices[-1]
                after_news = session_events.loc[session_events.index > last_news_idx]
                # chat_question ë°œìƒ í™•ì¸
                if (after_news["event_name"] == "chat_question").any():
                    additional_question_sessions.add(session_id)
        additional_question_count = len(additional_question_sessions)
        
        # ì „í™˜ìœ¨ ê³„ì‚°
        response_rate = (chat_response_count / question_count * 100) if question_count > 0 else 0
        news_click_rate = (news_click_count / chat_response_count * 100) if chat_response_count > 0 else 0
        glossary_rate = (glossary_after_news / news_click_count * 100) if news_click_count > 0 else 0
        additional_question_rate = (additional_question_count / news_click_count * 100) if news_click_count > 0 else 0
        
        funnel2_data = pd.DataFrame({
            "ë‹¨ê³„": ["ì§ˆë¬¸", "ì±—ë´‡ ì‘ë‹µ", "ì¶”ì²œ ë‰´ìŠ¤ í´ë¦­", "Glossary í´ë¦­", "ì¶”ê°€ ì§ˆë¬¸"],
            "ì„¸ì…˜ ìˆ˜": [question_count, chat_response_count, news_click_count, glossary_after_news, additional_question_count],
            "ì „í™˜ìœ¨ (%)": [100.0, response_rate, news_click_rate, glossary_rate, additional_question_rate]
        })
        
        st.dataframe(funnel2_data, use_container_width=True)
        
        if px is not None:
            fig2 = px.funnel(
                funnel2_data,
                x="ì„¸ì…˜ ìˆ˜",
                y="ë‹¨ê³„",
                title="ì±—ë´‡ ê¸°ë°˜ íƒìƒ‰ í¼ë„ (ì§ˆë¬¸ â†’ ë‰´ìŠ¤ë¥¼ í†µí•œ í•™ìŠµ íë¦„)"
            )
            st.plotly_chart(fig2, use_container_width=True)
        
        # í•µì‹¬ ì§€í‘œ ê°•ì¡°
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì¶”ì²œ ë‰´ìŠ¤ í´ë¦­ë¥ ", f"{news_click_rate:.1f}%",
                     help="ì±—ë´‡ ì‘ë‹µ í›„ ì¶”ì²œ ë‰´ìŠ¤ í´ë¦­ë¥ ")
        with col2:
            st.metric("Glossary ì‚¬ìš©ë¥ ", f"{glossary_rate:.1f}%",
                     help="ë‰´ìŠ¤ í´ë¦­ í›„ Glossary ì‚¬ìš©ë¥ ")
        with col3:
            st.metric("ì¶”ê°€ ì§ˆë¬¸ ë¹„ìœ¨", f"{additional_question_rate:.1f}%",
                     help="ë‰´ìŠ¤ í´ë¦­ í›„ ì¶”ê°€ ì§ˆë¬¸ ë¹„ìœ¨")
    
    # ========== í¼ë„ 3: ì „ì²´ í•™ìŠµ ì—¬ì • í¼ë„ (Overall Learning Journey) ==========
    # ëª©ì : ì „ì²´ ì„œë¹„ìŠ¤ ì‚¬ìš© íë¦„ ë¶„ì„
    # ì§„ì… â†’ ë‰´ìŠ¤/ê²€ìƒ‰ ì¤‘ ì„ íƒ â†’ ë‰´ìŠ¤ íƒìƒ‰ â†’ Glossary/ì§ˆë¬¸ (RAG ì‚¬ìš©) â†’ ì¬íƒìƒ‰
    st.markdown("---")
    st.markdown("##### 3ï¸âƒ£ ì „ì²´ í•™ìŠµ ì—¬ì • í¼ë„ (Overall Learning Journey)")
    st.caption("**ëª©ì **: ì „ì²´ ì„œë¹„ìŠ¤ ì‚¬ìš© íë¦„ ë° í•™ìŠµ ì—¬ì • ë¶„ì„")
    st.caption("**ì°¸ê³ **: Glossary í´ë¦­ê³¼ ì±—ë´‡ ì§ˆë¬¸ì€ ëª¨ë‘ RAGë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ í•˜ë‚˜ë¡œ ë¬¶ì–´ì„œ ë¶„ì„í•©ë‹ˆë‹¤. ì‘ë‹µ/í•´ì„¤ì€ ì§ˆë¬¸ì´ ë“¤ì–´ì˜¤ë©´ 100% ìƒì„±ë˜ë¯€ë¡œ í¼ë„ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.")
    
    # ì „ì²´ ì„¸ì…˜ ìˆ˜
    total_sessions = df_view[session_column].nunique() if session_column in df_view.columns else 0
    
    if total_sessions > 0:
        all_sessions = set(df_view[session_column].dropna().unique())
        
        # 1ë‹¨ê³„: ì§„ì… (ì„¸ì…˜ ì‹œì‘)
        entry_sessions = total_sessions
        
        # 2ë‹¨ê³„: ë‰´ìŠ¤/ì±—ë´‡ ì‹œì‘
        # ì²« ë‰´ìŠ¤ í´ë¦­ ë˜ëŠ” ì²« ì±—ë´‡ ì§ˆë¬¸ (ì„±ê²©ê³¼ ë¬´ê´€í•˜ê²Œ)
        news_sessions_all = set(df_view[df_view["event_name"] == "news_click"][session_column].dropna().unique())
        # ì²« ì±—ë´‡ ì§ˆë¬¸ìœ¼ë¡œ ì‹œì‘í•œ ì„¸ì…˜ (RAG ì—¬ë¶€, ë§í¬ ì—¬ë¶€ ë¬´ê´€)
        chat_question_sessions = set(df_view[df_view["event_name"] == "chat_question"][session_column].dropna().unique())
        news_or_chat_sessions = news_sessions_all | chat_question_sessions
        selected_path_count = len(news_or_chat_sessions)
        
        # 3ë‹¨ê³„: ë‰´ìŠ¤ íƒìƒ‰ (ë‰´ìŠ¤ í´ë¦­ ë˜ëŠ” ìƒì„¸ ì—´ê¸°)
        news_explore_sessions = set(df_view[df_view["event_name"].isin(["news_click", "news_detail_open"])][session_column].dropna().unique())
        news_explore_count = len(news_explore_sessions & news_or_chat_sessions)
        
        # 4ë‹¨ê³„: Glossary/ì§ˆë¬¸ (RAG ì‚¬ìš©)
        glossary_all_sessions = set(df_view[df_view["event_name"] == "glossary_click"][session_column].dropna().unique())
        rag_chat_question_all_sessions = _get_rag_chat_question_sessions(df_view, session_column)
        rag_usage_all_sessions = glossary_all_sessions | rag_chat_question_all_sessions  # Glossary ë˜ëŠ” RAG ì§ˆë¬¸
        # 2ë‹¨ê³„ì—ì„œ ì‹œì‘í•œ ì‚¬ìš©ìë“¤ ì¤‘ â†’ 3ë‹¨ê³„ â†’ 4ë‹¨ê³„ë¡œ ê°„ ë¹„ìœ¨ì„ ì •í™•íˆ ë³´ê¸° ìœ„í•´ ì„¸ ì§‘í•© ëª¨ë‘ êµì§‘í•©
        rag_usage_count = len(rag_usage_all_sessions & news_explore_sessions & news_or_chat_sessions)
        
        # 5ë‹¨ê³„: ì¬íƒìƒ‰ (Glossary/ì§ˆë¬¸ ì´í›„ ë‹¤ì‹œ ë‰´ìŠ¤ í´ë¦­ ë˜ëŠ” ê²€ìƒ‰)
        re_explore_all = set()
        for session_id in rag_usage_all_sessions & news_explore_sessions:
            session_events = df_view[df_view[session_column] == session_id].sort_values("event_time")
            # Glossary/ì§ˆë¬¸ ì´ë²¤íŠ¸ ì°¾ê¸° (RAG ì§ˆë¬¸ë§Œ)
            glossary_indices = session_events[session_events["event_name"] == "glossary_click"].index
            rag_chat_indices = session_events[
                (session_events["event_name"] == "chat_question") & 
                (session_events[session_column] == session_id)
            ].index
            # RAG ì±—ë´‡ ì§ˆë¬¸ì¸ì§€ í™•ì¸ (ì´í›„ glossary_answerê°€ ìˆëŠ”ì§€)
            rag_chat_valid_indices = []
            for chat_idx in rag_chat_indices:
                after_chat = session_events.loc[session_events.index > chat_idx]
                if (after_chat["event_name"] == "glossary_answer").any():
                    rag_chat_valid_indices.append(chat_idx)
            rag_indices = glossary_indices.tolist() + rag_chat_valid_indices
            if len(rag_indices) > 0:
                last_rag_idx = max(rag_indices)
                after_rag = session_events.loc[session_events.index > last_rag_idx]
                if len(after_rag) > 0:
                    has_re_explore = (
                        (after_rag["event_name"] == "news_click").any() or
                        (after_rag["event_name"] == "news_search_from_chat").any()
                    )
                    if has_re_explore:
                        re_explore_all.add(session_id)
        re_explore_all_count = len(re_explore_all)
        
        # ì „í™˜ìœ¨ ê³„ì‚°
        path_selection_rate = (selected_path_count / entry_sessions * 100) if entry_sessions > 0 else 0
        news_explore_rate = (news_explore_count / selected_path_count * 100) if selected_path_count > 0 else 0
        rag_usage_journey_rate = (rag_usage_count / news_explore_count * 100) if news_explore_count > 0 else 0
        re_explore_journey_rate = (re_explore_all_count / rag_usage_count * 100) if rag_usage_count > 0 else 0
        
        funnel3_data = pd.DataFrame({
            "ë‹¨ê³„": ["ì§„ì…", "ë‰´ìŠ¤/ì±—ë´‡ ì‹œì‘", "ë‰´ìŠ¤ íƒìƒ‰", "Glossary/ì§ˆë¬¸ (RAG)", "ì¬íƒìƒ‰"],
            "ì„¸ì…˜ ìˆ˜": [entry_sessions, selected_path_count, news_explore_count, rag_usage_count, re_explore_all_count],
            "ì „í™˜ìœ¨ (%)": [100.0, path_selection_rate, news_explore_rate, rag_usage_journey_rate, re_explore_journey_rate]
        })
        
        st.dataframe(funnel3_data, use_container_width=True)
        
        if px is not None:
            fig3 = px.funnel(
                funnel3_data,
                x="ì„¸ì…˜ ìˆ˜",
                y="ë‹¨ê³„",
                title="ì „ì²´ í•™ìŠµ ì—¬ì • í¼ë„ (Overall Learning Journey)"
            )
            st.plotly_chart(fig3, use_container_width=True)
    


# ============================================================================
# ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë° í‚¤ì›Œë“œ ë¶„ì„ íŒ¨ë„ (5ê°œ)
# ============================================================================

def _render_category_distribution_for_prompt(news_df: pd.DataFrame):
    """
    1ï¸âƒ£ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (í”„ë¡¬í”„íŠ¸ ê°œì„  ê²€ì¦ìš©)
    LLM ë¶„ë¥˜ ê²°ê³¼ê°€ ê· í˜• ìˆê²Œ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸
    basic_concept, macro_market, major_industry, investment_basic ë“±
    """
    if news_df.empty:
        return
    
    if "primary_category" not in news_df.columns:
        st.info("ğŸ“Š primary_category ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    st.markdown("##### 1ï¸âƒ£ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (LLM ë¶„ë¥˜ ê²°ê³¼ ê²€ì¦)")
    
    # primary_categoryê°€ NULLì´ ì•„ë‹Œ ë°ì´í„°ë§Œ í•„í„°ë§
    valid_categories = news_df[news_df["primary_category"].notna() & (news_df["primary_category"] != "")]
    
    if valid_categories.empty:
        st.info("ğŸ“Š ì¹´í…Œê³ ë¦¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    total_count = len(valid_categories)
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜ ì§‘ê³„
    category_counts = valid_categories["primary_category"].value_counts()
    
    # ì´ˆë³´ì ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (í”„ë¡¬í”„íŠ¸ ê°œì„  ê²€ì¦ìš©)
    beginner_categories = {
        "basic_concept": "ê¸°ì´ˆ ê°œë…",
        "macro_market": "ê±°ì‹œ ì‹œì¥",
        "major_industry": "ì£¼ìš” ì‚°ì—…",
        "investment_basic": "íˆ¬ì ê¸°ì´ˆ"
    }
    
    # ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ë„ í¬í•¨
    all_categories = {}
    for cat in category_counts.index:
        if cat in beginner_categories:
            all_categories[cat] = beginner_categories[cat]
        else:
            all_categories[cat] = cat
    
    # ì¹´í…Œê³ ë¦¬ë³„ ê±´ìˆ˜ ë° ë¹„ìœ¨ ê³„ì‚°
    category_data = []
    for cat, count in category_counts.items():
        category_data.append({
            "ì¹´í…Œê³ ë¦¬": all_categories.get(cat, cat),
            "ê±´ìˆ˜": count,
            "ë¹„ìœ¨ (%)": round((count / total_count) * 100, 1) if total_count > 0 else 0
        })
    
    category_df = pd.DataFrame(category_data).sort_values("ê±´ìˆ˜", ascending=False)
    
    col1, col2 = st.columns(2)
    
    with col1:
        # Bar ì°¨íŠ¸
        if px is not None:
            fig_bar = px.bar(
                category_df,
                x="ì¹´í…Œê³ ë¦¬",
                y="ê±´ìˆ˜",
                title="ì¹´í…Œê³ ë¦¬ë³„ ìˆ˜ì§‘ ê±´ìˆ˜",
                labels={"ì¹´í…Œê³ ë¦¬": "ì¹´í…Œê³ ë¦¬", "ê±´ìˆ˜": "ê±´ìˆ˜"},
                text="ê±´ìˆ˜"
            )
            fig_bar.update_traces(texttemplate='%{text}', textposition='outside')
            fig_bar.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig_bar, use_container_width=True)
    
    with col2:
        # Pie ì°¨íŠ¸
        if px is not None:
            fig_pie = px.pie(
                category_df,
                values="ê±´ìˆ˜",
                names="ì¹´í…Œê³ ë¦¬",
                title="ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë¹„ìœ¨"
            )
            fig_pie.update_layout(height=400)
            st.plotly_chart(fig_pie, use_container_width=True)
    
    # ê· í˜•ë„ ê²½ê³ 
    if len(category_counts) > 0:
        max_ratio = category_df["ë¹„ìœ¨ (%)"].max()
        if max_ratio > 60:
            st.warning(f"âš ï¸ íŠ¹ì • ì¹´í…Œê³ ë¦¬ê°€ {max_ratio:.1f}%ë¡œ í¸ì¤‘ë˜ì–´ ìˆìŠµë‹ˆë‹¤. LLM ë¶„ë¥˜ ê¸°ì¤€ì„ ì¬ê²€í† í•˜ì„¸ìš”.")
        elif max_ratio < 20 and len(category_counts) >= 4:
            st.info("âœ… ì¹´í…Œê³ ë¦¬ ë¶„í¬ê°€ ë¹„êµì  ê· í˜• ì¡í˜€ ìˆìŠµë‹ˆë‹¤.")

def _render_category_engagement_analysis(news_df: pd.DataFrame, event_logs_df: pd.DataFrame):
    """ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš©ì ì°¸ì—¬ë„ ë¶„ì„ (ì²´ë¥˜ì‹œê°„ / Glossary í´ë¦­ë¥ )"""
    if news_df.empty or event_logs_df.empty:
        return
    
    if "primary_category" not in news_df.columns:
        return
    
    st.markdown("#### ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì‚¬ìš©ì ì°¸ì—¬ë„ ë¶„ì„")
    
    # ë‰´ìŠ¤ í´ë¦­ ë° ìƒì„¸ ì—´ê¸° ì´ë²¤íŠ¸ ì¶”ì¶œ
    news_clicks = event_logs_df[event_logs_df["event_name"] == "news_click"].copy()
    detail_opens = event_logs_df[event_logs_df["event_name"] == "news_detail_open"].copy()
    glossary_clicks = event_logs_df[event_logs_df["event_name"] == "glossary_click"].copy()
    
    if news_clicks.empty and detail_opens.empty:
        st.info("ğŸ“Š ë‰´ìŠ¤ í´ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # news_id ì¶”ì¶œ ë° ì¹´í…Œê³ ë¦¬ ë§¤í•‘
    def _get_news_id_from_row(row):
        news_id = row.get("news_id")
        if pd.notna(news_id) and news_id != "":
            return str(news_id)
        payload = _parse_payload(row.get("payload"))
        if payload:
            news_id = payload.get("news_id")
            if news_id:
                return str(news_id)
        return None
    
    # ë‰´ìŠ¤ IDë³„ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
    news_id_to_category = {}
    for idx, row in news_df.iterrows():
        news_id = str(row.get("news_id", ""))
        category = row.get("primary_category")
        if pd.notna(category) and category != "":
            news_id_to_category[news_id] = category
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ìˆ˜ì§‘
    category_stats = {}
    
    # ë‰´ìŠ¤ í´ë¦­ ìˆ˜
    for idx, row in news_clicks.iterrows():
        news_id = _get_news_id_from_row(row)
        if news_id and news_id in news_id_to_category:
            category = news_id_to_category[news_id]
            if category not in category_stats:
                category_stats[category] = {
                    "clicks": 0,
                    "detail_opens": 0,
                    "glossary_clicks": 0,
                    "dwell_times": []
                }
            category_stats[category]["clicks"] += 1
    
    # ìƒì„¸ ì—´ê¸° ìˆ˜ ë° ì²´ë¥˜ì‹œê°„ ê³„ì‚°
    for idx, row in detail_opens.iterrows():
        news_id = _get_news_id_from_row(row)
        if news_id and news_id in news_id_to_category:
            category = news_id_to_category[news_id]
            if category not in category_stats:
                category_stats[category] = {
                    "clicks": 0,
                    "detail_opens": 0,
                    "glossary_clicks": 0,
                    "dwell_times": []
                }
            category_stats[category]["detail_opens"] += 1
            
            # ì²´ë¥˜ì‹œê°„ ê³„ì‚° (payloadì—ì„œ duration_sec ì¶”ì¶œ)
            payload = _parse_payload(row.get("payload"))
            if payload:
                duration_sec = payload.get("duration_sec")
                if duration_sec is not None:
                    try:
                        category_stats[category]["dwell_times"].append(float(duration_sec))
                    except:
                        pass
    
    # Glossary í´ë¦­ ìˆ˜
    for idx, row in glossary_clicks.iterrows():
        news_id = _get_news_id_from_row(row)
        if news_id and news_id in news_id_to_category:
            category = news_id_to_category[news_id]
            if category not in category_stats:
                category_stats[category] = {
                    "clicks": 0,
                    "detail_opens": 0,
                    "glossary_clicks": 0,
                    "dwell_times": []
                }
            category_stats[category]["glossary_clicks"] += 1
    
    if not category_stats:
        st.info("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì°¸ì—¬ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # í†µê³„ ë°ì´í„°í”„ë ˆì„ ìƒì„±
    stats_data = []
    for category, stats in category_stats.items():
        avg_dwell = sum(stats["dwell_times"]) / len(stats["dwell_times"]) if stats["dwell_times"] else 0
        glossary_rate = (stats["glossary_clicks"] / stats["clicks"] * 100) if stats["clicks"] > 0 else 0
        
        stats_data.append({
            "ì¹´í…Œê³ ë¦¬": category,
            "ë‰´ìŠ¤ í´ë¦­ ìˆ˜": stats["clicks"],
            "ìƒì„¸ ì—´ê¸° ìˆ˜": stats["detail_opens"],
            "í‰ê·  ì²´ë¥˜ì‹œê°„ (ì´ˆ)": round(avg_dwell, 1),
            "Glossary í´ë¦­ ìˆ˜": stats["glossary_clicks"],
            "Glossary í´ë¦­ë¥  (%)": round(glossary_rate, 1)
        })
    
    stats_df = pd.DataFrame(stats_data).sort_values("ë‰´ìŠ¤ í´ë¦­ ìˆ˜", ascending=False)
    
    # ì‹œê°í™”
    col1, col2 = st.columns(2)
    
    with col1:
        # í‰ê·  ì²´ë¥˜ì‹œê°„ ë¹„êµ
        if px is not None and len(stats_df) > 0:
            fig_dwell = px.bar(
                stats_df,
                x="ì¹´í…Œê³ ë¦¬",
                y="í‰ê·  ì²´ë¥˜ì‹œê°„ (ì´ˆ)",
                title="ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ì²´ë¥˜ì‹œê°„",
                labels={"ì¹´í…Œê³ ë¦¬": "ì¹´í…Œê³ ë¦¬", "í‰ê·  ì²´ë¥˜ì‹œê°„ (ì´ˆ)": "ì²´ë¥˜ì‹œê°„ (ì´ˆ)"},
                text="í‰ê·  ì²´ë¥˜ì‹œê°„ (ì´ˆ)"
            )
            fig_dwell.update_traces(texttemplate='%{text:.1f}ì´ˆ', textposition='outside')
            fig_dwell.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig_dwell, use_container_width=True)
    
    with col2:
        # Glossary í´ë¦­ë¥  ë¹„êµ
        if px is not None and len(stats_df) > 0:
            fig_glossary = px.bar(
                stats_df,
                x="ì¹´í…Œê³ ë¦¬",
                y="Glossary í´ë¦­ë¥  (%)",
                title="ì¹´í…Œê³ ë¦¬ë³„ Glossary í´ë¦­ë¥ ",
                labels={"ì¹´í…Œê³ ë¦¬": "ì¹´í…Œê³ ë¦¬", "Glossary í´ë¦­ë¥  (%)": "í´ë¦­ë¥  (%)"},
                text="Glossary í´ë¦­ë¥  (%)"
            )
            fig_glossary.update_traces(texttemplate='%{text:.1f}%', textposition='outside')
            fig_glossary.update_layout(height=400, showlegend=False)
            st.plotly_chart(fig_glossary, use_container_width=True)
    
    # ìƒì„¸ í†µê³„ í…Œì´ë¸”
    st.markdown("**ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì°¸ì—¬ í†µê³„**")
    st.dataframe(stats_df, use_container_width=True, height=300)


