# LLM 호출 시간 단축 방법

## 현재 적용된 최적화

### ✅ 이미 적용됨
1. **모델 변경**: `gpt-4o-mini` → `gpt-3.5-turbo`
   - 예상 개선: 30-50% 속도 향상
2. **max_tokens 감소**:
   - 일반 질문: 500 → 350 (30% 감소)
   - 구조화 질문: 400 → 300 (25% 감소)
3. **temperature 감소**: 0.3 → 0.2
   - 예상 개선: 5-10% 속도 향상
4. **프롬프트 간소화**: ~80줄 → ~20줄
5. **Few-shot 예제 감소**: 7개 → 3개
6. **캐싱 강화**: TTL 1시간 → 24시간

## 추가 최적화 방법

### 1. max_tokens 추가 감소 ⭐⭐⭐ (최우선)

**현재 값:**
- 일반 질문: 350 tokens
- 구조화 질문: 300 tokens

**제안 값:**
- 일반 질문: 350 → **250 tokens** (28% 감소)
- 구조화 질문: 300 → **200 tokens** (33% 감소)

**예상 개선:**
- 15-25% 속도 향상
- 응답 길이 제한으로 더 간결한 답변

**구현 위치:**
- `persona/persona.py` 205-206줄 (일반 질문)
- `persona/persona.py` 495-496줄 (구조화 질문)

**주의사항:**
- 응답이 너무 짧아질 수 있음
- 테스트 후 조정 필요

---

### 2. Few-shot 예제 추가 감소 ⭐⭐

**현재 값:**
- 3개 예제

**제안 값:**
- 3개 → **1-2개** (핵심 예제만 유지)

**예상 개선:**
- 5-10% 속도 향상
- 입력 토큰 수 감소

**구현 위치:**
- `persona/persona.py` 65-107줄 (`_FEWSHOT_GENERAL`)

**제안:**
- 예제 1 (인플레이션)만 유지 (가장 대표적)
- 예제 2, 3 제거 또는 선택적 사용

---

### 3. 프롬프트 더 간소화 ⭐⭐

**현재 프롬프트:**
- 시스템 프롬프트: ~20줄
- 구조화 출력 가이드: ~10줄

**제안:**
- 핵심 지시사항만 유지
- 불필요한 설명 제거
- 예시 제거 (Few-shot으로 대체)

**예상 개선:**
- 5-10% 속도 향상
- 입력 토큰 수 감소

**구현 위치:**
- `persona/persona.py` 28-52줄 (`_system_prompt`)
- `persona/persona.py` 112-123줄 (`_STRUCTURED_OUTPUT_GUIDE`)

**제안 프롬프트 (간소화 버전):**
```python
def _system_prompt(today_kst: str) -> str:
    return (
        "너는 '알부엉'이라는 친근한 금융 튜터 AI야.\n"
        "- 반말 사용 (~해, ~야)\n"
        "- 전문 용어를 일상 언어로 번역\n"
        "- 응답 구조: 1️⃣ 이름 2️⃣ 뜻 3️⃣ 영향\n"
        "- 문장 짧게 (15-20자)\n"
        f"- 기준일: {today_kst}\n"
    )
```

---

### 4. 응답 형식 단순화 ⭐

**현재 형식:**
- JSON 구조: `definition`, `impact`, `analogy`
- 각 필드 3-4 문장

**제안:**
- JSON 구조 유지하되 필드별 길이 제한
- `analogy` 필드 선택적 (필요시만)
- 각 필드 2-3 문장으로 제한

**예상 개선:**
- 5-10% 속도 향상
- 더 간결한 답변

**구현 위치:**
- `persona/persona.py` 112-123줄 (`_STRUCTURED_OUTPUT_GUIDE`)

---

### 5. temperature 추가 감소 ⚠️ (신중히)

**현재 값:**
- 0.2

**제안 값:**
- 0.2 → **0.1** 또는 **0.0**

**예상 개선:**
- 3-5% 속도 향상
- 더 일관된 출력

**주의사항:**
- 너무 낮으면 창의성 부족
- 응답 다양성 감소
- 테스트 후 결정

---

### 6. 프롬프트에 "간결하게" 명시 ⭐

**제안:**
- 시스템 프롬프트에 "간결하게", "핵심만", "짧게" 명시
- 출력 가이드에 최대 문장 수 제한

**예상 개선:**
- 3-5% 속도 향상
- 응답 길이 자동 제한

**구현 예시:**
```python
"## 응답 규칙\n"
"- 최대 3-4 문장으로 간결하게\n"
"- 핵심만 전달, 불필요한 설명 제거\n"
"- 각 섹션 2문장 이내\n"
```

---

### 7. 캐싱 강화 ⭐⭐⭐

**현재:**
- TTL 24시간
- 메시지 해시 기반 캐싱

**제안:**
- 유사 질문 감지 (의미 기반)
- 질문 패턴 캐싱
- 자주 묻는 질문 프리로딩

**예상 개선:**
- 재요청 시 100% 속도 향상
- 캐시 히트율에 따라 평균 30-50% 개선

**구현 방법:**
- 질문 정규화 (대소문자, 띄어쓰기 등)
- 유사도 기반 캐시 검색
- 인기 질문 목록 관리

---

### 8. 조건부 LLM 호출 ⭐⭐

**제안:**
- 간단한 질문은 템플릿 응답 사용
- 인사말, 감사 인사 등은 LLM 호출 생략
- 자주 묻는 질문은 캐시된 응답 사용

**예상 개선:**
- 특정 질문 유형에서 100% 속도 향상
- 평균 10-20% 개선

**구현 예시:**
```python
# 인사말 패턴 감지
if user_input.strip() in ["안녕", "안녕하세요", "하이", "hi"]:
    return "안녕! 오늘도 신문을 품에 안고 왔어. 궁금한 경제 이야기가 있으면 편하게 물어봐!"

# 간단한 질문 템플릿
if user_input.endswith("뭐야?") and len(user_input) < 10:
    # 짧은 질문은 템플릿 응답
    return template_response(user_input)
```

---

### 9. 스트리밍 최적화 ⭐

**현재:**
- 스트리밍 수집 후 표시

**제안:**
- 첫 토큰 수신 시간 단축
- 청크 크기 최적화
- 불필요한 메타데이터 처리 제거

**예상 개선:**
- 체감 시간 10-20% 개선
- 실제 시간은 동일하지만 사용자 경험 개선

---

### 10. 모델 파라미터 최적화 ⚠️

**제안:**
- `top_p` 파라미터 추가 (0.9 권장)
- `frequency_penalty` 조정
- `presence_penalty` 조정

**예상 개선:**
- 2-5% 속도 향상
- 응답 품질 개선 가능

**주의사항:**
- 효과가 미미할 수 있음
- 테스트 필요

---

## 우선순위별 적용 계획

### 즉시 적용 (높은 효과, 낮은 위험)

1. **max_tokens 추가 감소** ⭐⭐⭐
   - 일반: 350 → 250
   - 구조화: 300 → 200
   - 예상 개선: 15-25%

2. **Few-shot 예제 감소** ⭐⭐
   - 3개 → 1-2개
   - 예상 개선: 5-10%

3. **프롬프트 간소화** ⭐⭐
   - 핵심만 유지
   - 예상 개선: 5-10%

### 단기 적용 (중간 효과, 중간 위험)

4. **응답 형식 단순화** ⭐
   - 필드별 길이 제한
   - 예상 개선: 5-10%

5. **프롬프트에 "간결하게" 명시** ⭐
   - 명시적 지시
   - 예상 개선: 3-5%

6. **조건부 LLM 호출** ⭐⭐
   - 템플릿 응답 사용
   - 예상 개선: 10-20%

### 중장기 적용 (높은 효과, 높은 복잡도)

7. **캐싱 강화** ⭐⭐⭐
   - 유사 질문 감지
   - 예상 개선: 30-50%

8. **스트리밍 최적화** ⭐
   - 체감 시간 개선
   - 예상 개선: 10-20% (체감)

### 신중히 검토 필요

9. **temperature 추가 감소** ⚠️
   - 0.2 → 0.1
   - 예상 개선: 3-5%
   - 위험: 창의성 부족

10. **모델 파라미터 최적화** ⚠️
    - 추가 파라미터 조정
    - 예상 개선: 2-5%

---

## 예상 총 개선 효과

### 즉시 적용 시
- max_tokens 감소: 15-25%
- Few-shot 감소: 5-10%
- 프롬프트 간소화: 5-10%
- **총 예상 개선: 25-45%**

### 단기 적용 시
- 추가 개선: 10-20%
- **총 예상 개선: 35-65%**

### 중장기 적용 시
- 캐싱 강화: 30-50% (평균)
- **총 예상 개선: 65-115%** (재요청 시)

---

## 구현 체크리스트

### Phase 1: 즉시 적용 (1-2일)
- [ ] max_tokens 감소 (350→250, 300→200)
- [ ] Few-shot 예제 감소 (3→1-2개)
- [ ] 프롬프트 간소화

### Phase 2: 단기 적용 (3-5일)
- [ ] 응답 형식 단순화
- [ ] 프롬프트에 "간결하게" 명시
- [ ] 조건부 LLM 호출 (인사말 등)

### Phase 3: 중장기 적용 (1-2주)
- [ ] 캐싱 강화 (유사 질문 감지)
- [ ] 스트리밍 최적화
- [ ] 성능 모니터링 및 조정

---

## 주의사항

1. **응답 품질 유지**
   - 너무 짧거나 간결하면 사용자 만족도 하락
   - 테스트 후 조정 필요

2. **점진적 적용**
   - 한 번에 모든 최적화 적용하지 말 것
   - 단계별로 적용하고 효과 측정

3. **성능 모니터링**
   - 각 최적화 후 응답 시간 측정
   - 사용자 피드백 수집

4. **롤백 계획**
   - 문제 발생 시 이전 버전으로 복구 가능하도록

---

## 참고 자료

- `persona/persona.py`: LLM 호출 로직
- `core/utils.py`: `llm_chat` 함수
- `docs_public/llm_response_generation_analysis.md`: 상세 분석

