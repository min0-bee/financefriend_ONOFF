# ì±—ë´‡ ì‘ë‹µ ì‹œê°„ ìµœì í™” ê°€ì´ë“œ

## ğŸ“Š í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### ì£¼ìš” ë³‘ëª© ì§€ì 
1. **LLM API í˜¸ì¶œ** - OpenAI API ì‘ë‹µ ëŒ€ê¸° ì‹œê°„ (ê°€ì¥ í° ë³‘ëª©)
2. **RAG ê²€ìƒ‰** - ë²¡í„° ì„ë² ë”© ë° ê²€ìƒ‰ ì—°ì‚°
3. **ìˆœì°¨ ì²˜ë¦¬** - ì—¬ëŸ¬ ì‘ì—…ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
4. **ìºì‹± ë¶€ì¡±** - ë™ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ì¬ì‚¬ìš© ë¯¸í¡

---

## ğŸš€ ìµœì í™” ë°©ë²•

### 1. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™” (ê°€ì¥ íš¨ê³¼ì )

**í˜„ì¬ ìƒíƒœ**: ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì€ êµ¬í˜„ë˜ì–´ ìˆìœ¼ë‚˜ ì‹¤ì œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ

**ê°œì„  ë°©ë²•**:
- `llm_chat()` í•¨ìˆ˜ì˜ `stream=True` ì˜µì…˜ í™œìš©
- ì‚¬ìš©ìì—ê²Œ ì¦‰ê°ì ì¸ í”¼ë“œë°± ì œê³µ (ì²« í† í°ë¶€í„° í‘œì‹œ)
- ì²´ê° ì‘ë‹µ ì‹œê°„ ëŒ€í­ ê°ì†Œ

**ì˜ˆìƒ íš¨ê³¼**: ì²´ê° ì‘ë‹µ ì‹œê°„ 50-70% ê°ì†Œ

**êµ¬í˜„ ìœ„ì¹˜**:
- `ui/components/chat_panel.py`ì˜ ì‘ë‹µ ìƒì„± ë¶€ë¶„
- `persona/persona.py`ì˜ `generate_structured_persona_reply()` í•¨ìˆ˜

---

### 2. ì‘ë‹µ ìºì‹± ê°•í™”

**í˜„ì¬ ìƒíƒœ**: ì¼ë¶€ ìºì‹±ì´ ìˆìœ¼ë‚˜ ì œí•œì 

**ê°œì„  ë°©ë²•**:

#### A. LLM ì‘ë‹µ ìºì‹±
```python
# ì„¸ì…˜ë³„ ì‘ë‹µ ìºì‹œ (ë™ì¼ ì§ˆë¬¸ ì¬ì‚¬ìš©)
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def cached_llm_response(user_input: str, term: str = None) -> str:
    # LLM í˜¸ì¶œ
    pass
```

#### B. RAG ê²€ìƒ‰ ê²°ê³¼ ìºì‹±
- ì´ë¯¸ `rag_embedding_cache`ê°€ ìˆìœ¼ë‚˜ í™•ì¥ í•„ìš”
- ê²€ìƒ‰ ê²°ê³¼ ìì²´ë„ ìºì‹± (ë©”íƒ€ë°ì´í„° + ê±°ë¦¬ ì •ë³´)

**ì˜ˆìƒ íš¨ê³¼**: ë™ì¼ ì§ˆë¬¸ ì¬ìš”ì²­ ì‹œ 90% ì´ìƒ ì‹œê°„ ë‹¨ì¶•

---

### 3. ë³‘ë ¬ ì²˜ë¦¬ ë„ì…

**í˜„ì¬ ìƒíƒœ**: ëª¨ë“  ì‘ì—…ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰

**ê°œì„  ë°©ë²•**:

#### A. RAG ê²€ìƒ‰ê³¼ LLM í˜¸ì¶œ ë³‘ë ¬í™”
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

# RAG ê²€ìƒ‰ê³¼ ì¼ë°˜ LLM ì‘ë‹µì„ ë™ì‹œì— ì¤€ë¹„
with ThreadPoolExecutor(max_workers=2) as executor:
    rag_future = executor.submit(search_terms_by_rag, user_input)
    general_future = executor.submit(albwoong_persona_reply, user_input)
    
    # ë¨¼ì € ì™„ë£Œëœ ê²°ê³¼ ì‚¬ìš©
    rag_results = rag_future.result(timeout=1.0)
    if rag_results:
        # RAG ê²°ê³¼ ì‚¬ìš©
    else:
        # ì¼ë°˜ ì‘ë‹µ ì‚¬ìš©
```

#### B. ë²¡í„° ê²€ìƒ‰ ìµœì í™”
- `top_k` ê°’ì„ ì¤„ì—¬ì„œ ê²€ìƒ‰ ì†ë„ í–¥ìƒ (í˜„ì¬ 3 â†’ 1ë¡œ ì¤„ì´ê¸°)
- ê±°ë¦¬ ì„ê³„ê°’ì„ ë¨¼ì € ì²´í¬í•˜ì—¬ ë¶ˆí•„ìš”í•œ LLM í˜¸ì¶œ ë°©ì§€

**ì˜ˆìƒ íš¨ê³¼**: ì „ì²´ ì‘ë‹µ ì‹œê°„ 20-30% ê°ì†Œ

---

### 4. ëª¨ë¸ ë° íŒŒë¼ë¯¸í„° ìµœì í™”

**ê°œì„  ë°©ë²•**:

#### A. ë” ë¹ ë¥¸ ëª¨ë¸ ì‚¬ìš©
- `gpt-4o-mini` â†’ `gpt-3.5-turbo` (ë” ë¹ ë¥´ê³  ì €ë ´)
- ë˜ëŠ” `gpt-4o-mini` ìœ ì§€í•˜ë˜ `max_tokens` ê°ì†Œ

#### B. Temperature ë° max_tokens ì¡°ì •
```python
# í˜„ì¬: temperature=0.3, max_tokens=600
# ê°œì„ : temperature=0.2, max_tokens=400
# â†’ ë” ë¹ ë¥¸ ìƒì„± + ì¼ê´€ëœ ì‘ë‹µ
```

#### C. Few-shot ì˜ˆì œ ì¶•ì†Œ
- `_FEWSHOT_GENERAL` ë¦¬ìŠ¤íŠ¸ë¥¼ 3-5ê°œë¡œ ì¤„ì´ê¸°
- í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ê°ì†Œë¡œ í† í° ìˆ˜ ê°ì†Œ

**ì˜ˆìƒ íš¨ê³¼**: API ì‘ë‹µ ì‹œê°„ 15-25% ê°ì†Œ

---

### 5. RAG ê²€ìƒ‰ ìµœì í™”

**í˜„ì¬ ìƒíƒœ**: 
- ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹œê°„ (ì²« ì‹¤í–‰ ì‹œ 10-20ì´ˆ)
- ë²¡í„° ê²€ìƒ‰ ì‹œê°„

**ê°œì„  ë°©ë²•**:

#### A. ì„ë² ë”© ëª¨ë¸ ì‚¬ì „ ë¡œë“œ
- ì•± ì‹œì‘ ì‹œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ëª¨ë¸ ë¡œë“œ
- `initialize_rag_system_background()` í™œìš©

#### B. ê²€ìƒ‰ ì „ëµ ê°œì„ 
```python
# 1ë‹¨ê³„: ì •í™• ë§¤ì¹­ (ë¹ ë¦„, 0ms)
# 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ (ëŠë¦¼, 100-200ms)
# â†’ ì •í™• ë§¤ì¹­ ì‹¤íŒ¨ ì‹œì—ë§Œ ë²¡í„° ê²€ìƒ‰
```

#### C. ì„ë² ë”© ìºì‹± í™•ëŒ€
- ì§ˆë¬¸ë³„ ì„ë² ë”© ìºì‹± (ì´ë¯¸ êµ¬í˜„ë¨)
- ìœ ì‚¬ ì§ˆë¬¸ ê°ì§€í•˜ì—¬ ìºì‹œ ì¬ì‚¬ìš©

**ì˜ˆìƒ íš¨ê³¼**: RAG ê²€ìƒ‰ ì‹œê°„ 30-50% ê°ì†Œ

---

### 6. í”„ë¡¬í”„íŠ¸ ìµœì í™”

**ê°œì„  ë°©ë²•**:

#### A. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°„ì†Œí™”
- `_system_prompt()` í•¨ìˆ˜ì˜ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì¶•ì†Œ
- í•µì‹¬ ì§€ì‹œì‚¬í•­ë§Œ ìœ ì§€

#### B. êµ¬ì¡°í™”ëœ ì¶œë ¥ í˜•ì‹ ë‹¨ìˆœí™”
- JSON íŒŒì‹± ëŒ€ì‹  ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ í˜•ì‹ ì‚¬ìš©
- ë˜ëŠ” JSON ìŠ¤í‚¤ë§ˆë¥¼ ë” ê°„ë‹¨í•˜ê²Œ

**ì˜ˆìƒ íš¨ê³¼**: API í˜¸ì¶œ ì‹œê°„ 10-15% ê°ì†Œ

---

### 7. ì¡°ê±´ë¶€ ì²˜ë¦¬ ìµœì í™”

**í˜„ì¬ ìƒíƒœ**: 
- ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì¡°ê±´ ì²´í¬ê°€ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰
- ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ë„ ìˆ˜í–‰

**ê°œì„  ë°©ë²•**:

#### A. ì¡°ê¸° ì¢…ë£Œ (Early Exit)
```python
# ê¸ˆìœµ ìš©ì–´ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ ì¦‰ì‹œ ì¼ë°˜ ì‘ë‹µ
if not has_financial_keyword and not is_definition_question:
    # RAG ê²€ìƒ‰ ìƒëµí•˜ê³  ë°”ë¡œ ì¼ë°˜ ì‘ë‹µ
    return albwoong_persona_reply(user_input)
```

#### B. ê²€ìƒ‰ ë²”ìœ„ ì¶•ì†Œ
- ê¸ˆìœµ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ RAG ê²€ìƒ‰ ìƒëµ
- ì§ˆë¬¸ íŒ¨í„´ì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ ì¦‰ì‹œ ì¼ë°˜ ì‘ë‹µ

**ì˜ˆìƒ íš¨ê³¼**: ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ì œê±°ë¡œ 100-200ms ì ˆì•½

---

### 8. UI/UX ê°œì„  (ì²´ê° ì†ë„ í–¥ìƒ)

**ê°œì„  ë°©ë²•**:

#### A. ë¡œë”© ì¸ë””ì¼€ì´í„° ê°œì„ 
- ìŠ¤í”¼ë„ˆ ëŒ€ì‹  ì§„í–‰ë¥  í‘œì‹œ
- "ë‹µë³€ ìƒì„± ì¤‘..." â†’ "ë‹µë³€ì„ ì¤€ë¹„í•˜ê³  ìˆì–´ìš”..."

#### B. ë¶€ë¶„ ì‘ë‹µ í‘œì‹œ
- ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²« ë¬¸ì¥ë¶€í„° í‘œì‹œ
- ì‚¬ìš©ìê°€ ê¸°ë‹¤ë¦¬ëŠ” ëŠë‚Œ ê°ì†Œ

**ì˜ˆìƒ íš¨ê³¼**: ì²´ê° ì‘ë‹µ ì‹œê°„ 40-60% ê°ì†Œ

---

## ğŸ“ˆ ìš°ì„ ìˆœìœ„ë³„ êµ¬í˜„ ê°€ì´ë“œ

### ğŸ”¥ ìµœìš°ì„  (ì¦‰ì‹œ íš¨ê³¼)
1. **ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”** - ê°€ì¥ í° íš¨ê³¼
2. **ì‘ë‹µ ìºì‹± ê°•í™”** - ë™ì¼ ì§ˆë¬¸ ì¬ìš”ì²­ ì‹œ ì¦‰ì‹œ ì‘ë‹µ
3. **ì¡°ê¸° ì¢…ë£Œ ë¡œì§** - ë¶ˆí•„ìš”í•œ ê²€ìƒ‰ ì œê±°

### âš¡ ë†’ì€ ìš°ì„ ìˆœìœ„ (ë‹¨ê¸°)
4. **ë³‘ë ¬ ì²˜ë¦¬ ë„ì…** - RAGì™€ LLM ë³‘ë ¬ ì‹¤í–‰
5. **ëª¨ë¸ íŒŒë¼ë¯¸í„° ìµœì í™”** - max_tokens ê°ì†Œ
6. **RAG ê²€ìƒ‰ ìµœì í™”** - top_k ê°ì†Œ, ì„ê³„ê°’ ì¡°ì •

### ğŸ“Š ì¤‘ê°„ ìš°ì„ ìˆœìœ„ (ì¤‘ê¸°)
7. **í”„ë¡¬í”„íŠ¸ ìµœì í™”** - í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì¶•ì†Œ
8. **Few-shot ì˜ˆì œ ì¶•ì†Œ** - ì˜ˆì œ ê°œìˆ˜ ê°ì†Œ

### ğŸ¨ ë‚®ì€ ìš°ì„ ìˆœìœ„ (ì¥ê¸°)
9. **UI/UX ê°œì„ ** - ë¡œë”© ì¸ë””ì¼€ì´í„° ê°œì„ 
10. **ëª¨ë‹ˆí„°ë§ ì¶”ê°€** - ì‘ë‹µ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…

---

## ğŸ’¡ êµ¬í˜„ ì˜ˆì‹œ

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ êµ¬í˜„ ì˜ˆì‹œ

```python
# ui/components/chat_panel.py ìˆ˜ì •
if user_input:
    # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì‘ë‹µ ìƒì„±
    if is_term_question:
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        response_placeholder = st.empty()
        full_response = ""
        
        for chunk in generate_structured_persona_reply_stream(
            user_input=user_input,
            term=extracted_term,
            stream=True
        ):
            full_response += chunk
            response_placeholder.markdown(full_response + "â–Œ")
        
        # ìµœì¢… ì‘ë‹µ ì €ì¥
        st.session_state.chat_history.append({
            "role": "assistant",
            "content": full_response
        })
```

### ì‘ë‹µ ìºì‹± êµ¬í˜„ ì˜ˆì‹œ

```python
# persona/persona.py ìˆ˜ì •
@st.cache_data(ttl=3600, show_spinner=False)
def cached_persona_reply(
    user_input: str,
    term: str = None,
    context: dict = None
) -> str:
    """ìºì‹œëœ í˜ë¥´ì†Œë‚˜ ì‘ë‹µ"""
    return generate_structured_persona_reply(
        user_input=user_input,
        term=term,
        context=context
    )
```

---

## ğŸ“Š ì˜ˆìƒ ì„±ëŠ¥ ê°œì„ 

| ìµœì í™” ë°©ë²• | í˜„ì¬ ì‹œê°„ | ê°œì„  í›„ | ê°œì„ ìœ¨ |
|------------|----------|---------|--------|
| ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™” | 2-3ì´ˆ | 0.5-1ì´ˆ (ì²« í† í°) | 70% |
| ì‘ë‹µ ìºì‹± | 2-3ì´ˆ | 0.05ì´ˆ (ìºì‹œ íˆíŠ¸) | 98% |
| ë³‘ë ¬ ì²˜ë¦¬ | 2-3ì´ˆ | 1.5-2ì´ˆ | 25% |
| ëª¨ë¸ ìµœì í™” | 2-3ì´ˆ | 1.5-2.5ì´ˆ | 20% |
| ì¡°ê¸° ì¢…ë£Œ | 2-3ì´ˆ | 1.5-2ì´ˆ | 25% |

**ì¢…í•© ì˜ˆìƒ**: ì „ì²´ ì‘ë‹µ ì‹œê°„ **50-70% ê°ì†Œ**

---

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ì¸¡ì •

### ì‘ë‹µ ì‹œê°„ ì¸¡ì •
```python
# ê° ë‹¨ê³„ë³„ ì‹œê°„ ì¸¡ì •
import time

t0 = time.perf_counter()
# RAG ê²€ìƒ‰
rag_time = time.perf_counter() - t0

t1 = time.perf_counter()
# LLM í˜¸ì¶œ
llm_time = time.perf_counter() - t1

# ë¡œê¹…
log_event("response_timing", 
    rag_ms=rag_time*1000,
    llm_ms=llm_time*1000,
    total_ms=(time.perf_counter()-t0)*1000
)
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ í™œì„±í™”
- [ ] ì‘ë‹µ ìºì‹± êµ¬í˜„
- [ ] ë³‘ë ¬ ì²˜ë¦¬ ë„ì…
- [ ] ëª¨ë¸ íŒŒë¼ë¯¸í„° ìµœì í™”
- [ ] RAG ê²€ìƒ‰ ìµœì í™”
- [ ] ì¡°ê¸° ì¢…ë£Œ ë¡œì§ ì¶”ê°€
- [ ] í”„ë¡¬í”„íŠ¸ ìµœì í™”
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¶”ê°€

---

## ğŸ“š ì°¸ê³  ìë£Œ

- OpenAI Streaming API: https://platform.openai.com/docs/api-reference/streaming
- Streamlit Caching: https://docs.streamlit.io/library/advanced-features/caching
- ChromaDB Performance: https://docs.trychroma.com/guides/performance

