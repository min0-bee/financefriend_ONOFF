"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ëª¨ë“ˆ (RAG ì‹œìŠ¤í…œ í†µí•©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import re
import streamlit as st
import pickle
import hashlib
import json
import gzip
import os
import time
import threading
import pandas as pd
from typing import Dict, List, Optional
from persona.persona import (
    albwoong_persona_reply,
    generate_structured_persona_reply,
)
from core.logger import get_supabase_client
from core.config import SUPABASE_ENABLE
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import pickle
import hashlib
import json
import gzip
from core.logger import get_supabase_client
from core.config import SUPABASE_ENABLE

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ ì „ì—­ ìºì‹œ: ì„ë² ë”© ëª¨ë¸ (ì„¸ì…˜ ê°„ ì¬ì‚¬ìš©)
# - SentenceTransformer ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ì „ì—­ìœ¼ë¡œ ìºì‹œ
# - ëª¨ë“  ì„¸ì…˜ì—ì„œ ë™ì¼í•œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_embedding_model_cache = None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ ì „ì—­ ìºì‹œ: ì„ë² ë”© ëª¨ë¸ (ì„¸ì…˜ ê°„ ì¬ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_embedding_model_cache = None
_RAG_AVAILABLE = chromadb is not None and SentenceTransformer is not None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ê¸°ë³¸ ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ (RAG/ì‚¬ì „ ì—†ì´ë„ ë™ì‘í•˜ëŠ” ìµœì†Œ ì„¸íŠ¸)
# - ê° ìš©ì–´ëŠ” 'ì •ì˜', 'ì„¤ëª…', 'ë¹„ìœ 'ë¡œ êµ¬ì„±
# - ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” DB/CSV/RAGë¡œ ëŒ€ì²´ ê°€ëŠ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_TERMS = {
    "ì–‘ì ì™„í™”": {
        "ì •ì˜": "ì¤‘ì•™ì€í–‰ì´ ì‹œì¤‘ì— í†µí™”ë¥¼ ê³µê¸‰í•˜ê¸° ìœ„í•´ êµ­ì±„ ë“±ì„ ë§¤ì…í•˜ëŠ” ì •ì±…",
        "ì„¤ëª…": "ê²½ê¸° ë¶€ì–‘ì„ ìœ„í•´ ì¤‘ì•™ì€í–‰ì´ ëˆì„ í’€ì–´ ì‹œì¥ ìœ ë™ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "ë§ˆë¥¸ ë•…ì— ë¬¼ì„ ë¿Œë ¤ì£¼ëŠ” ê²ƒì²˜ëŸ¼, ê²½ì œì— ëˆì´ë¼ëŠ” ë¬¼ì„ ê³µê¸‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.",
    },
    "ê¸°ì¤€ê¸ˆë¦¬": {
        "ì •ì˜": "ì¤‘ì•™ì€í–‰ì´ ì‹œì¤‘ì€í–‰ì— ëˆì„ ë¹Œë ¤ì¤„ ë•Œ ì ìš©í•˜ëŠ” ê¸°ì¤€ì´ ë˜ëŠ” ê¸ˆë¦¬",
        "ì„¤ëª…": "ëª¨ë“  ê¸ˆë¦¬ì˜ ê¸°ì¤€ì´ ë˜ë©°, ê¸°ì¤€ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ëŒ€ì¶œì´ìë„ í•¨ê»˜ ì˜¤ë¦…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "ë¬¼ê°€ì˜ ì˜¨ë„ì¡°ì ˆê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ê²½ì œê°€ ê³¼ì—´ë˜ë©´ ì˜¬ë¦¬ê³ , ì¹¨ì²´ë˜ë©´ ë‚´ë¦½ë‹ˆë‹¤.",
    },
    "ë°°ë‹¹": {
        "ì •ì˜": "ê¸°ì—…ì´ ë²Œì–´ë“¤ì¸ ì´ìµ ì¤‘ ì¼ë¶€ë¥¼ ì£¼ì£¼ë“¤ì—ê²Œ ë‚˜ëˆ ì£¼ëŠ” ê²ƒ",
        "ì„¤ëª…": "ì£¼ì‹ì„ ë³´ìœ í•œ ì£¼ì£¼ì—ê²Œ ê¸°ì—…ì˜ ì´ìµì„ ë¶„ë°°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "í•¨ê»˜ ì‹ë‹¹ì„ ìš´ì˜í•˜ëŠ” ë™ì—…ìë“¤ì´ ë§¤ì¶œ ì¤‘ ì¼ë¶€ë¥¼ ë‚˜ëˆ ê°–ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.",
    },
    "PER": {
        "ì •ì˜": "ì£¼ê°€ìˆ˜ìµë¹„ìœ¨. ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœì´ìµìœ¼ë¡œ ë‚˜ëˆˆ ê°’",
        "ì„¤ëª…": "ì£¼ì‹ì´ 1ë…„ ì¹˜ ì´ìµì˜ ëª‡ ë°°ì— ê±°ë˜ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì €í‰ê°€ëœ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ë¹„ìœ ": "1ë…„ì— 100ë§Œì› ë²„ëŠ” ê°€ê²Œë¥¼ ëª‡ ë…„ ì¹˜ ìˆ˜ìµì„ ì£¼ê³  ì‚¬ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
    },
    "í™˜ìœ¨": {
        "ì •ì˜": "ì„œë¡œ ë‹¤ë¥¸ ë‘ ë‚˜ë¼ í™”íì˜ êµí™˜ ë¹„ìœ¨",
        "ì„¤ëª…": "ì›í™”ë¥¼ ë‹¬ëŸ¬ë¡œ, ë‹¬ëŸ¬ë¥¼ ì›í™”ë¡œ ë°”ê¿€ ë•Œ ì ìš©ë˜ëŠ” ë¹„ìœ¨ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "í•´ì™¸ ì‡¼í•‘ëª°ì—ì„œ ë¬¼ê±´ì„ ì‚´ ë•Œ ì ìš©ë˜ëŠ” í™˜ì „ ë¹„ìœ¨ì…ë‹ˆë‹¤.",
    },
}


def _cache_rag_metadata(metadatas: List[Dict]):
    """
    RAG ë©”íƒ€ë°ì´í„°ë¥¼ ì„¸ì…˜ì— ìºì‹±í•˜ì—¬ ë°˜ë³µì ì¸ collection.get() í˜¸ì¶œì„ ì¤„ì…ë‹ˆë‹¤.
    term / synonym ëª¨ë‘ ì†Œë¬¸ìë¡œ í‚¤ë¥¼ ë§Œë“¤ì–´ lookup ì†ë„ë¥¼ ë†’ì´ê³ ,
    í•˜ì´ë¼ì´íŠ¸ìš© ìš©ì–´ ì„¸íŠ¸ë„ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
    """
    metadata_map: Dict[str, Dict] = {}
    highlight_terms = set()

    for meta in metadatas:
        term = (meta.get("term") or "").strip()
        if term:
            metadata_map[term.lower()] = meta
            highlight_terms.add(term)

        synonym_field = (meta.get("synonym") or "").strip()
        if synonym_field:
            for raw in re.split(r"[,\n]", synonym_field):
                synonym = raw.strip()
                if synonym:
                    metadata_map[synonym.lower()] = meta
                    highlight_terms.add(synonym)

    st.session_state["rag_metadata_by_term"] = metadata_map
    st.session_state["rag_terms_for_highlight"] = highlight_terms


def _perf_enabled() -> bool:
    return st.session_state.get("rag_perf_enable", True)


def _perf_step(perf_enabled: bool, steps: List[Dict], label: str, start_time: float) -> float:
    if not perf_enabled:
        return time.perf_counter()
    elapsed_ms = (time.perf_counter() - start_time) * 1000
    steps.append({"step": label, "ms": round(elapsed_ms, 2)})
    return time.perf_counter()


def _record_perf(section: str, steps: List[Dict]):
    if not steps:
        return
    logs = st.session_state.setdefault("rag_perf_logs", {})
    history = logs.setdefault(section, [])
    history.append({
        "timestamp": time.strftime("%H:%M:%S", time.localtime()),
        "steps": steps,
    })
    logs[section] = history[-10:]
    st.session_state[f"rag_last_{section}_perf"] = steps


def _sync_supabase_async(documents, embeddings, metadatas, ids, checksum):
    if not SUPABASE_ENABLE:
        return
    if st.session_state.get("rag_cache_synced") or st.session_state.get("rag_cache_sync_in_progress"):
        return

    def _worker():
        try:
            if _save_embeddings_to_supabase(documents, embeddings, metadatas, ids, checksum):
                st.session_state["rag_cache_synced"] = True
        except Exception as e:
            st.session_state["rag_cache_sync_error"] = str(e)
        finally:
            st.session_state["rag_cache_sync_in_progress"] = False

    st.session_state["rag_cache_sync_in_progress"] = True
    threading.Thread(target=_worker, daemon=True).start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§° ì„¸ì…˜ì— ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë³´ì¥ (RAG í†µí•© ë²„ì „)
#   - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: DEFAULT_TERMSë§Œ ë³µì‚¬
#   2. ì‹ ê·œ: RAG ì‹œìŠ¤í…œ ìë™ ì´ˆê¸°í™” ì¶”ê°€
#   3. Fallback: RAG ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ DEFAULT_TERMS ì‚¬ìš©
# - Streamlitì€ ì‚¬ìš©ìë³„ ì„¸ì…˜ ìƒíƒœ(st.session_state)ë¥¼ ì œê³µ
# - ìµœì´ˆ 1íšŒë§Œ DEFAULT_TERMSë¥¼ ë³µì‚¬í•´ ë„£ì–´ ì¤‘ê°„ ë³€ê²½ì—ë„ ì›ë³¸ ë³´ì¡´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_financial_terms():
    """
    ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ì´ˆê¸°í™” ë° RAG ì‹œìŠ¤í…œ ìë™ ì‹œì‘
    - ì„¸ì…˜ ìµœì´ˆ ì‹¤í–‰ ì‹œ RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”
    - Fallbackìœ¼ë¡œ ê¸°ë³¸ ìš©ì–´ ì‚¬ì „ë„ ìœ ì§€
    """
    # 1ï¸âƒ£ ê¸°ë³¸ ìš©ì–´ ì‚¬ì „ ì´ˆê¸°í™” (Fallbackìš©)
    if "financial_terms" not in st.session_state:
        st.session_state.financial_terms = DEFAULT_TERMS.copy()

    # 2ï¸âƒ£ RAG ì‹œìŠ¤í…œ ìë™ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
    if "rag_initialized" not in st.session_state:
        if not _RAG_AVAILABLE:
            st.session_state.rag_initialized = False
            st.warning("âš ï¸ ê³ ê¸‰ ìš©ì–´ ê²€ìƒ‰ ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            initialize_rag_system()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”´ ê¸°ì¡´ í•¨ìˆ˜ (ì£¼ì„ì²˜ë¦¬): í•˜ë“œì½”ë”©ëœ ì‚¬ì „ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def highlight_terms(text: str) -> str:
#     highlighted = text
#
#     # í˜„ì¬ ì„¸ì…˜ì˜ ìš©ì–´ ì‚¬ì „ì—ì„œ í‚¤(ìš©ì–´)ë§Œ ìˆœíšŒ
#     for term in st.session_state.financial_terms.keys():
#         # re.escape(term): íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ìš©ì–´ë„ ì•ˆì „í•˜ê²Œ ë§¤ì¹­
#         # re.IGNORECASE: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰ (ì˜ë¬¸ ìš©ì–´ ëŒ€ë¹„)
#         pattern = re.compile(re.escape(term), re.IGNORECASE)
#
#         # âš ï¸ ì£¼ì˜: ì•„ë˜ ëŒ€ì²´ ë¬¸ìì—´ì˜ {term}ì€ 'ì‚¬ì „ í‚¤' í‘œê¸°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
#         # - ë§¤ì¹­ëœ ì›ë˜ í‘œê¸°(ëŒ€ì†Œë¬¸ì)ë¥¼ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ repl í•¨ìˆ˜ ì‚¬ìš© í•„ìš”
#         #   ì˜ˆ) pattern.sub(lambda m: f"...>{m.group(0)}</mark>", highlighted)
#         highlighted = pattern.sub(
#             f'<mark class="clickable-term" data-term="{term}" '
#             f'style="background-color: #FFEB3B; cursor: pointer; padding: 2px 4px; border-radius: 3px;">{term}</mark>',
#             highlighted,
#         )
#     return highlighted


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ¨ ë³¸ë¬¸ì—ì„œ ê¸ˆìœµ ìš©ì–´ í•˜ì´ë¼ì´íŠ¸ (RAG í†µí•© ë²„ì „)
# - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: st.session_state.financial_terms ì‚¬ì „ì—ì„œë§Œ ê²€ìƒ‰
#   2. ì‹ ê·œ: RAGì— ì €ì¥ëœ ëª¨ë“  ìš©ì–´ë¥¼ í•˜ì´ë¼ì´íŠ¸ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©
#   3. Fallback: RAG ë¯¸ì´ˆê¸°í™” ì‹œ ê¸°ì¡´ ì‚¬ì „ ì‚¬ìš©
# - ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ë¥¼ ì°¾ì•„ <mark> íƒœê·¸ë¡œ ê°ì‹¸ ê°•ì¡°
# - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ(re.IGNORECASE) â†’ ì˜ë¬¸ ì•½ì–´ ë“±ì—ë„ ëŒ€ì‘
# - data-term ì†ì„±: ì¶”í›„ JS/ì´ë²¤íŠ¸ ì—°ê²° ì‹œ ì–´ë–¤ ìš©ì–´ì¸ì§€ ì‹ë³„ ìš©ì´
# - Streamlit ì¶œë ¥ ì‹œ st.markdown(..., unsafe_allow_html=True) í•„ìš”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def highlight_terms(text: str) -> str:
    """
    ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ê¸ˆìœµ ìš©ì–´ë¥¼ ì°¾ì•„ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸(ê¸°ì‚¬ ë³¸ë¬¸ ë“±)

    Returns:
        ê¸ˆìœµ ìš©ì–´ê°€ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ëœ HTML ë¬¸ìì—´
    """
    highlighted = text
    terms_to_highlight = set()

    cached_terms = st.session_state.get("rag_terms_for_highlight")
    if cached_terms:
        terms_to_highlight = set(cached_terms)
    elif st.session_state.get("rag_initialized", False):
        try:
            collection = st.session_state.get("rag_collection")
            if collection is None:
                raise ValueError("RAG ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
            all_data = collection.get()
            if all_data and all_data['metadatas']:
                _cache_rag_metadata(all_data['metadatas'])
                terms_to_highlight = set(st.session_state.get("rag_terms_for_highlight", []))
        except Exception as e:
            st.warning(f"âš ï¸ RAG ìš©ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")
            terms_to_highlight = set(st.session_state.get("financial_terms", DEFAULT_TERMS).keys())
    else:
        terms_to_highlight = set(st.session_state.get("financial_terms", DEFAULT_TERMS).keys())

    # 3ï¸âƒ£ ìš©ì–´ë³„ë¡œ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬
    # ê¸´ ìš©ì–´ë¶€í„° ì²˜ë¦¬í•˜ì—¬ ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€ (ì˜ˆ: "ë¶€ê°€ê°€ì¹˜ì„¸"ê°€ "ë¶€ê°€ê°€ì¹˜"ë³´ë‹¤ ë¨¼ì € ì²˜ë¦¬)
    sorted_terms = sorted(terms_to_highlight, key=len, reverse=True)

    # ì´ë¯¸ í•˜ì´ë¼ì´íŠ¸ëœ ë¶€ë¶„ì„ ë³´í˜¸í•˜ê¸° ìœ„í•œ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë” ë§µ
    placeholders = {}
    placeholder_counter = 0

    for term in sorted_terms:
        if not term:  # ë¹ˆ ë¬¸ìì—´ ìŠ¤í‚µ
            continue

        # í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ë§¤ì¹­í•˜ë„ë¡ íŒ¨í„´ ìƒì„±
        # __PLACEHOLDER_ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì€ ì œì™¸
        escaped_term = re.escape(term)

        # ë§¤ì¹­ëœ ì›ë˜ í‘œê¸°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•˜ì´ë¼ì´íŠ¸
        matches = []
        pattern = re.compile(escaped_term, re.IGNORECASE)

        for match in pattern.finditer(highlighted):
            # ë§¤ì¹­ëœ ìœ„ì¹˜ê°€ í”Œë ˆì´ìŠ¤í™€ë” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            start_pos = match.start()
            # ë§¤ì¹­ ìœ„ì¹˜ ì´ì „ì— í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆê³  ì•„ì§ ë‹«íˆì§€ ì•Šì•˜ëŠ”ì§€ ì²´í¬
            prefix = highlighted[:start_pos]
            # í”Œë ˆì´ìŠ¤í™€ë” ì•ˆì— ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì €ì¥
            if '__PLACEHOLDER_' not in highlighted[max(0, start_pos-20):start_pos]:
                matches.append(match)

        # ë’¤ì—ì„œë¶€í„° ì¹˜í™˜ (ì¸ë±ìŠ¤ ë³€ê²½ ë°©ì§€)
        for match in reversed(matches):
            matched_text = match.group(0)
            # HTML íƒœê·¸ ìƒì„± (Streamlitì€ í´ë¦­ ì´ë²¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‹œê°ì  í‘œì‹œë§Œ)
            placeholder = f"__PLACEHOLDER_{placeholder_counter}__"
            mark_html = (
                f'<mark class="financial-term" '
                f'style="background-color: #FFEB3B; padding: 2px 4px; border-radius: 3px;">'
                f'{matched_text}</mark>'
            )
            placeholders[placeholder] = mark_html
            placeholder_counter += 1

            # í…ìŠ¤íŠ¸ ì¹˜í™˜
            highlighted = highlighted[:match.start()] + placeholder + highlighted[match.end():]

    # ëª¨ë“  í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ HTMLë¡œ ë³µì›
    for placeholder, mark_html in placeholders.items():
        highlighted = highlighted.replace(placeholder, mark_html)

    return highlighted

def _build_structured_context_from_metadata(
    base_term: str,
    metadata: Dict[str, str],
    question_term: Optional[str] = None,
    synonym_matched: bool = False,
) -> Dict[str, str]:
    context: Dict[str, str] = {}

    def _add(key: str, value: Optional[str]):
        if value:
            value = str(value).strip()
            if value:
                context[key] = value

    _add("definition", metadata.get("definition"))
    _add("analogy", metadata.get("analogy"))
    _add("importance", metadata.get("importance"))
    _add("correction", metadata.get("correction"))
    _add("example", metadata.get("example"))
    _add("synonym", metadata.get("synonym"))

    if question_term and question_term.lower() != base_term.lower():
        label = "question_term_synonym" if synonym_matched else "question_term"
        _add(label, question_term)

    context["term"] = base_term
    context["source"] = "RAG"
    return context


def _build_structured_context_from_default(term: str, info: Dict[str, str]) -> Dict[str, str]:
    context: Dict[str, str] = {}

    mapping = {
        "definition": info.get("ì •ì˜"),
        "detail": info.get("ì„¤ëª…"),
        "analogy": info.get("ë¹„ìœ "),
    }

    for key, value in mapping.items():
        if value:
            value = str(value).strip()
            if value:
                context[key] = value

    context["term"] = term
    context["source"] = "DEFAULT_DICTIONARY"
    return context


def _generate_structured_term_response(
    base_term: str,
    context: Dict[str, str],
    question_term: Optional[str] = None,
    temperature: float = 0.25,
) -> str:
    question_text = question_term or base_term
    user_prompt = f"{question_text}ê°€ ë­ì•¼?"
    response = generate_structured_persona_reply(
        user_input=user_prompt,
        term=base_term,
        context=context,
        temperature=temperature,
    )
    if response and "(LLM ì—°ê²° ì˜¤ë¥˜" not in response:
        return response

    # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì •ë³´ë¼ë„ ì œê³µ
    parts: List[str] = [f"ğŸ¤– **{base_term}** ì— ëŒ€í•´ ì„¤ëª…í•´ì¤„ê²Œ! ğŸ¯"]
    if context.get("definition"):
        parts.append(f"ğŸ“– ì •ì˜: {context['definition']}")
    if context.get("detail"):
        parts.append(f"ğŸ’¡ ì„¤ëª…: {context['detail']}")
    if context.get("importance"):
        parts.append(f"â— ì™œ ì¤‘ìš”í•´?: {context['importance']}")
    if context.get("analogy"):
        parts.append(f"ğŸŒŸ ë¹„ìœ : {context['analogy']}")
    if context.get("example"):
        parts.append(f"ğŸ“° ì˜ˆì‹œ: {context['example']}")
    parts.append("ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!")
    return "\n".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“ CSV íŒŒì¼ì—ì„œ ê¸ˆìœµìš©ì–´ ë¡œë“œ
# - rag/glossary/ê¸ˆìœµìš©ì–´.csv íŒŒì¼ì„ pandasë¡œ ì½ì–´ì˜´
# - ì»¬ëŸ¼: ë²ˆí˜¸, ê¸ˆìœµìš©ì–´, ì •ì˜, ë¹„ìœ , ì™œ ì¤‘ìš”?, ì˜¤í•´ êµì •, ì˜ˆì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_glossary_from_csv() -> pd.DataFrame:
    """ê¸ˆìœµìš©ì–´.csv íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    csv_path = os.path.join(os.path.dirname(__file__), "glossary", "ê¸ˆìœµìš©ì–´.csv")

    if not os.path.exists(csv_path):
        st.warning(f"âš ï¸ ê¸ˆìœµìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return pd.DataFrame()

    try:
        df = pd.read_csv(csv_path, encoding="utf-8")
        # ê²°ì¸¡ì¹˜ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        df = df.fillna("")
        return df
    except Exception as e:
        st.error(f"âŒ CSV ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” CSV íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _calculate_csv_checksum(csv_path: str) -> str:
    """CSV íŒŒì¼ì˜ ì²´í¬ì„¬ì„ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
    try:
        with open(csv_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return file_hash
    except Exception:
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹œ ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ì„ ì „ì—­ ìºì‹œì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ë¡œë“œ"""
    global _embedding_model_cache
    
    if _embedding_model_cache is None:
        _embedding_model_cache = SentenceTransformer('jhgan/ko-sroberta-multitask')
    
    return _embedding_model_cache


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¾ ì„ë² ë”© ë²¡í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_cache_dir():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    cache_dir = os.path.join(os.path.dirname(__file__), "glossary", ".cache")
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir


def _get_embeddings_cache_path():
    """ì„ë² ë”© ë²¡í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ"""

    return os.path.join(_get_cache_dir(), "embeddings.pkl")


def _get_metadata_cache_path():
    """ë©”íƒ€ë°ì´í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
    return os.path.join(_get_cache_dir(), "metadata.pkl")


def _get_checksum_cache_path():
    """ì²´í¬ì„¬ ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
    return os.path.join(_get_cache_dir(), "checksum.json")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¾ ì„ë² ë”© ë²¡í„° ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_embeddings_cache(documents: List[str], embeddings, metadatas: List[Dict], ids: List[str], checksum: str):
    """ì„ë² ë”© ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥ (ë¡œì»¬ì€ ì••ì¶• ì—†ìŒ, ë¹ ë¥¸ ë¡œë“œ)"""
    try:
        cache_dir = _get_cache_dir()
        
        # ì„ë² ë”© ë²¡í„° ì €ì¥ (ì••ì¶• ì—†ìŒ - ë¹ ë¥¸ ë¡œë“œ)
        cache_data = {
            'documents': documents,
            'embeddings': embeddings,
            'metadatas': metadatas,
            'ids': ids
        }
        
        with open(_get_embeddings_cache_path(), 'wb') as f:
            pickle.dump({
                'documents': documents,
                'embeddings': embeddings,
                'metadatas': metadatas,
                'ids': ids
            }, f)

        
        # ì²´í¬ì„¬ ì €ì¥
        with open(_get_checksum_cache_path(), 'w', encoding='utf-8') as f:
            json.dump({'checksum': checksum}, f)
        
    except Exception as e:
        st.warning(f"âš ï¸ ì„ë² ë”© ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‚ ì„ë² ë”© ë²¡í„° ë¡œë“œ (ë¡œì»¬ ìºì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_cache(checksum: str) -> Optional[Dict]:
    """ì €ì¥ëœ ì„ë² ë”© ë²¡í„°ë¥¼ ë¡œì»¬ ìºì‹œ íŒŒì¼ì—ì„œ ë¡œë“œ"""
    try:
        # ì²´í¬ì„¬ í™•ì¸
        checksum_path = _get_checksum_cache_path()
        if not os.path.exists(checksum_path):
            return None
        
        with open(checksum_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
            if cached_data.get('checksum') != checksum:
                return None  # CSV íŒŒì¼ì´ ë³€ê²½ë¨
        
        # ì„ë² ë”© ë²¡í„° ë¡œë“œ

        embeddings_path = _get_embeddings_cache_path()
        if not os.path.exists(embeddings_path):
            return None
        
        with open(embeddings_path, 'rb') as f:
            return pickle.load(f)
    
    except Exception as e:
        st.warning(f"âš ï¸ ë¡œì»¬ ì„ë² ë”© ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜ï¸ Supabase Storageì— ì„ë² ë”© ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_embeddings_to_supabase(documents: List[str], embeddings, metadatas: List[Dict], ids: List[str], checksum: str) -> bool:
    """Supabase Storageì— ì„ë² ë”© ë²¡í„° ì €ì¥"""
    if not SUPABASE_ENABLE:
        return False
    
    supabase = get_supabase_client()
    if not supabase:
        return False
    
    try:
        # 1. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„
        cache_data = {
            'documents': documents,
            'embeddings': embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings,
            'metadatas': metadatas,
            'ids': ids
        }
        
        pickled_data = pickle.dumps(cache_data)
        compressed_data = gzip.compress(pickled_data)
        
        # 3. Storage ë²„í‚·ê³¼ ê²½ë¡œ ì„¤ì • (gzip í™•ì¥ì)
        bucket_name = "glossary-cache"
        storage_path = f"embeddings/{checksum}.pkl.gz"
        
        # 4. Storageì— ì—…ë¡œë“œ (ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°)
        try:
            # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì‹œë„ (ìˆìœ¼ë©´)
            supabase.storage.from_(bucket_name).remove([storage_path])
        except:
            pass  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
        
        # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ (gzip ì••ì¶•ëœ ë°ì´í„°)
        supabase.storage.from_(bucket_name).upload(
            storage_path,
            compressed_data,
            file_options={"content-type": "application/octet-stream", "upsert": "true"}
        )
        
        # 5. ë©”íƒ€ë°ì´í„°ë¥¼ í…Œì´ë¸”ì— ì €ì¥ (glossary_embeddings í…Œì´ë¸”)
        try:
            supabase.table("glossary_embeddings").upsert({
                "checksum": checksum,
                "storage_path": storage_path,
                "term_count": len(documents),
                "updated_at": "now()"
            }).execute()
        except Exception as table_error:
            # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ (StorageëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ)
            st.warning(f"âš ï¸ glossary_embeddings í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {table_error}")
        
        return True
    
    except Exception as e:
        st.warning(f"âš ï¸ Supabase Storage ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜ï¸ Supabase Storageì—ì„œ ì„ë² ë”© ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_from_supabase(checksum: str) -> Optional[Dict]:
    """Supabase Storageì—ì„œ ì„ë² ë”© ë²¡í„° ë¡œë“œ (1ìˆœìœ„)"""
    if not SUPABASE_ENABLE:
        return None
    
    supabase = get_supabase_client()
    if not supabase:
        return None
    
    try:
        # 1. ë©”íƒ€ë°ì´í„° í…Œì´ë¸”ì—ì„œ í™•ì¸ (ì„ íƒì , ì—†ì–´ë„ ì§„í–‰)
        bucket_name = "glossary-cache"
        storage_path = f"embeddings/{checksum}.pkl"

        
        try:
            # ë©”íƒ€ë°ì´í„° í™•ì¸ (ìˆìœ¼ë©´ ì²´í¬ì„¬ ê²€ì¦)
            result = supabase.table("glossary_embeddings").select("*").eq("checksum", checksum).execute()
            if result.data and len(result.data) > 0:
                # ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©
                metadata = result.data[0]
                storage_path = metadata.get("storage_path", storage_path)

        except:
            # í…Œì´ë¸”ì´ ì—†ì–´ë„ Storageì—ì„œ ì§ì ‘ í™•ì¸
            pass
        

        # 2. Storageì—ì„œ ë‹¤ìš´ë¡œë“œ
        response = supabase.storage.from_(bucket_name).download(storage_path)
        
        if not response:
            return None
        
        # 3. pickleë¡œ ì—­ì§ë ¬í™”
        return pickle.loads(response)

    
    except Exception as e:
        # íŒŒì¼ì´ ì—†ê±°ë‚˜ ì—ëŸ¬ ë°œìƒ ì‹œ None ë°˜í™˜ (ì¡°ìš©íˆ ì‹¤íŒ¨)
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ë¡œë“œ: Supabase ìš°ì„ , ë¡œì»¬ Fallback
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_with_fallback(checksum: str) -> Optional[Dict]:
    """
    ì„ë² ë”© ë²¡í„° ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)

    ìš°ì„ ìˆœìœ„:
    1. ë¡œì»¬ ìºì‹œ íŒŒì¼ (ë¹ ë¥¸ ë¡œì»¬ ì ‘ê·¼)
    2. Supabase Storage (ì›ê²© ì €ì¥ì†Œ)
    3. None (ìƒˆë¡œ ìƒì„± í•„ìš”)
    """
    cached_data = _load_embeddings_cache(checksum)
    if cached_data:
        st.session_state["rag_cache_source"] = "local"
        _sync_supabase_async(
            cached_data['documents'],
            cached_data['embeddings'],
            cached_data['metadatas'],
            cached_data['ids'],
            checksum
        )
        return cached_data

    cached_data = _load_embeddings_from_supabase(checksum)
    if cached_data:
        st.session_state["rag_cache_source"] = "supabase"
        try:
            _save_embeddings_cache(
                cached_data['documents'],
                cached_data['embeddings'],
                cached_data['metadatas'],
                cached_data['ids'],
                checksum
            )
            st.session_state["rag_cache_synced"] = True
        except:
            pass
        return cached_data

    st.session_state["rag_cache_source"] = "none"
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§  RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ë²¡í„° DB êµ¬ì¶• (í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ë²„ì „)
# - ì„ë² ë”© ëª¨ë¸: ì „ì—­ ìºì‹œë¡œ ì¬ì‚¬ìš© (ì„¸ì…˜ë§ˆë‹¤ ì¬ë¡œë“œ ë°©ì§€)
# - ì„ë² ë”© ë²¡í„°: Supabase Storage ìš°ì„ , ë¡œì»¬ Fallback (í•˜ì´ë¸Œë¦¬ë“œ)
# - ChromaDB: persistent ëª¨ë“œë¡œ ë””ìŠ¤í¬ì— ì €ì¥ (ì„¸ì…˜ ê°„ ìœ ì§€)
# - CSV ì²´í¬ì„¬: íŒŒì¼ ë³€ê²½ ê°ì§€í•˜ì—¬ ìë™ ì¬ì„ë² ë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”: ë²¡í„° DB ìƒì„± ë° ê¸ˆìœµìš©ì–´ ì„ë² ë”© (í•˜ì´ë¸Œë¦¬ë“œ ìºì‹œ)"""

    # ì„¸ì…˜ì— ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if "rag_initialized" in st.session_state and st.session_state.rag_initialized:
        return

    perf_enabled = _perf_enabled()
    perf_steps: List[Dict] = []
    total_start = time.perf_counter() if perf_enabled else 0.0
    step_start = total_start
    perf_logged = False

    try:
        # 1ï¸âƒ£ CSV ë¡œë“œ ë° ì²´í¬ì„¬ ê³„ì‚°
        with st.spinner("ğŸ“„ ê¸ˆìœµìš©ì–´ íŒŒì¼ ë¡œë“œ ì¤‘..."):
            csv_path = os.path.join(os.path.dirname(__file__), "glossary", "ê¸ˆìœµìš©ì–´.csv")
            if not os.path.exists(csv_path):
                st.warning(f"âš ï¸ ê¸ˆìœµìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
                st.session_state.rag_initialized = False
                return

            df = pd.read_csv(csv_path, encoding="utf-8")
            df = df.fillna("")
            csv_checksum = _calculate_csv_checksum(csv_path)
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "csv_load", step_start)

        # 2ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹œ ì‚¬ìš©)
        # ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë¡œë“œê°€ ë§¤ìš° ëŠë¦¬ë¯€ë¡œ í•­ìƒ ìŠ¤í”¼ë„ˆ í‘œì‹œ
        embedding_model = _get_embedding_model()
        if embedding_model is None or _embedding_model_cache is None:
            with st.spinner("ğŸ¤– í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ 10-20ì´ˆ ì†Œìš”)"):
                embedding_model = _get_embedding_model()
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "model_ready", step_start)

        # 3ï¸âƒ£ ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (persistent ëª¨ë“œ)
        with st.spinner("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
            chroma_db_path = os.path.join(_get_cache_dir(), "chroma_db")
            chroma_client = chromadb.PersistentClient(
                path=chroma_db_path,
                settings=Settings(
                    anonymized_telemetry=False
                )
            )
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "chroma_client", step_start)

        # 4ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ë¡œë“œ ì‹œë„ (Supabase ìš°ì„ , ë¡œì»¬ Fallback)
        with st.spinner("ğŸ”„ ì„ë² ë”© ë²¡í„° ë¡œë“œ ì¤‘..."):
            cached_data = _load_embeddings_with_fallback(csv_checksum)
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "cache_lookup", step_start)

        # 5ï¸âƒ£ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        collection_name = "financial_terms"
        with st.spinner("ğŸ” ë²¡í„° ì»¬ë ‰ì…˜ í™•ì¸ ì¤‘..."):
            try:
                collection = chroma_client.get_collection(name=collection_name)
                if collection.count() > 0 and cached_data is not None:
                    documents = cached_data['documents']
                    metadatas = cached_data['metadatas']
                    ids = cached_data['ids']

                    st.session_state.rag_collection = collection
                    st.session_state.rag_embedding_model = embedding_model
                    st.session_state.rag_initialized = True
                    st.session_state.rag_term_count = len(documents)
                    _cache_rag_metadata(metadatas)
                    st.session_state["rag_explanation_cache"] = {}

                    if perf_enabled:
                        step_start = _perf_step(perf_enabled, perf_steps, "cache_ready", step_start)
                        perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
                        _record_perf("initialize", perf_steps)
                        perf_logged = True

                    cache_source = "Supabase" if SUPABASE_ENABLE else "ë¡œì»¬"
                    st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({cache_source} ìºì‹œ ì‚¬ìš©, {len(documents)}ê°œ ìš©ì–´)")
                    return
                elif cached_data is None:
                    try:
                        chroma_client.delete_collection(name=collection_name)
                    except:
                        pass
                    collection = chroma_client.create_collection(
                        name=collection_name,
                        metadata={"description": "ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë²¡í„° DB"}
                    )
            except:
                collection = chroma_client.create_collection(
                    name=collection_name,
                    metadata={"description": "ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë²¡í„° DB"}
                )
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "collection_ready", step_start)

        if cached_data is not None:
            with st.spinner("ğŸ“¦ ìºì‹œëœ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                documents = cached_data['documents']
                embeddings = cached_data['embeddings']
                metadatas = cached_data['metadatas']
                ids = cached_data['ids']

                if collection.count() == 0:
                    collection.add(
                        documents=documents,
                        metadatas=metadatas,
                        embeddings=embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings,
                        ids=ids
                    )
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "cache_materialize", step_start)
            _cache_rag_metadata(metadatas)
        else:
            with st.spinner("ğŸ“ ê¸ˆìœµìš©ì–´ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                documents = []
                metadatas = []
                ids = []

                for idx, row in df.iterrows():
                    term = str(row.get("ê¸ˆìœµìš©ì–´", "")).strip()
                    if not term:
                        continue

                    synonym = str(row.get("ìœ ì˜ì–´", "")).strip()
                    definition = str(row.get("ì •ì˜", "")).strip()
                    analogy = str(row.get("ë¹„ìœ ", "")).strip()

                    search_text = f"{term}"
                    if synonym:
                        search_text += f" ({synonym})"
                    search_text += f" - {definition}"
                    if analogy:
                        search_text += f" | ë¹„ìœ : {analogy}"

                    documents.append(search_text)

                    metadatas.append({
                        "term": term,
                        "synonym": synonym,
                        "definition": definition,
                        "analogy": analogy,
                        "importance": str(row.get("ì™œ ì¤‘ìš”?", "")).strip(),
                        "correction": str(row.get("ì˜¤í•´ êµì •", "")).strip(),
                        "example": str(row.get("ì˜ˆì‹œ", "")).strip(),
                        "difficulty": str(row.get("ë‹¨ì–´ ë‚œì´ë„", "")).strip(),
                    })

                    ids.append(f"term_{idx}")
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "documents_prepared", step_start)

            with st.spinner(f"ğŸ”„ {len(documents)}ê°œ ê¸ˆìœµìš©ì–´ ë²¡í„°í™” ì¤‘..."):
                embeddings = embedding_model.encode(documents, show_progress_bar=False)
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "embedding_encode", step_start)

            collection.add(
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings.tolist(),
                ids=ids
            )
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "collection_populate", step_start)

            with st.spinner("ğŸ’¾ ì„ë² ë”© ë²¡í„° ì €ì¥ ì¤‘..."):
                _save_embeddings_cache(documents, embeddings, metadatas, ids, csv_checksum)
                st.session_state["rag_cache_synced"] = False
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "cache_save", step_start)

            _sync_supabase_async(documents, embeddings, metadatas, ids, csv_checksum)

        st.session_state.rag_collection = collection
        st.session_state.rag_embedding_model = embedding_model
        st.session_state.rag_initialized = True
        st.session_state.rag_term_count = len(documents)
        _cache_rag_metadata(metadatas)
        st.session_state["rag_explanation_cache"] = {}

        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "session_update", step_start)
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
            _record_perf("initialize", perf_steps)
            perf_logged = True

        if cached_data is not None:
            cache_source = "Supabase" if SUPABASE_ENABLE else "ë¡œì»¬"
            st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({cache_source} ìºì‹œ ì‚¬ìš©, {len(documents)}ê°œ ìš©ì–´)")
        else:
            save_source = "Supabase + ë¡œì»¬" if SUPABASE_ENABLE else "ë¡œì»¬"
            st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({len(documents)}ê°œ ìš©ì–´ ë¡œë“œ, {save_source}ì— ì €ì¥ë¨)")

    except Exception as e:
        st.error(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.session_state.rag_initialized = False
    finally:
        if perf_enabled and not perf_logged:
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
            _record_perf("initialize", perf_steps)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” RAG ê¸°ë°˜ ìš©ì–´ ê²€ìƒ‰
# - ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ ìœ ì‚¬í•œ ìš©ì–´ ê²€ìƒ‰
# - ìƒìœ„ kê°œì˜ ê´€ë ¨ ìš©ì–´ ë°˜í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_terms_by_rag(query: str, top_k: int = 3) -> List[Dict]:
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê¸ˆìœµ ìš©ì–´ ê²€ìƒ‰"""

    if not st.session_state.get("rag_initialized", False):
        return []

    perf_enabled = _perf_enabled()
    perf_steps: List[Dict] = []
    total_start = time.perf_counter() if perf_enabled else 0.0
    step_start = total_start
    perf_logged = False

    try:
        collection = st.session_state.rag_collection
        embedding_model = st.session_state.rag_embedding_model

        query_embedding = embedding_model.encode([query])[0]
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "encode", step_start)

        results = collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "query", step_start)

        matched_terms = []
        if results and results['metadatas']:
            for metadata in results['metadatas'][0]:
                matched_terms.append(metadata)
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "format", step_start)
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2), "info": {"top_k": top_k, "returned": len(matched_terms)}})
            _record_perf("query", perf_steps)
            perf_logged = True

        return matched_terms

    except Exception as e:
        st.error(f"âŒ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []
    finally:
        if perf_enabled and not perf_logged:
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
            _record_perf("query", perf_steps)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¦‰ ì±—ë´‡ ì‘ë‹µìš©: RAG ê¸°ë°˜ ìš©ì–´ ì„¤ëª… ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´)
# - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: í•˜ë“œì½”ë”©ëœ DEFAULT_TERMS ì‚¬ì „ì—ì„œ ê²€ìƒ‰
#   2. ì‹ ê·œ: RAG ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ ìš©ì–´ ì°¾ê¸°
#   3. Fallback: RAG ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë™ì‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def explain_term(term: str, chat_history=None, return_rag_info: bool = False):
    """
    ìš©ì–´ ì„¤ëª… ìƒì„± (RAG ì •í™• ë§¤ì¹­ ìš°ì„ , ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì‚¬ì „ ì‚¬ìš©)

    Args:
        term: ì„¤ëª…í•  ê¸ˆìœµ ìš©ì–´
        chat_history: ì±„íŒ… ì´ë ¥ (í–¥í›„ ì»¨í…ìŠ¤íŠ¸ ê°•í™”ìš©)
        return_rag_info: Trueì¼ ê²½ìš° (explanation, rag_info) íŠœí”Œ ë°˜í™˜

    Returns:
        return_rag_info=False: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš©ì–´ ì„¤ëª… ë¬¸ìì—´
        return_rag_info=True: (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš©ì–´ ì„¤ëª…, RAG ë©”íƒ€ë°ì´í„° ë˜ëŠ” None) íŠœí”Œ
    """

    rag_info: Optional[Dict] = None
    metadata: Optional[Dict] = None
    synonym_matched = False

    if st.session_state.get("rag_initialized", False):
        try:
            metadata_map = st.session_state.get("rag_metadata_by_term")
            if not metadata_map:
                collection = st.session_state.get("rag_collection")
                if collection is None:
                    raise ValueError("RAG ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
                all_data = collection.get()
                if all_data and all_data["metadatas"]:
                    _cache_rag_metadata(all_data["metadatas"])
                    metadata_map = st.session_state.get("rag_metadata_by_term", {})

            if metadata_map:
                metadata = metadata_map.get(term.lower())
                if metadata:
                    base_term = (metadata.get("term") or "").strip()
                    synonym_field = (metadata.get("synonym") or "").strip()
                    if synonym_field:
                        synonyms = [s.strip().lower() for s in re.split(r"[,\n]", synonym_field) if s.strip()]
                        synonym_matched = term.lower() in synonyms and term.lower() != base_term.lower()
                    else:
                        synonym_matched = False

                    definition = metadata.get("definition", "")
                    analogy = metadata.get("analogy", "")
                    importance = metadata.get("importance", "")
                    correction = metadata.get("correction", "")
                    example = metadata.get("example", "")

                    cache = st.session_state.setdefault("rag_explanation_cache", {})
                    cache_key = base_term.lower()
                    response = cache.get(cache_key)

                    if response is None:
                        structured_context = _build_structured_context_from_metadata(
                            base_term=base_term,
                            metadata=metadata,
                            question_term=term,
                            synonym_matched=synonym_matched,
                        )
                        response = _generate_structured_term_response(
                            base_term=base_term,
                            context=structured_context,
                            question_term=term,
                        )
                        cache[cache_key] = response

                    if return_rag_info:
                        rag_info = {
                            "search_method": "exact_match",
                            "matched_term": base_term,
                            "synonym_used": synonym_matched,
                            "source": "rag"
                        }

                    if return_rag_info:
                        return response, rag_info
                    return response
        except Exception as e:
            st.warning(f"âš ï¸ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")

    terms = st.session_state.get("financial_terms", DEFAULT_TERMS)

    if term not in terms:
        message = f"'{term}'ì— ëŒ€í•œ ì •ë³´ê°€ ì•„ì§ ì—†ì–´. ë‹¤ë¥¸ ìš©ì–´ë¥¼ ì„ íƒí•´ì¤˜."
        if return_rag_info:
            return message, None
        return message

    info = terms[term]
    structured_context = _build_structured_context_from_default(term, info)
    response = _generate_structured_term_response(
        base_term=term,
        context=structured_context,
        question_term=term,
    )

    if return_rag_info:
        return response, None
    return response
