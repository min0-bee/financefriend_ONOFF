"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ëª¨ë“ˆ (RAG ì‹œìŠ¤í…œ í†µí•©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import re
import streamlit as st
import pickle
import hashlib
import json
import gzip
import os
import pandas as pd
from typing import Dict, List, Optional
from financefriend_ONOFF.persona.persona import albwoong_persona_rewrite_section, albwoong_persona_reply
from core.logger import get_supabase_client
from core.config import SUPABASE_ENABLE
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ ì „ì—­ ìºì‹œ: ì„ë² ë”© ëª¨ë¸ (ì„¸ì…˜ ê°„ ì¬ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_embedding_model_cache = None
_RAG_AVAILABLE = chromadb is not None and SentenceTransformer is not None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ê¸°ë³¸ ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ (RAG/ì‚¬ì „ ì—†ì´ë„ ë™ì‘í•˜ëŠ” ìµœì†Œ ì„¸íŠ¸)
# - ê° ìš©ì–´ëŠ” 'ì •ì˜', 'ì„¤ëª…', 'ë¹„ìœ 'ë¡œ êµ¬ì„±
# - ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” DB/CSV/RAGë¡œ ëŒ€ì²´ ê°€ëŠ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_TERMS = {
    "ì–‘ì ì™„í™”": {
        "ì •ì˜": "ì¤‘ì•™ì€í–‰ì´ ì‹œì¤‘ì— í†µí™”ë¥¼ ê³µê¸‰í•˜ê¸° ìœ„í•´ êµ­ì±„ ë“±ì„ ë§¤ì…í•˜ëŠ” ì •ì±…",
        "ì„¤ëª…": "ê²½ê¸° ë¶€ì–‘ì„ ìœ„í•´ ì¤‘ì•™ì€í–‰ì´ ëˆì„ í’€ì–´ ì‹œì¥ ìœ ë™ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "ë§ˆë¥¸ ë•…ì— ë¬¼ì„ ë¿Œë ¤ì£¼ëŠ” ê²ƒì²˜ëŸ¼, ê²½ì œì— ëˆì´ë¼ëŠ” ë¬¼ì„ ê³µê¸‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.",
    },
    "ê¸°ì¤€ê¸ˆë¦¬": {
        "ì •ì˜": "ì¤‘ì•™ì€í–‰ì´ ì‹œì¤‘ì€í–‰ì— ëˆì„ ë¹Œë ¤ì¤„ ë•Œ ì ìš©í•˜ëŠ” ê¸°ì¤€ì´ ë˜ëŠ” ê¸ˆë¦¬",
        "ì„¤ëª…": "ëª¨ë“  ê¸ˆë¦¬ì˜ ê¸°ì¤€ì´ ë˜ë©°, ê¸°ì¤€ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ëŒ€ì¶œì´ìë„ í•¨ê»˜ ì˜¤ë¦…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "ë¬¼ê°€ì˜ ì˜¨ë„ì¡°ì ˆê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ê²½ì œê°€ ê³¼ì—´ë˜ë©´ ì˜¬ë¦¬ê³ , ì¹¨ì²´ë˜ë©´ ë‚´ë¦½ë‹ˆë‹¤.",
    },
    "ë°°ë‹¹": {
        "ì •ì˜": "ê¸°ì—…ì´ ë²Œì–´ë“¤ì¸ ì´ìµ ì¤‘ ì¼ë¶€ë¥¼ ì£¼ì£¼ë“¤ì—ê²Œ ë‚˜ëˆ ì£¼ëŠ” ê²ƒ",
        "ì„¤ëª…": "ì£¼ì‹ì„ ë³´ìœ í•œ ì£¼ì£¼ì—ê²Œ ê¸°ì—…ì˜ ì´ìµì„ ë¶„ë°°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "í•¨ê»˜ ì‹ë‹¹ì„ ìš´ì˜í•˜ëŠ” ë™ì—…ìë“¤ì´ ë§¤ì¶œ ì¤‘ ì¼ë¶€ë¥¼ ë‚˜ëˆ ê°–ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.",
    },
    "PER": {
        "ì •ì˜": "ì£¼ê°€ìˆ˜ìµë¹„ìœ¨. ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœì´ìµìœ¼ë¡œ ë‚˜ëˆˆ ê°’",
        "ì„¤ëª…": "ì£¼ì‹ì´ 1ë…„ ì¹˜ ì´ìµì˜ ëª‡ ë°°ì— ê±°ë˜ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì €í‰ê°€ëœ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ë¹„ìœ ": "1ë…„ì— 100ë§Œì› ë²„ëŠ” ê°€ê²Œë¥¼ ëª‡ ë…„ ì¹˜ ìˆ˜ìµì„ ì£¼ê³  ì‚¬ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
    },
    "í™˜ìœ¨": {
        "ì •ì˜": "ì„œë¡œ ë‹¤ë¥¸ ë‘ ë‚˜ë¼ í™”íì˜ êµí™˜ ë¹„ìœ¨",
        "ì„¤ëª…": "ì›í™”ë¥¼ ë‹¬ëŸ¬ë¡œ, ë‹¬ëŸ¬ë¥¼ ì›í™”ë¡œ ë°”ê¿€ ë•Œ ì ìš©ë˜ëŠ” ë¹„ìœ¨ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "í•´ì™¸ ì‡¼í•‘ëª°ì—ì„œ ë¬¼ê±´ì„ ì‚´ ë•Œ ì ìš©ë˜ëŠ” í™˜ì „ ë¹„ìœ¨ì…ë‹ˆë‹¤.",
    },
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§° ì„¸ì…˜ì— ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë³´ì¥ (RAG í†µí•© ë²„ì „)
#   - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: DEFAULT_TERMSë§Œ ë³µì‚¬
#   2. ì‹ ê·œ: RAG ì‹œìŠ¤í…œ ìë™ ì´ˆê¸°í™” ì¶”ê°€
#   3. Fallback: RAG ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ DEFAULT_TERMS ì‚¬ìš©
# - Streamlitì€ ì‚¬ìš©ìë³„ ì„¸ì…˜ ìƒíƒœ(st.session_state)ë¥¼ ì œê³µ
# - ìµœì´ˆ 1íšŒë§Œ DEFAULT_TERMSë¥¼ ë³µì‚¬í•´ ë„£ì–´ ì¤‘ê°„ ë³€ê²½ì—ë„ ì›ë³¸ ë³´ì¡´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_financial_terms():
    """
    ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ì´ˆê¸°í™” ë° RAG ì‹œìŠ¤í…œ ìë™ ì‹œì‘
    - ì„¸ì…˜ ìµœì´ˆ ì‹¤í–‰ ì‹œ RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”
    - Fallbackìœ¼ë¡œ ê¸°ë³¸ ìš©ì–´ ì‚¬ì „ë„ ìœ ì§€
    """
    # 1ï¸âƒ£ ê¸°ë³¸ ìš©ì–´ ì‚¬ì „ ì´ˆê¸°í™” (Fallbackìš©)
    if "financial_terms" not in st.session_state:
        st.session_state.financial_terms = DEFAULT_TERMS.copy()

    # 2ï¸âƒ£ RAG ì‹œìŠ¤í…œ ìë™ ì´ˆê¸°í™” (ìµœì´ˆ 1íšŒë§Œ)
    if "rag_initialized" not in st.session_state:
        if not _RAG_AVAILABLE:
            st.session_state.rag_initialized = False
            st.warning("âš ï¸ ê³ ê¸‰ ìš©ì–´ ê²€ìƒ‰ ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ê¸°ë³¸ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        else:
            initialize_rag_system()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”´ ê¸°ì¡´ í•¨ìˆ˜ (ì£¼ì„ì²˜ë¦¬): í•˜ë“œì½”ë”©ëœ ì‚¬ì „ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def highlight_terms(text: str) -> str:
#     highlighted = text
#
#     # í˜„ì¬ ì„¸ì…˜ì˜ ìš©ì–´ ì‚¬ì „ì—ì„œ í‚¤(ìš©ì–´)ë§Œ ìˆœíšŒ
#     for term in st.session_state.financial_terms.keys():
#         # re.escape(term): íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ìš©ì–´ë„ ì•ˆì „í•˜ê²Œ ë§¤ì¹­
#         # re.IGNORECASE: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰ (ì˜ë¬¸ ìš©ì–´ ëŒ€ë¹„)
#         pattern = re.compile(re.escape(term), re.IGNORECASE)
#
#         # âš ï¸ ì£¼ì˜: ì•„ë˜ ëŒ€ì²´ ë¬¸ìì—´ì˜ {term}ì€ 'ì‚¬ì „ í‚¤' í‘œê¸°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
#         # - ë§¤ì¹­ëœ ì›ë˜ í‘œê¸°(ëŒ€ì†Œë¬¸ì)ë¥¼ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ repl í•¨ìˆ˜ ì‚¬ìš© í•„ìš”
#         #   ì˜ˆ) pattern.sub(lambda m: f"...>{m.group(0)}</mark>", highlighted)
#         highlighted = pattern.sub(
#             f'<mark class="clickable-term" data-term="{term}" '
#             f'style="background-color: #FFEB3B; cursor: pointer; padding: 2px 4px; border-radius: 3px;">{term}</mark>',
#             highlighted,
#         )
#     return highlighted


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ¨ ë³¸ë¬¸ì—ì„œ ê¸ˆìœµ ìš©ì–´ í•˜ì´ë¼ì´íŠ¸ (RAG í†µí•© ë²„ì „)
# - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: st.session_state.financial_terms ì‚¬ì „ì—ì„œë§Œ ê²€ìƒ‰
#   2. ì‹ ê·œ: RAGì— ì €ì¥ëœ ëª¨ë“  ìš©ì–´ë¥¼ í•˜ì´ë¼ì´íŠ¸ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©
#   3. Fallback: RAG ë¯¸ì´ˆê¸°í™” ì‹œ ê¸°ì¡´ ì‚¬ì „ ì‚¬ìš©
# - ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ë¥¼ ì°¾ì•„ <mark> íƒœê·¸ë¡œ ê°ì‹¸ ê°•ì¡°
# - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ(re.IGNORECASE) â†’ ì˜ë¬¸ ì•½ì–´ ë“±ì—ë„ ëŒ€ì‘
# - data-term ì†ì„±: ì¶”í›„ JS/ì´ë²¤íŠ¸ ì—°ê²° ì‹œ ì–´ë–¤ ìš©ì–´ì¸ì§€ ì‹ë³„ ìš©ì´
# - Streamlit ì¶œë ¥ ì‹œ st.markdown(..., unsafe_allow_html=True) í•„ìš”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def highlight_terms(text: str) -> str:
    """
    ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ê¸ˆìœµ ìš©ì–´ë¥¼ ì°¾ì•„ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸ (ê¸°ì‚¬ ë³¸ë¬¸ ë“±)

    Returns:
        ê¸ˆìœµ ìš©ì–´ê°€ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ëœ HTML ë¬¸ìì—´
    """
    highlighted = text
    terms_to_highlight = set()

    # 1ï¸âƒ£ RAGê°€ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ RAGì˜ ëª¨ë“  ìš©ì–´ ì‚¬ìš©
    if st.session_state.get("rag_initialized", False):
        try:
            collection = st.session_state.rag_collection
            # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ìš©ì–´ ì¶”ì¶œ
            all_data = collection.get()
            if all_data and all_data['metadatas']:
                for metadata in all_data['metadatas']:
                    term = metadata.get('term', '').strip()
                    if term:
                        terms_to_highlight.add(term)
                    # ìœ ì˜ì–´ë„ í•˜ì´ë¼ì´íŠ¸ ëŒ€ìƒì— ì¶”ê°€
                    # synonym = metadata.get('synonym', '').strip()
                    # if synonym:
                    #     terms_to_highlight.add(synonym)
        except Exception as e:
            st.warning(f"âš ï¸ RAG ìš©ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ì‚¬ì „ ì‚¬ìš©: {e}")
            # Fallback: ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ì‚¬ì „ ì‚¬ìš©
            terms_to_highlight = set(st.session_state.get("financial_terms", DEFAULT_TERMS).keys())
    else:
        # 2ï¸âƒ£ RAG ë¯¸ì´ˆê¸°í™” ì‹œ ê¸°ì¡´ ì‚¬ì „ ì‚¬ìš©
        terms_to_highlight = set(st.session_state.get("financial_terms", DEFAULT_TERMS).keys())

    # 3ï¸âƒ£ ìš©ì–´ë³„ë¡œ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬
    # ê¸´ ìš©ì–´ë¶€í„° ì²˜ë¦¬í•˜ì—¬ ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€ (ì˜ˆ: "ë¶€ê°€ê°€ì¹˜ì„¸"ê°€ "ë¶€ê°€ê°€ì¹˜"ë³´ë‹¤ ë¨¼ì € ì²˜ë¦¬)
    sorted_terms = sorted(terms_to_highlight, key=len, reverse=True)

    # ì´ë¯¸ í•˜ì´ë¼ì´íŠ¸ëœ ë¶€ë¶„ì„ ë³´í˜¸í•˜ê¸° ìœ„í•œ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©
    placeholders = {}
    placeholder_counter = 0

    for term in sorted_terms:
        if not term:  # ë¹ˆ ë¬¸ìì—´ ìŠ¤í‚µ
            continue

        # í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ë§¤ì¹­í•˜ë„ë¡ íŒ¨í„´ ìƒì„±
        # __PLACEHOLDER_ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì€ ì œì™¸
        escaped_term = re.escape(term)

        # ë§¤ì¹­ëœ ì›ë˜ í‘œê¸°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•˜ì´ë¼ì´íŠ¸
        matches = []
        pattern = re.compile(escaped_term, re.IGNORECASE)

        for match in pattern.finditer(highlighted):
            # ë§¤ì¹­ëœ ìœ„ì¹˜ê°€ í”Œë ˆì´ìŠ¤í™€ë” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            start_pos = match.start()
            # ë§¤ì¹­ ìœ„ì¹˜ ì´ì „ì— í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆê³  ì•„ì§ ë‹«íˆì§€ ì•Šì•˜ëŠ”ì§€ ì²´í¬
            prefix = highlighted[:start_pos]
            # í”Œë ˆì´ìŠ¤í™€ë” ì•ˆì— ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ ì €ì¥
            if '__PLACEHOLDER_' not in highlighted[max(0, start_pos-20):start_pos]:
                matches.append(match)

        # ë’¤ì—ì„œë¶€í„° ì¹˜í™˜ (ì¸ë±ìŠ¤ ë³€ê²½ ë°©ì§€)
        for match in reversed(matches):
            matched_text = match.group(0)
            # HTML íƒœê·¸ ìƒì„± (Streamlitì€ í´ë¦­ ì´ë²¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‹œê°ì  í‘œì‹œë§Œ)
            placeholder = f"__PLACEHOLDER_{placeholder_counter}__"
            mark_html = (
                f'<mark class="financial-term" '
                f'style="background-color: #FFEB3B; padding: 2px 4px; border-radius: 3px;">'
                f'{matched_text}</mark>'
            )
            placeholders[placeholder] = mark_html
            placeholder_counter += 1

            # í…ìŠ¤íŠ¸ ì¹˜í™˜
            highlighted = highlighted[:match.start()] + placeholder + highlighted[match.end():]

    # ëª¨ë“  í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ HTMLë¡œ ë³µì›
    for placeholder, mark_html in placeholders.items():
        highlighted = highlighted.replace(placeholder, mark_html)

    return highlighted

def _fmt(header_icon: str, header_text: str, body_md: str) -> str:
    if not body_md or not body_md.strip():
        return ""
    return f"{header_icon} **{header_text}**\n\n{body_md}\n"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“ CSV íŒŒì¼ì—ì„œ ê¸ˆìœµìš©ì–´ ë¡œë“œ
# - rag/glossary/ê¸ˆìœµìš©ì–´.csv íŒŒì¼ì„ pandasë¡œ ì½ì–´ì˜´
# - ì»¬ëŸ¼: ë²ˆí˜¸, ê¸ˆìœµìš©ì–´, ì •ì˜, ë¹„ìœ , ì™œ ì¤‘ìš”?, ì˜¤í•´ êµì •, ì˜ˆì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_glossary_from_csv() -> pd.DataFrame:
    """ê¸ˆìœµìš©ì–´.csv íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    csv_path = os.path.join(os.path.dirname(__file__), "glossary", "ê¸ˆìœµìš©ì–´.csv")

    if not os.path.exists(csv_path):
        st.warning(f"âš ï¸ ê¸ˆìœµìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return pd.DataFrame()

    try:
        df = pd.read_csv(csv_path, encoding="utf-8")
        # ê²°ì¸¡ì¹˜ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        df = df.fillna("")
        return df
    except Exception as e:
        st.error(f"âŒ CSV ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” CSV íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _calculate_csv_checksum(csv_path: str) -> str:
    """CSV íŒŒì¼ì˜ ì²´í¬ì„¬ì„ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
    try:
        with open(csv_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return file_hash
    except Exception:
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹œ ì‚¬ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_embedding_model():
    """ì„ë² ë”© ëª¨ë¸ì„ ì „ì—­ ìºì‹œì—ì„œ ë¡œë“œí•˜ê±°ë‚˜ ìƒˆë¡œ ë¡œë“œ"""
    global _embedding_model_cache
    
    if _embedding_model_cache is None:
        _embedding_model_cache = SentenceTransformer('jhgan/ko-sroberta-multitask')
    
    return _embedding_model_cache


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¾ ì„ë² ë”© ë²¡í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_cache_dir():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    cache_dir = os.path.join(os.path.dirname(__file__), "glossary", ".cache")
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir


def _get_embeddings_cache_path():
    """ì„ë² ë”© ë²¡í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ (ë¡œì»¬ì€ ì••ì¶• ì—†ìŒ, ë¹ ë¥¸ ë¡œë“œ)"""
    return os.path.join(_get_cache_dir(), "embeddings.pkl")


def _get_metadata_cache_path():
    """ë©”íƒ€ë°ì´í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
    return os.path.join(_get_cache_dir(), "metadata.pkl")


def _get_checksum_cache_path():
    """ì²´í¬ì„¬ ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
    return os.path.join(_get_cache_dir(), "checksum.json")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¾ ì„ë² ë”© ë²¡í„° ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_embeddings_cache(documents: List[str], embeddings, metadatas: List[Dict], ids: List[str], checksum: str):
    """ì„ë² ë”© ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥ (ë¡œì»¬ì€ ì••ì¶• ì—†ìŒ, ë¹ ë¥¸ ë¡œë“œ)"""
    try:
        cache_dir = _get_cache_dir()
        
        # ì„ë² ë”© ë²¡í„° ì €ì¥ (ì••ì¶• ì—†ìŒ - ë¹ ë¥¸ ë¡œë“œ)
        cache_data = {
            'documents': documents,
            'embeddings': embeddings,
            'metadatas': metadatas,
            'ids': ids
        }
        
        with open(_get_embeddings_cache_path(), 'wb') as f:
            pickle.dump(cache_data, f)
        
        # ì²´í¬ì„¬ ì €ì¥
        with open(_get_checksum_cache_path(), 'w', encoding='utf-8') as f:
            json.dump({'checksum': checksum}, f)
        
    except Exception as e:
        st.warning(f"âš ï¸ ì„ë² ë”© ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‚ ì„ë² ë”© ë²¡í„° ë¡œë“œ (ë¡œì»¬ ìºì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_cache(checksum: str) -> Optional[Dict]:
    """ì €ì¥ëœ ì„ë² ë”© ë²¡í„°ë¥¼ ë¡œì»¬ ìºì‹œ íŒŒì¼ì—ì„œ ë¡œë“œ"""
    try:
        # ì²´í¬ì„¬ í™•ì¸
        checksum_path = _get_checksum_cache_path()
        if not os.path.exists(checksum_path):
            return None
        
        with open(checksum_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
            if cached_data.get('checksum') != checksum:
                return None  # CSV íŒŒì¼ì´ ë³€ê²½ë¨
        
        # ì„ë² ë”© ë²¡í„° ë¡œë“œ (ì••ì¶• ì—†ìŒ - ë¹ ë¥¸ ë¡œë“œ)
        embeddings_path = _get_embeddings_cache_path()
        if not os.path.exists(embeddings_path):
            return None
        
        with open(embeddings_path, 'rb') as f:
            return pickle.load(f)
    
    except Exception as e:
        st.warning(f"âš ï¸ ë¡œì»¬ ì„ë² ë”© ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜ï¸ Supabase Storageì— ì„ë² ë”© ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_embeddings_to_supabase(documents: List[str], embeddings, metadatas: List[Dict], ids: List[str], checksum: str) -> bool:
    """Supabase Storageì— ì„ë² ë”© ë²¡í„° ì €ì¥"""
    if not SUPABASE_ENABLE:
        return False
    
    supabase = get_supabase_client()
    if not supabase:
        return False
    
    try:
        # 1. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„
        cache_data = {
            'documents': documents,
            'embeddings': embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings,
            'metadatas': metadatas,
            'ids': ids
        }
        
        # 2. pickleë¡œ ì§ë ¬í™” í›„ gzip ì••ì¶•
        pickled_data = pickle.dumps(cache_data)
        compressed_data = gzip.compress(pickled_data)
        
        # 3. Storage ë²„í‚·ê³¼ ê²½ë¡œ ì„¤ì • (gzip í™•ì¥ì)
        bucket_name = "glossary-cache"
        storage_path = f"embeddings/{checksum}.pkl.gz"
        
        # 4. Storageì— ì—…ë¡œë“œ (ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°)
        try:
            # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì‹œë„ (ìˆìœ¼ë©´)
            supabase.storage.from_(bucket_name).remove([storage_path])
        except:
            pass  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
        
        # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ (gzip ì••ì¶•ëœ ë°ì´í„°)
        supabase.storage.from_(bucket_name).upload(
            storage_path,
            compressed_data,
            file_options={"content-type": "application/gzip", "upsert": "true"}
        )
        
        # 5. ë©”íƒ€ë°ì´í„°ë¥¼ í…Œì´ë¸”ì— ì €ì¥ (glossary_embeddings í…Œì´ë¸”)
        try:
            supabase.table("glossary_embeddings").upsert({
                "checksum": checksum,
                "storage_path": storage_path,
                "term_count": len(documents),
                "updated_at": "now()"
            }).execute()
        except Exception as table_error:
            # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ (StorageëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ)
            st.warning(f"âš ï¸ glossary_embeddings í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {table_error}")
        
        return True
    
    except Exception as e:
        st.warning(f"âš ï¸ Supabase Storage ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜ï¸ Supabase Storageì—ì„œ ì„ë² ë”© ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_from_supabase(checksum: str) -> Optional[Dict]:
    """Supabase Storageì—ì„œ ì„ë² ë”© ë²¡í„° ë¡œë“œ (1ìˆœìœ„)"""
    if not SUPABASE_ENABLE:
        return None
    
    supabase = get_supabase_client()
    if not supabase:
        return None
    
    try:
        # 1. ë©”íƒ€ë°ì´í„° í…Œì´ë¸”ì—ì„œ í™•ì¸ (ì„ íƒì , ì—†ì–´ë„ ì§„í–‰)
        bucket_name = "glossary-cache"
        # ê¸°ì¡´ .pkl íŒŒì¼ê³¼ ìƒˆ .pkl.gz íŒŒì¼ ëª¨ë‘ ì§€ì› (í•˜ìœ„ í˜¸í™˜ì„±)
        storage_path_gz = f"embeddings/{checksum}.pkl.gz"
        storage_path_pkl = f"embeddings/{checksum}.pkl"
        storage_path = storage_path_gz  # ê¸°ë³¸ê°’: ì••ì¶• íŒŒì¼
        
        try:
            # ë©”íƒ€ë°ì´í„° í™•ì¸ (ìˆìœ¼ë©´ ì²´í¬ì„¬ ê²€ì¦)
            result = supabase.table("glossary_embeddings").select("*").eq("checksum", checksum).execute()
            if result.data and len(result.data) > 0:
                # ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©
                metadata = result.data[0]
                storage_path = metadata.get("storage_path", storage_path_gz)
        except:
            # í…Œì´ë¸”ì´ ì—†ì–´ë„ Storageì—ì„œ ì§ì ‘ í™•ì¸
            pass
        
        # 2. Storageì—ì„œ ë‹¤ìš´ë¡œë“œ (ì••ì¶• íŒŒì¼ ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ì¡´ íŒŒì¼)
        response = None
        try:
            response = supabase.storage.from_(bucket_name).download(storage_path_gz)
        except:
            # ì••ì¶• íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ .pkl íŒŒì¼ ì‹œë„ (í•˜ìœ„ í˜¸í™˜ì„±)
            try:
                response = supabase.storage.from_(bucket_name).download(storage_path_pkl)
            except:
                pass
        
        if not response:
            return None
        
        # 3. gzip ì••ì¶• í•´ì œ í›„ pickle ì—­ì§ë ¬í™”
        try:
            # gzip ì••ì¶•ëœ ë°ì´í„°ì¸ì§€ í™•ì¸ (ì••ì¶• íŒŒì¼ í™•ì¥ì ë˜ëŠ” magic number)
            is_gzipped = storage_path.endswith('.gz') or (len(response) >= 2 and response[:2] == b'\x1f\x8b')
            if is_gzipped:
                decompressed_data = gzip.decompress(response)
                return pickle.loads(decompressed_data)
            else:
                # ê¸°ì¡´ .pkl íŒŒì¼ (ì••ì¶• ì—†ìŒ)
                return pickle.loads(response)
        except Exception as e:
            # ì••ì¶• í•´ì œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì‹œë„
            try:
                return pickle.loads(response)
            except:
                return None
    
    except Exception as e:
        # íŒŒì¼ì´ ì—†ê±°ë‚˜ ì—ëŸ¬ ë°œìƒ ì‹œ None ë°˜í™˜ (ì¡°ìš©íˆ ì‹¤íŒ¨)
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ë¡œë“œ: Supabase ìš°ì„ , ë¡œì»¬ Fallback
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_with_fallback(checksum: str) -> Optional[Dict]:
    """
    ì„ë² ë”© ë²¡í„° ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
    
    ìš°ì„ ìˆœìœ„:
    1. Supabase Storage (ì¤‘ì•™ ì €ì¥ì†Œ, ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œ)
    2. ë¡œì»¬ ìºì‹œ íŒŒì¼ (Fallback)
    3. None (ìƒˆë¡œ ìƒì„± í•„ìš”)
    """
    # 1ìˆœìœ„: Supabase Storage
    cached_data = _load_embeddings_from_supabase(checksum)
    if cached_data:
        # Supabaseì—ì„œ ë¡œë“œ ì„±ê³µ ì‹œ ë¡œì»¬ì—ë„ ë°±ì—… ì €ì¥ (ì„ íƒì )
        try:
            _save_embeddings_cache(
                cached_data['documents'],
                cached_data['embeddings'],
                cached_data['metadatas'],
                cached_data['ids'],
                checksum
            )
        except:
            pass  # ë¡œì»¬ ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
        return cached_data
    
    # 2ìˆœìœ„: ë¡œì»¬ ìºì‹œ
    cached_data = _load_embeddings_cache(checksum)
    if cached_data:
        # ë¡œì»¬ì— ìˆìœ¼ë©´ Supabaseì—ë„ ë°±ì—… ì €ì¥ (ì„ íƒì , ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬ ê°€ëŠ¥)
        try:
            _save_embeddings_to_supabase(
                cached_data['documents'],
                cached_data['embeddings'],
                cached_data['metadatas'],
                cached_data['ids'],
                checksum
            )
        except:
            pass  # Supabase ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
        return cached_data
    
    # 3ìˆœìœ„: ì—†ìŒ (ìƒˆë¡œ ìƒì„± í•„ìš”)
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§  RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ë²¡í„° DB êµ¬ì¶• (í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ë²„ì „)
# - ì„ë² ë”© ëª¨ë¸: ì „ì—­ ìºì‹œë¡œ ì¬ì‚¬ìš© (ì„¸ì…˜ë§ˆë‹¤ ì¬ë¡œë“œ ë°©ì§€)
# - ì„ë² ë”© ë²¡í„°: Supabase Storage ìš°ì„ , ë¡œì»¬ Fallback (í•˜ì´ë¸Œë¦¬ë“œ)
# - ChromaDB: persistent ëª¨ë“œë¡œ ë””ìŠ¤í¬ì— ì €ì¥ (ì„¸ì…˜ ê°„ ìœ ì§€)
# - CSV ì²´í¬ì„¬: íŒŒì¼ ë³€ê²½ ê°ì§€í•˜ì—¬ ìë™ ì¬ì„ë² ë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def initialize_rag_system():
    """RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”: ë²¡í„° DB ìƒì„± ë° ê¸ˆìœµìš©ì–´ ì„ë² ë”© (í•˜ì´ë¸Œë¦¬ë“œ ìºì‹œ)"""

    # ì„¸ì…˜ì— ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if "rag_initialized" in st.session_state and st.session_state.rag_initialized:
        return

    try:
        # 1ï¸âƒ£ CSV ë¡œë“œ ë° ì²´í¬ì„¬ ê³„ì‚°
        with st.spinner("ğŸ“„ ê¸ˆìœµìš©ì–´ íŒŒì¼ ë¡œë“œ ì¤‘..."):
            csv_path = os.path.join(os.path.dirname(__file__), "glossary", "ê¸ˆìœµìš©ì–´.csv")
            if not os.path.exists(csv_path):
                st.warning(f"âš ï¸ ê¸ˆìœµìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
                st.session_state.rag_initialized = False
                return
            
            df = load_glossary_from_csv()
            if df.empty:
                st.warning("âš ï¸ CSV íŒŒì¼ì´ ë¹„ì–´ìˆì–´ ê¸°ë³¸ ìš©ì–´ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                st.session_state.rag_initialized = False
                return
            
            # CSV íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
            csv_checksum = _calculate_csv_checksum(csv_path)

        # 2ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì „ì—­ ìºì‹œ ì‚¬ìš©)
        # ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë¡œë“œê°€ ë§¤ìš° ëŠë¦¬ë¯€ë¡œ í•­ìƒ ìŠ¤í”¼ë„ˆ í‘œì‹œ
        embedding_model = _get_embedding_model()
        if embedding_model is None or _embedding_model_cache is None:
            # ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë¡œë“œ (10-20ì´ˆ ì†Œìš”)
            with st.spinner("ğŸ¤– í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ 10-20ì´ˆ ì†Œìš”)"):
                embedding_model = _get_embedding_model()

        # 3ï¸âƒ£ ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (persistent ëª¨ë“œ)
        with st.spinner("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
            chroma_db_path = os.path.join(_get_cache_dir(), "chroma_db")
            chroma_client = chromadb.PersistentClient(
                path=chroma_db_path,
                settings=Settings(
                    anonymized_telemetry=False
                )
            )

        # 4ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ë¡œë“œ ì‹œë„ (Supabase ìš°ì„ , ë¡œì»¬ Fallback)
        with st.spinner("ğŸ”„ ì„ë² ë”© ë²¡í„° ë¡œë“œ ì¤‘..."):
            cached_data = _load_embeddings_with_fallback(csv_checksum)
        
        # 5ï¸âƒ£ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        collection_name = "financial_terms"
        with st.spinner("ğŸ” ë²¡í„° ì»¬ë ‰ì…˜ í™•ì¸ ì¤‘..."):
            try:
                collection = chroma_client.get_collection(name=collection_name)
                # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ê³  ìºì‹œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¹ ë¥¸ ì¢…ë£Œ
                if collection.count() > 0 and cached_data is not None:
                    # ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
                    documents = cached_data['documents']
                    metadatas = cached_data['metadatas']
                    ids = cached_data['ids']
                    
                    # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.rag_collection = collection
                    st.session_state.rag_embedding_model = embedding_model
                    st.session_state.rag_initialized = True
                    st.session_state.rag_term_count = len(documents)
                    
                    # ìºì‹œ ì†ŒìŠ¤ í™•ì¸ (ê°„ë‹¨íˆ SUPABASE_ENABLE ì—¬ë¶€ë§Œ í™•ì¸)
                    cache_source = "Supabase" if SUPABASE_ENABLE else "ë¡œì»¬"
                    st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({cache_source} ìºì‹œ ì‚¬ìš©, {len(documents)}ê°œ ìš©ì–´)")
                    return  # ìºì‹œ ì‚¬ìš©ìœ¼ë¡œ ë¹ ë¥¸ ì¢…ë£Œ
                elif cached_data is None:
                    # CSV íŒŒì¼ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ìºì‹œê°€ ì—†ìŒ - ì¬ìƒì„± í•„ìš”
                    try:
                        chroma_client.delete_collection(name=collection_name)
                    except:
                        pass
                    collection = chroma_client.create_collection(
                        name=collection_name,
                        metadata={"description": "ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë²¡í„° DB"}
                    )
            except:
                # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
                collection = chroma_client.create_collection(
                    name=collection_name,
                    metadata={"description": "ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë²¡í„° DB"}
                )

        # 6ï¸âƒ£ ìºì‹œëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        if cached_data is not None:
            # ìºì‹œëœ ë°ì´í„° ì‚¬ìš©
            with st.spinner("ğŸ“¦ ìºì‹œëœ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                documents = cached_data['documents']
                embeddings = cached_data['embeddings']
                metadatas = cached_data['metadatas']
                ids = cached_data['ids']
                
                # ì»¬ë ‰ì…˜ì— ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
                if collection.count() == 0:
                    collection.add(
                        documents=documents,
                        metadatas=metadatas,
                        embeddings=embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings,
                        ids=ids
                    )
        else:
            # 6ï¸âƒ£ ìºì‹œê°€ ì—†ê±°ë‚˜ CSVê°€ ë³€ê²½ë¨ - ìƒˆë¡œ ìƒì„±
            with st.spinner("ğŸ“ ê¸ˆìœµìš©ì–´ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                documents = []
                metadatas = []
                ids = []

                for idx, row in df.iterrows():
                    term = str(row.get("ê¸ˆìœµìš©ì–´", "")).strip()
                    if not term:  # ë¹ˆ ìš©ì–´ëŠ” ìŠ¤í‚µ
                        continue

                    # ê²€ìƒ‰ ë¬¸ì„œ: ìš©ì–´ + ìœ ì˜ì–´ + ì •ì˜ + ë¹„ìœ ë¥¼ ê²°í•©
                    synonym = str(row.get("ìœ ì˜ì–´", "")).strip()
                    definition = str(row.get("ì •ì˜", "")).strip()
                    analogy = str(row.get("ë¹„ìœ ", "")).strip()

                    # ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ ìƒì„±
                    search_text = f"{term}"
                    if synonym:
                        search_text += f" ({synonym})"
                    search_text += f" - {definition}"
                    if analogy:
                        search_text += f" | ë¹„ìœ : {analogy}"

                    documents.append(search_text)

                    # ë©”íƒ€ë°ì´í„°: ì „ì²´ ì •ë³´ ì €ì¥
                    metadatas.append({
                        "term": term,
                        "synonym": synonym,
                        "definition": definition,
                        "analogy": analogy,
                        "importance": str(row.get("ì™œ ì¤‘ìš”?", "")).strip(),
                        "correction": str(row.get("ì˜¤í•´ êµì •", "")).strip(),
                        "example": str(row.get("ì˜ˆì‹œ", "")).strip(),
                        "difficulty": str(row.get("ë‹¨ì–´ ë‚œì´ë„", "")).strip(),
                    })

                    ids.append(f"term_{idx}")

            # 7ï¸âƒ£ ì„ë² ë”© ìƒì„± ë° DBì— ì¶”ê°€
            with st.spinner(f"ğŸ”„ {len(documents)}ê°œ ê¸ˆìœµìš©ì–´ ë²¡í„°í™” ì¤‘..."):
                embeddings = embedding_model.encode(documents, show_progress_bar=False)

                # ì»¬ë ‰ì…˜ì— ì¶”ê°€
                collection.add(
                    documents=documents,
                    metadatas=metadatas,
                    embeddings=embeddings.tolist(),
                    ids=ids
                )

            # 8ï¸âƒ£ ì„ë² ë”© ë²¡í„° ì €ì¥ (í•˜ì´ë¸Œë¦¬ë“œ: Supabase ìš°ì„ , ë¡œì»¬ Fallback)
            with st.spinner("ğŸ’¾ ì„ë² ë”© ë²¡í„° ì €ì¥ ì¤‘..."):
                # Supabase Storageì— ì €ì¥ (1ìˆœìœ„)
                if _save_embeddings_to_supabase(documents, embeddings, metadatas, ids, csv_checksum):
                    # Supabase ì €ì¥ ì„±ê³µ ì‹œ ë¡œì»¬ì—ë„ ë°±ì—…
                    _save_embeddings_cache(documents, embeddings, metadatas, ids, csv_checksum)
                else:
                    # Supabase ì‹¤íŒ¨ ì‹œ ë¡œì»¬ì—ë§Œ ì €ì¥
                    _save_embeddings_cache(documents, embeddings, metadatas, ids, csv_checksum)

        # 9ï¸âƒ£ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
        st.session_state.rag_collection = collection
        st.session_state.rag_embedding_model = embedding_model
        st.session_state.rag_initialized = True
        st.session_state.rag_term_count = len(documents)

        # ìºì‹œ ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë©”ì‹œì§€
        if cached_data is not None:
            cache_source = "Supabase" if SUPABASE_ENABLE else "ë¡œì»¬"
            st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({cache_source} ìºì‹œ ì‚¬ìš©, {len(documents)}ê°œ ìš©ì–´)")
        else:
            save_source = "Supabase + ë¡œì»¬" if SUPABASE_ENABLE else "ë¡œì»¬"
            st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({len(documents)}ê°œ ìš©ì–´ ë¡œë“œ, {save_source}ì— ì €ì¥ë¨)")

    except Exception as e:
        st.error(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.session_state.rag_initialized = False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” RAG ê¸°ë°˜ ìš©ì–´ ê²€ìƒ‰
# - ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ ìœ ì‚¬í•œ ìš©ì–´ ê²€ìƒ‰
# - ìƒìœ„ kê°œì˜ ê´€ë ¨ ìš©ì–´ ë°˜í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_terms_by_rag(query: str, top_k: int = 3) -> List[Dict]:
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê¸ˆìœµ ìš©ì–´ ê²€ìƒ‰"""

    if not st.session_state.get("rag_initialized", False):
        return []

    try:
        collection = st.session_state.rag_collection
        embedding_model = st.session_state.rag_embedding_model

        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = embedding_model.encode([query])[0]

        # ìœ ì‚¬ë„ ê²€ìƒ‰
        results = collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )

        # ê²°ê³¼ í¬ë§·íŒ…
        matched_terms = []
        if results and results['metadatas']:
            for metadata in results['metadatas'][0]:
                matched_terms.append(metadata)

        return matched_terms

    except Exception as e:
        st.error(f"âŒ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¦‰ ì±—ë´‡ ì‘ë‹µìš©: RAG ê¸°ë°˜ ìš©ì–´ ì„¤ëª… ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´)
# - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: í•˜ë“œì½”ë”©ëœ DEFAULT_TERMS ì‚¬ì „ì—ì„œ ê²€ìƒ‰
#   2. ì‹ ê·œ: RAG ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ ìš©ì–´ ì°¾ê¸°
#   3. Fallback: RAG ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë™ì‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def explain_term(term: str, chat_history=None, return_rag_info: bool = False):
    """ìš©ì–´ ì„¤ëª… ìƒì„± (RAG ì •í™• ë§¤ì¹­ ìš°ì„ , ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‚¬ì „ ì‚¬ìš©)"""
    rag_info: Optional[Dict] = None

    if st.session_state.get("rag_initialized", False):
        try:
            collection = st.session_state.get("rag_collection")
            if collection is None:
                raise ValueError("RAG ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")

            all_data = collection.get()
            if all_data and all_data["metadatas"]:
                for metadata in all_data["metadatas"]:
                    rag_term = (metadata.get("term") or "").strip()
                    synonym = (metadata.get("synonym") or "").strip()

                    if rag_term.lower() != term.lower() and (not synonym or synonym.lower() != term.lower()):
                        continue

                    definition = metadata.get("definition", "")
                    analogy = metadata.get("analogy", "")
                    importance = metadata.get("importance", "")
                    correction = metadata.get("correction", "")
                    example = metadata.get("example", "")

                    if return_rag_info:
                        rag_info = {
                            "search_method": "exact_match",
                            "matched_term": rag_term,
                            "synonym_used": synonym.lower() == term.lower() if synonym else False,
                            "source": "rag"
                        }

                    parts: List[str] = []
                    parts.append(f"ğŸ¤– **{rag_term}** ì— ëŒ€í•´ ì„¤ëª…í•´ì¤„ê²Œ! ğŸ¯\n")

                    if definition:
                        out = albwoong_persona_rewrite_section(definition, "ì •ì˜", term=rag_term, max_sentences=2)
                        parts.append(_fmt("ğŸ“–", "ì •ì˜", out))

                    if analogy:
                        out = albwoong_persona_rewrite_section(analogy, "ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°", term=rag_term, max_sentences=2)
                        parts.append(_fmt("ğŸŒŸ", "ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°", out))

                    if importance:
                        out = albwoong_persona_rewrite_section(importance, "ì™œ ì¤‘ìš”í• ê¹Œ?", term=rag_term, max_sentences=2)
                        parts.append(_fmt("â—", "ì™œ ì¤‘ìš”í• ê¹Œ?", out))

                    if correction:
                        out = albwoong_persona_rewrite_section(correction, "í”í•œ ì˜¤í•´", term=rag_term, max_sentences=2)
                        parts.append(_fmt("âš ï¸", "í”í•œ ì˜¤í•´", out))

                    if example:
                        out = albwoong_persona_rewrite_section(example, "ì˜ˆì‹œ", term=rag_term, max_sentences=2)
                        parts.append(_fmt("ğŸ“°", "ì˜ˆì‹œ", out))

                    parts.append("ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!")
                    response = "\n".join([p for p in parts if p])

                    if return_rag_info:
                        return response, rag_info
                    return response
        except Exception as e:
            st.warning(f"âš ï¸ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ì‚¬ì „ ì‚¬ìš©: {e}")

    terms = st.session_state.get("financial_terms", DEFAULT_TERMS)

    if term not in terms:
        message = f"'{term}'ì— ëŒ€í•œ ì •ë³´ê°€ ì•„ì§ ì—†ì–´. ë‹¤ë¥¸ ìš©ì–´ë¥¼ ì„ íƒí•´ì¤˜."
        if return_rag_info:
            return message, None
        return message

    info = terms[term]
    parts: List[str] = []
    parts.append(f"ğŸ¤– **{term}** ì— ëŒ€í•´ ì„¤ëª…í•´ì¤„ê²Œ! ğŸ¯\n")

    if info.get("ì •ì˜"):
        out = albwoong_persona_rewrite_section(info["ì •ì˜"], "ì •ì˜", term=term, max_sentences=2)
        parts.append(_fmt("ğŸ“–", "ì •ì˜", out))

    if info.get("ë¹„ìœ "):
        out = albwoong_persona_rewrite_section(info["ë¹„ìœ "], "ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°", term=term, max_sentences=2)
        parts.append(_fmt("ğŸŒŸ", "ë¹„ìœ ë¡œ ì´í•´í•˜ê¸°", out))

    if info.get("ì„¤ëª…"):
        out = albwoong_persona_rewrite_section(info["ì„¤ëª…"], "ì‰¬ìš´ ì„¤ëª…", term=term, max_sentences=2)
        parts.append(_fmt("ğŸ’¡", "ì‰¬ìš´ ì„¤ëª…", out))

    parts.append("ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!")
    response = "\n".join([p for p in parts if p])

    if return_rag_info:
        return response, None
    return response
