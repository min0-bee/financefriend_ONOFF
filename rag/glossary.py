"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ëª¨ë“ˆ (RAG ì‹œìŠ¤í…œ í†µí•©)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import re
import streamlit as st
import pickle
import hashlib
import json
import gzip
import os
import time
import threading
import pandas as pd
from typing import Dict, List, Optional, Union
from persona.persona import (
    albwoong_persona_reply,
    generate_structured_persona_reply,
)
from core.logger import get_supabase_client
from core.config import SUPABASE_ENABLE
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import pickle
import hashlib
import json
import gzip
from core.logger import get_supabase_client
from core.config import SUPABASE_ENABLE

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ ì „ì—­ ìºì‹œ: ì„ë² ë”© ëª¨ë¸ (ì„¸ì…˜ ê°„ ì¬ì‚¬ìš©)
# - SentenceTransformer ëª¨ë¸ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ì „ì—­ìœ¼ë¡œ ìºì‹œ
# - ëª¨ë“  ì„¸ì…˜ì—ì„œ ë™ì¼í•œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ì¬ì‚¬ìš©
# âœ… ìµœì í™”: st.cache_resourceë¡œ ìºì‹±í•˜ë¯€ë¡œ ì „ì—­ ë³€ìˆ˜ ì œê±°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
_RAG_AVAILABLE = chromadb is not None and SentenceTransformer is not None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ê¸°ë³¸ ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ (RAG/ì‚¬ì „ ì—†ì´ë„ ë™ì‘í•˜ëŠ” ìµœì†Œ ì„¸íŠ¸)
# - ê° ìš©ì–´ëŠ” 'ì •ì˜', 'ì„¤ëª…', 'ë¹„ìœ 'ë¡œ êµ¬ì„±
# - ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” DB/CSV/RAGë¡œ ëŒ€ì²´ ê°€ëŠ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEFAULT_TERMS = {
    "ì–‘ì ì™„í™”": {
        "ì •ì˜": "ì¤‘ì•™ì€í–‰ì´ ì‹œì¤‘ì— í†µí™”ë¥¼ ê³µê¸‰í•˜ê¸° ìœ„í•´ êµ­ì±„ ë“±ì„ ë§¤ì…í•˜ëŠ” ì •ì±…",
        "ì„¤ëª…": "ê²½ê¸° ë¶€ì–‘ì„ ìœ„í•´ ì¤‘ì•™ì€í–‰ì´ ëˆì„ í’€ì–´ ì‹œì¥ ìœ ë™ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "ë§ˆë¥¸ ë•…ì— ë¬¼ì„ ë¿Œë ¤ì£¼ëŠ” ê²ƒì²˜ëŸ¼, ê²½ì œì— ëˆì´ë¼ëŠ” ë¬¼ì„ ê³µê¸‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.",
    },
    "ê¸°ì¤€ê¸ˆë¦¬": {
        "ì •ì˜": "ì¤‘ì•™ì€í–‰ì´ ì‹œì¤‘ì€í–‰ì— ëˆì„ ë¹Œë ¤ì¤„ ë•Œ ì ìš©í•˜ëŠ” ê¸°ì¤€ì´ ë˜ëŠ” ê¸ˆë¦¬",
        "ì„¤ëª…": "ëª¨ë“  ê¸ˆë¦¬ì˜ ê¸°ì¤€ì´ ë˜ë©°, ê¸°ì¤€ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ëŒ€ì¶œì´ìë„ í•¨ê»˜ ì˜¤ë¦…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "ë¬¼ê°€ì˜ ì˜¨ë„ì¡°ì ˆê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ê²½ì œê°€ ê³¼ì—´ë˜ë©´ ì˜¬ë¦¬ê³ , ì¹¨ì²´ë˜ë©´ ë‚´ë¦½ë‹ˆë‹¤.",
    },
    "ë°°ë‹¹": {
        "ì •ì˜": "ê¸°ì—…ì´ ë²Œì–´ë“¤ì¸ ì´ìµ ì¤‘ ì¼ë¶€ë¥¼ ì£¼ì£¼ë“¤ì—ê²Œ ë‚˜ëˆ ì£¼ëŠ” ê²ƒ",
        "ì„¤ëª…": "ì£¼ì‹ì„ ë³´ìœ í•œ ì£¼ì£¼ì—ê²Œ ê¸°ì—…ì˜ ì´ìµì„ ë¶„ë°°í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "í•¨ê»˜ ì‹ë‹¹ì„ ìš´ì˜í•˜ëŠ” ë™ì—…ìë“¤ì´ ë§¤ì¶œ ì¤‘ ì¼ë¶€ë¥¼ ë‚˜ëˆ ê°–ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤.",
    },
    "PER": {
        "ì •ì˜": "ì£¼ê°€ìˆ˜ìµë¹„ìœ¨. ì£¼ê°€ë¥¼ ì£¼ë‹¹ìˆœì´ìµìœ¼ë¡œ ë‚˜ëˆˆ ê°’",
        "ì„¤ëª…": "ì£¼ì‹ì´ 1ë…„ ì¹˜ ì´ìµì˜ ëª‡ ë°°ì— ê±°ë˜ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë‚®ì„ìˆ˜ë¡ ì €í‰ê°€ëœ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ë¹„ìœ ": "1ë…„ì— 100ë§Œì› ë²„ëŠ” ê°€ê²Œë¥¼ ëª‡ ë…„ ì¹˜ ìˆ˜ìµì„ ì£¼ê³  ì‚¬ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
    },
    "í™˜ìœ¨": {
        "ì •ì˜": "ì„œë¡œ ë‹¤ë¥¸ ë‘ ë‚˜ë¼ í™”íì˜ êµí™˜ ë¹„ìœ¨",
        "ì„¤ëª…": "ì›í™”ë¥¼ ë‹¬ëŸ¬ë¡œ, ë‹¬ëŸ¬ë¥¼ ì›í™”ë¡œ ë°”ê¿€ ë•Œ ì ìš©ë˜ëŠ” ë¹„ìœ¨ì…ë‹ˆë‹¤.",
        "ë¹„ìœ ": "í•´ì™¸ ì‡¼í•‘ëª°ì—ì„œ ë¬¼ê±´ì„ ì‚´ ë•Œ ì ìš©ë˜ëŠ” í™˜ì „ ë¹„ìœ¨ì…ë‹ˆë‹¤.",
    },
}


def _cache_rag_metadata(metadatas: List[Dict]):
    """
    RAG ë©”íƒ€ë°ì´í„°ë¥¼ ì„¸ì…˜ì— ìºì‹±í•˜ì—¬ ë°˜ë³µì ì¸ collection.get() í˜¸ì¶œì„ ì¤„ì…ë‹ˆë‹¤.
    term / synonym ëª¨ë‘ ì†Œë¬¸ìë¡œ í‚¤ë¥¼ ë§Œë“¤ì–´ lookup ì†ë„ë¥¼ ë†’ì´ê³ ,
    í•˜ì´ë¼ì´íŠ¸ìš© ìš©ì–´ ì„¸íŠ¸ë„ í•¨ê»˜ ì €ì¥í•©ë‹ˆë‹¤.
    """
    metadata_map: Dict[str, Dict] = {}
    highlight_terms = set()

    for meta in metadatas:
        term = (meta.get("term") or "").strip()
        if term:
            metadata_map[term.lower()] = meta
            highlight_terms.add(term)

        synonym_field = (meta.get("synonym") or "").strip()
        if synonym_field:
            for raw in re.split(r"[,\n]", synonym_field):
                synonym = raw.strip()
                if synonym:
                    metadata_map[synonym.lower()] = meta
                    highlight_terms.add(synonym)

    st.session_state["rag_metadata_by_term"] = metadata_map
    st.session_state["rag_terms_for_highlight"] = highlight_terms


def _perf_enabled() -> bool:
    return st.session_state.get("rag_perf_enable", True)


def _perf_step(perf_enabled: bool, steps: List[Dict], label: str, start_time: float) -> float:
    if not perf_enabled:
        return time.perf_counter()
    elapsed_ms = (time.perf_counter() - start_time) * 1000
    steps.append({"step": label, "ms": round(elapsed_ms, 2)})
    return time.perf_counter()


def _record_perf(section: str, steps: List[Dict]):
    if not steps:
        return
    logs = st.session_state.setdefault("rag_perf_logs", {})
    history = logs.setdefault(section, [])
    history.append({
        "timestamp": time.strftime("%H:%M:%S", time.localtime()),
        "steps": steps,
    })
    logs[section] = history[-10:]
    st.session_state[f"rag_last_{section}_perf"] = steps


def _sync_supabase_async(documents, embeddings, metadatas, ids, checksum):
    if not SUPABASE_ENABLE:
        return
    if st.session_state.get("rag_cache_synced") or st.session_state.get("rag_cache_sync_in_progress"):
        return

    def _worker():
        try:
            if _save_embeddings_to_supabase(documents, embeddings, metadatas, ids, checksum):
                st.session_state["rag_cache_synced"] = True
        except Exception as e:
            st.session_state["rag_cache_sync_error"] = str(e)
        finally:
            st.session_state["rag_cache_sync_in_progress"] = False

    st.session_state["rag_cache_sync_in_progress"] = True
    threading.Thread(target=_worker, daemon=True).start()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§° ì„¸ì…˜ì— ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë³´ì¥ (RAG í†µí•© ë²„ì „)
#   - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: DEFAULT_TERMSë§Œ ë³µì‚¬
#   2. ì‹ ê·œ: RAG ì‹œìŠ¤í…œ ìë™ ì´ˆê¸°í™” ì¶”ê°€
#   3. Fallback: RAG ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ DEFAULT_TERMS ì‚¬ìš©
# - Streamlitì€ ì‚¬ìš©ìë³„ ì„¸ì…˜ ìƒíƒœ(st.session_state)ë¥¼ ì œê³µ
# - ìµœì´ˆ 1íšŒë§Œ DEFAULT_TERMSë¥¼ ë³µì‚¬í•´ ë„£ì–´ ì¤‘ê°„ ë³€ê²½ì—ë„ ì›ë³¸ ë³´ì¡´
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âš¡ ë¹ ë¥¸ í…ìŠ¤íŠ¸ ì‚¬ì „ ë¡œë“œ (CSVì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_text_glossary_fast() -> Dict[str, Dict[str, str]]:
    """
    CSVì—ì„œ í…ìŠ¤íŠ¸ ì‚¬ì „ë§Œ ë¹ ë¥´ê²Œ ë¡œë“œ (ì„ë² ë”© ì—†ì´)
    - í•˜ì´ë¼ì´íŠ¸ì™€ ê¸°ë³¸ ì„¤ëª…ì— ì‚¬ìš©
    - ë§¤ìš° ë¹ ë¦„ (~0.1ì´ˆ)
    """
    terms_dict = {}
    
    try:
        csv_path = os.path.join(os.path.dirname(__file__), "glossary", "ê¸ˆìœµìš©ì–´.csv")
        if not os.path.exists(csv_path):
            return DEFAULT_TERMS.copy()
        
        df = pd.read_csv(csv_path, encoding="utf-8")
        df = df.fillna("")
        
        for _, row in df.iterrows():
            term = str(row.get("ê¸ˆìœµìš©ì–´", "")).strip()
            if not term:
                continue
            
            terms_dict[term] = {
                "ì •ì˜": str(row.get("ì •ì˜", "")).strip(),
                "ë¹„ìœ ": str(row.get("ë¹„ìœ ", "")).strip(),
                "ì„¤ëª…": str(row.get("ì •ì˜", "")).strip(),  # ê¸°ë³¸ ì„¤ëª…
                "ìœ ì˜ì–´": str(row.get("ìœ ì˜ì–´", "")).strip(),
                "ì™œ ì¤‘ìš”?": str(row.get("ì™œ ì¤‘ìš”?", "")).strip(),
                "ì˜¤í•´ êµì •": str(row.get("ì˜¤í•´ êµì •", "")).strip(),
                "ì˜ˆì‹œ": str(row.get("ì˜ˆì‹œ", "")).strip(),
            }
    except Exception as e:
        # CSV ë¡œë“œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‚¬ì „ ì‚¬ìš©
        pass
    
    # ê¸°ë³¸ ì‚¬ì „ê³¼ ë³‘í•© (ê¸°ë³¸ ì‚¬ì „ì´ ìš°ì„ )
    result = DEFAULT_TERMS.copy()
    result.update(terms_dict)
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def initialize_rag_system_background():
    """
    ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    - UIë¥¼ ë¸”ë¡œí‚¹í•˜ì§€ ì•ŠìŒ
    - ì´ˆê¸°í™” ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ í™œì„±í™”
    """
    if st.session_state.get("rag_initialized", False):
        return
    
    if st.session_state.get("rag_loading", False):
        return  # ì´ë¯¸ ë¡œë”© ì¤‘
    
    if not _RAG_AVAILABLE:
        st.session_state.rag_initialized = False
        return
    
    def _load_in_background():
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” ì‹¤ì œ ë¡œë”© í•¨ìˆ˜"""
        try:
            st.session_state["rag_loading"] = True
            st.session_state["rag_error"] = None
            
            # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œ ëª¨ë“œë¡œ ì‹¤í–‰)
            initialize_rag_system(is_background=True)
            
            st.session_state["rag_loading"] = False
        except Exception as e:
            st.session_state["rag_loading"] = False
            st.session_state["rag_error"] = str(e)
    
    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
    thread = threading.Thread(target=_load_in_background, daemon=True)
    thread.start()


def ensure_financial_terms():
    """
    ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ì´ˆê¸°í™” (Lazy Loading + ë°±ê·¸ë¼ìš´ë“œ ë¡œë”©)
    
    âœ… ìµœì í™”: í…ìŠ¤íŠ¸ ì‚¬ì „ë§Œ ë¨¼ì € ë¡œë“œ (0.1ì´ˆ) â†’ ì¦‰ì‹œ UI í‘œì‹œ
    âœ… ìµœì í™”: RAG ì‹œìŠ¤í…œì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¡œë“œ â†’ ì‚¬ìš©ìëŠ” ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ
    
    - ì„¸ì…˜ ìµœì´ˆ ì‹¤í–‰ ì‹œ í…ìŠ¤íŠ¸ ì‚¬ì „ë§Œ ë¹ ë¥´ê²Œ ë¡œë“œ
    - RAG ì‹œìŠ¤í…œì€ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ˆê¸°í™”
    - Fallbackìœ¼ë¡œ ê¸°ë³¸ ìš©ì–´ ì‚¬ì „ë„ ìœ ì§€
    """
    # 1ï¸âƒ£ í…ìŠ¤íŠ¸ ì‚¬ì „ ë¹ ë¥´ê²Œ ë¡œë“œ (ì¦‰ì‹œ UI í‘œì‹œ ê°€ëŠ¥)
    if "financial_terms" not in st.session_state:
        st.session_state.financial_terms = load_text_glossary_fast()
        st.session_state["terms_text_ready"] = True

    # 2ï¸âƒ£ RAG ì‹œìŠ¤í…œ ë°±ê·¸ë¼ìš´ë“œ ì´ˆê¸°í™” (UI ë¸”ë¡œí‚¹ ì—†ìŒ)
    if "rag_initialized" not in st.session_state and "rag_loading" not in st.session_state:
        if not _RAG_AVAILABLE:
            st.session_state.rag_initialized = False
        else:
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ˆê¸°í™” ì‹œì‘
            initialize_rag_system_background()



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”´ ê¸°ì¡´ í•¨ìˆ˜ (ì£¼ì„ì²˜ë¦¬): í•˜ë“œì½”ë”©ëœ ì‚¬ì „ ê¸°ë°˜ í•˜ì´ë¼ì´íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# def highlight_terms(text: str) -> str:
#     highlighted = text
#
#     # í˜„ì¬ ì„¸ì…˜ì˜ ìš©ì–´ ì‚¬ì „ì—ì„œ í‚¤(ìš©ì–´)ë§Œ ìˆœíšŒ
#     for term in st.session_state.financial_terms.keys():
#         # re.escape(term): íŠ¹ìˆ˜ë¬¸ì í¬í•¨ ìš©ì–´ë„ ì•ˆì „í•˜ê²Œ ë§¤ì¹­
#         # re.IGNORECASE: ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ê²€ìƒ‰ (ì˜ë¬¸ ìš©ì–´ ëŒ€ë¹„)
#         pattern = re.compile(re.escape(term), re.IGNORECASE)
#
#         # âš ï¸ ì£¼ì˜: ì•„ë˜ ëŒ€ì²´ ë¬¸ìì—´ì˜ {term}ì€ 'ì‚¬ì „ í‚¤' í‘œê¸°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
#         # - ë§¤ì¹­ëœ ì›ë˜ í‘œê¸°(ëŒ€ì†Œë¬¸ì)ë¥¼ ìœ ì§€í•˜ê³  ì‹¶ë‹¤ë©´ repl í•¨ìˆ˜ ì‚¬ìš© í•„ìš”
#         #   ì˜ˆ) pattern.sub(lambda m: f"...>{m.group(0)}</mark>", highlighted)
#         highlighted = pattern.sub(
#             f'<mark class="clickable-term" data-term="{term}" '
#             f'style="background-color: #FFEB3B; cursor: pointer; padding: 2px 4px; border-radius: 3px;">{term}</mark>',
#             highlighted,
#         )
#     return highlighted


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ¨ ë³¸ë¬¸ì—ì„œ ê¸ˆìœµ ìš©ì–´ í•˜ì´ë¼ì´íŠ¸ (RAG í†µí•© ë²„ì „)
# - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: st.session_state.financial_terms ì‚¬ì „ì—ì„œë§Œ ê²€ìƒ‰
#   2. ì‹ ê·œ: RAGì— ì €ì¥ëœ ëª¨ë“  ìš©ì–´ë¥¼ í•˜ì´ë¼ì´íŠ¸ ëŒ€ìƒìœ¼ë¡œ ì‚¬ìš©
#   3. Fallback: RAG ë¯¸ì´ˆê¸°í™” ì‹œ ê¸°ì¡´ ì‚¬ì „ ì‚¬ìš©
# - ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ë¥¼ ì°¾ì•„ <mark> íƒœê·¸ë¡œ ê°ì‹¸ ê°•ì¡°
# - ëŒ€ì†Œë¬¸ì ë¬´ì‹œ(re.IGNORECASE) â†’ ì˜ë¬¸ ì•½ì–´ ë“±ì—ë„ ëŒ€ì‘
# - data-term ì†ì„±: ì¶”í›„ JS/ì´ë²¤íŠ¸ ì—°ê²° ì‹œ ì–´ë–¤ ìš©ì–´ì¸ì§€ ì‹ë³„ ìš©ì´
# - Streamlit ì¶œë ¥ ì‹œ st.markdown(..., unsafe_allow_html=True) í•„ìš”
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def highlight_terms(text: str, article_id: Optional[str] = None, return_matched_terms: bool = False) -> Union[str, tuple[str, set[str]]]:
    """
    ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ê¸ˆìœµ ìš©ì–´ë¥¼ ì°¾ì•„ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ (ìºì‹± ì§€ì›)

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸(ê¸°ì‚¬ ë³¸ë¬¸ ë“±)
        article_id: ê¸°ì‚¬ ID (ìºì‹± í‚¤ë¡œ ì‚¬ìš©, Noneì´ë©´ ìºì‹± ì•ˆ í•¨)
        return_matched_terms: Trueì¼ ê²½ìš° (í•˜ì´ë¼ì´íŠ¸ëœ í…ìŠ¤íŠ¸, ë°œê²¬ëœ ìš©ì–´ ì„¸íŠ¸) íŠœí”Œ ë°˜í™˜

    Returns:
        return_matched_terms=False: ê¸ˆìœµ ìš©ì–´ê°€ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬ëœ HTML ë¬¸ìì—´
        return_matched_terms=True: (í•˜ì´ë¼ì´íŠ¸ëœ HTML ë¬¸ìì—´, ë°œê²¬ëœ ìš©ì–´ ì„¸íŠ¸) íŠœí”Œ
    """
    # âœ… ì„±ëŠ¥ ê°œì„ : ê¸°ì‚¬ë³„ í•˜ì´ë¼ì´íŠ¸ ê²°ê³¼ ìºì‹±
    if article_id:
        cache_key = f"highlight_cache_{article_id}"
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
        cache_entry = st.session_state.get(cache_key)
        
        # ìºì‹œê°€ ìˆê³  í…ìŠ¤íŠ¸ê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ìºì‹œëœ ê²°ê³¼ ë°˜í™˜
        if cache_entry and cache_entry.get("text_hash") == text_hash:
            cached_highlighted = cache_entry.get("highlighted", text)
            if return_matched_terms:
                cached_matched_terms = cache_entry.get("matched_terms", set())
                return cached_highlighted, cached_matched_terms
            return cached_highlighted
    
    highlighted = text
    terms_to_highlight = set()

    cached_terms = st.session_state.get("rag_terms_for_highlight")
    if cached_terms:
        terms_to_highlight = set(cached_terms)
    elif st.session_state.get("rag_initialized", False):
        try:
            collection = st.session_state.get("rag_collection")
            if collection is None:
                raise ValueError("RAG ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
            all_data = collection.get()
            if all_data and all_data['metadatas']:
                _cache_rag_metadata(all_data['metadatas'])
                terms_to_highlight = set(st.session_state.get("rag_terms_for_highlight", []))
        except Exception as e:
            st.warning(f"âš ï¸ RAG ìš©ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")
            terms_to_highlight = set(st.session_state.get("financial_terms", DEFAULT_TERMS).keys())
    else:
        terms_to_highlight = set(st.session_state.get("financial_terms", DEFAULT_TERMS).keys())

    # âœ… ì„±ëŠ¥ ê°œì„ : ì •ë ¬ëœ ìš©ì–´ ëª©ë¡ì„ ì„¸ì…˜ì— ìºì‹± (ìš©ì–´ ëª©ë¡ì´ ë³€ê²½ë˜ì§€ ì•ŠëŠ” í•œ ì¬ì‚¬ìš©)
    sorted_terms_cache_key = "highlight_sorted_terms_cache"
    sorted_terms_hash_key = "highlight_sorted_terms_hash"
    
    current_terms_hash = hashlib.md5(str(sorted(terms_to_highlight)).encode('utf-8')).hexdigest()
    cached_sorted_terms = st.session_state.get(sorted_terms_cache_key)
    cached_terms_hash = st.session_state.get(sorted_terms_hash_key)
    
    if cached_sorted_terms and cached_terms_hash == current_terms_hash:
        sorted_terms = cached_sorted_terms
    else:
        # ê¸´ ìš©ì–´ë¶€í„° ì²˜ë¦¬í•˜ì—¬ ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€ (ì˜ˆ: "ë¶€ê°€ê°€ì¹˜ì„¸"ê°€ "ë¶€ê°€ê°€ì¹˜"ë³´ë‹¤ ë¨¼ì € ì²˜ë¦¬)
        sorted_terms = sorted(terms_to_highlight, key=len, reverse=True)
        st.session_state[sorted_terms_cache_key] = sorted_terms
        st.session_state[sorted_terms_hash_key] = current_terms_hash

    # âœ… ì„±ëŠ¥ ê°œì„ : ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼ ìºì‹±
    pattern_cache_key = "highlight_pattern_cache"
    if pattern_cache_key not in st.session_state:
        st.session_state[pattern_cache_key] = {}
    pattern_cache = st.session_state[pattern_cache_key]

    # ì´ë¯¸ í•˜ì´ë¼ì´íŠ¸ëœ ë¶€ë¶„ì„ ë³´í˜¸í•˜ê¸° ìœ„í•œ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë” ë§µ
    placeholders = {}
    placeholder_counter = 0

    # âœ… ì„±ëŠ¥ ê°œì„ : ë¹ ë¥¸ ì‚¬ì „ í•„í„°ë§ - í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ìš©ì–´ë§Œ ì²˜ë¦¬
    text_lower = highlighted.lower()
    terms_in_text = [term for term in sorted_terms if term and term.lower() in text_lower]
    
    # âœ… ì„±ëŠ¥ ê°œì„ : ë°œê²¬ëœ ìš©ì–´ ì¶”ì  (ìš©ì–´ í•„í„°ë§ ì¬ì‚¬ìš©ì„ ìœ„í•´)
    matched_terms_set = set()

    for term in terms_in_text:
        # í”Œë ˆì´ìŠ¤í™€ë”ê°€ ì•„ë‹Œ ì‹¤ì œ í…ìŠ¤íŠ¸ë§Œ ë§¤ì¹­í•˜ë„ë¡ íŒ¨í„´ ìƒì„±
        # __PLACEHOLDER_ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ì€ ì œì™¸
        escaped_term = re.escape(term)

        # âœ… ì„±ëŠ¥ ê°œì„ : ì •ê·œì‹ íŒ¨í„´ ìºì‹±
        if escaped_term not in pattern_cache:
            pattern_cache[escaped_term] = re.compile(escaped_term, re.IGNORECASE)
        pattern = pattern_cache[escaped_term]

        # ë§¤ì¹­ëœ ì›ë˜ í‘œê¸°ë¥¼ ìœ ì§€í•˜ë©´ì„œ í•˜ì´ë¼ì´íŠ¸
        # âœ… ê°œì„ : ê°™ì€ ìš©ì–´ëŠ” ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ í•˜ì´ë¼ì´íŠ¸ (ê°€ë…ì„± í–¥ìƒ)
        matches = []
        for match in pattern.finditer(highlighted):
            # ë§¤ì¹­ëœ ìœ„ì¹˜ê°€ í”Œë ˆì´ìŠ¤í™€ë” ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
            start_pos = match.start()
            # ë§¤ì¹­ ìœ„ì¹˜ ì´ì „ì— í”Œë ˆì´ìŠ¤í™€ë”ê°€ ìˆê³  ì•„ì§ ë‹«íˆì§€ ì•Šì•˜ëŠ”ì§€ ì²´í¬
            # âœ… ì„±ëŠ¥ ê°œì„ : ë” íš¨ìœ¨ì ì¸ í”Œë ˆì´ìŠ¤í™€ë” ì²´í¬
            if start_pos > 0 and '__PLACEHOLDER_' in highlighted[max(0, start_pos-30):start_pos]:
                continue
            matches.append(match)
            # âœ… ê°œì„ : ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ ì²˜ë¦¬í•˜ê³  ì¤‘ë‹¨
            break

        # ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ í•˜ì´ë¼ì´íŠ¸ ì²˜ë¦¬
        if matches:
            match = matches[0]
            matched_text = match.group(0)
            # âœ… ì„±ëŠ¥ ê°œì„ : ë§¤ì¹­ëœ ìš©ì–´ ì¶”ì 
            matched_terms_set.add(term)
            
            # HTML íƒœê·¸ ìƒì„± (Streamlitì€ í´ë¦­ ì´ë²¤íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì‹œê°ì  í‘œì‹œë§Œ)
            placeholder = f"__PLACEHOLDER_{placeholder_counter}__"
            mark_html = (
                f'<mark class="financial-term" '
                f'style="background-color: #FFEB3B; padding: 2px 4px; border-radius: 3px;">'
                f'{matched_text}</mark>'
            )
            placeholders[placeholder] = mark_html
            placeholder_counter += 1

            # í…ìŠ¤íŠ¸ ì¹˜í™˜
            highlighted = highlighted[:match.start()] + placeholder + highlighted[match.end():]

    # ëª¨ë“  í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‹¤ì œ HTMLë¡œ ë³µì›
    for placeholder, mark_html in placeholders.items():
        highlighted = highlighted.replace(placeholder, mark_html)

    # âœ… ì„±ëŠ¥ ê°œì„ : ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
    if article_id:
        st.session_state[cache_key] = {
            "text_hash": text_hash,
            "highlighted": highlighted,
            "matched_terms": matched_terms_set  # ë°œê²¬ëœ ìš©ì–´ë„ í•¨ê»˜ ìºì‹±
        }

    # âœ… ì„±ëŠ¥ ê°œì„ : ë°œê²¬ëœ ìš©ì–´ ë°˜í™˜ (ìš©ì–´ í•„í„°ë§ ì¬ì‚¬ìš©)
    if return_matched_terms:
        return highlighted, matched_terms_set
    
    return highlighted

def _build_structured_context_from_metadata(
    base_term: str,
    metadata: Dict[str, str],
    question_term: Optional[str] = None,
    synonym_matched: bool = False,
) -> Dict[str, str]:
    context: Dict[str, str] = {}

    def _add(key: str, value: Optional[str]):
        if value:
            value = str(value).strip()
            if value:
                context[key] = value

    _add("definition", metadata.get("definition"))
    _add("analogy", metadata.get("analogy"))
    _add("importance", metadata.get("importance"))
    _add("correction", metadata.get("correction"))
    _add("example", metadata.get("example"))
    _add("synonym", metadata.get("synonym"))

    if question_term and question_term.lower() != base_term.lower():
        label = "question_term_synonym" if synonym_matched else "question_term"
        _add(label, question_term)

    context["term"] = base_term
    context["source"] = "RAG"
    return context


def _build_structured_context_from_default(term: str, info: Dict[str, str]) -> Dict[str, str]:
    context: Dict[str, str] = {}

    mapping = {
        "definition": info.get("ì •ì˜"),
        "detail": info.get("ì„¤ëª…"),
        "analogy": info.get("ë¹„ìœ "),
    }

    for key, value in mapping.items():
        if value:
            value = str(value).strip()
            if value:
                context[key] = value

    context["term"] = term
    context["source"] = "DEFAULT_DICTIONARY"
    return context


def _generate_structured_term_response(
    base_term: str,
    context: Dict[str, str],
    question_term: Optional[str] = None,
    temperature: float = 0.25,
) -> str:
    question_text = question_term or base_term
    user_prompt = f"{question_text}ê°€ ë­ì•¼?"
    response = generate_structured_persona_reply(
        user_input=user_prompt,
        term=base_term,
        context=context,
        temperature=temperature,
    )
    if response and "(LLM ì—°ê²° ì˜¤ë¥˜" not in response:
        return response

    # LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê°„ë‹¨í•œ ì •ë³´ë¼ë„ ì œê³µ
    parts: List[str] = [f"ğŸ¤– **{base_term}** ì— ëŒ€í•´ ì„¤ëª…í•´ì¤„ê²Œ! ğŸ¯"]
    if context.get("definition"):
        parts.append(f"ğŸ“– ì •ì˜: {context['definition']}")
    if context.get("detail"):
        parts.append(f"ğŸ’¡ ì„¤ëª…: {context['detail']}")
    if context.get("importance"):
        parts.append(f"â— ì™œ ì¤‘ìš”í•´?: {context['importance']}")
    if context.get("analogy"):
        parts.append(f"ğŸŒŸ ë¹„ìœ : {context['analogy']}")
    if context.get("example"):
        parts.append(f"ğŸ“° ì˜ˆì‹œ: {context['example']}")
    parts.append("ë” ê¶ê¸ˆí•œ ì  ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!")
    return "\n".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“ CSV íŒŒì¼ì—ì„œ ê¸ˆìœµìš©ì–´ ë¡œë“œ
# - rag/glossary/ê¸ˆìœµìš©ì–´.csv íŒŒì¼ì„ pandasë¡œ ì½ì–´ì˜´
# - ì»¬ëŸ¼: ë²ˆí˜¸, ê¸ˆìœµìš©ì–´, ì •ì˜, ë¹„ìœ , ì™œ ì¤‘ìš”?, ì˜¤í•´ êµì •, ì˜ˆì‹œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_glossary_from_csv() -> pd.DataFrame:
    """ê¸ˆìœµìš©ì–´.csv íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ DataFrameìœ¼ë¡œ ë°˜í™˜"""
    csv_path = os.path.join(os.path.dirname(__file__), "glossary", "ê¸ˆìœµìš©ì–´.csv")

    if not os.path.exists(csv_path):
        st.warning(f"âš ï¸ ê¸ˆìœµìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
        return pd.DataFrame()

    try:
        df = pd.read_csv(csv_path, encoding="utf-8")
        # ê²°ì¸¡ì¹˜ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
        df = df.fillna("")
        return df
    except Exception as e:
        st.error(f"âŒ CSV ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” CSV íŒŒì¼ ì²´í¬ì„¬ ê³„ì‚° (ë³€ê²½ ê°ì§€ìš©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _calculate_csv_checksum(csv_path: str) -> str:
    """CSV íŒŒì¼ì˜ ì²´í¬ì„¬ì„ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
    try:
        with open(csv_path, 'rb') as f:
            file_hash = hashlib.md5(f.read()).hexdigest()
        return file_hash
    except Exception:
        return ""


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (st.cache_resourceë¡œ ìºì‹±)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource
def _get_embedding_model():
    """
    ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œ (st.cache_resourceë¡œ ìºì‹±)
    - í•œ ë²ˆ ë¡œë“œëœ ëª¨ë¸ì€ ì„¸ì…˜ ê°„ ì¬ì‚¬ìš©
    - ë¦¬ì†ŒìŠ¤(ë©”ëª¨ë¦¬, ëª¨ë¸ íŒŒì¼)ë¥¼ ê³µìœ í•˜ë¯€ë¡œ cache_resource ì‚¬ìš©
    """
    return SentenceTransformer('jhgan/ko-sroberta-multitask')


@st.cache_resource
def _get_chroma_client():
    """
    ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (st.cache_resourceë¡œ ìºì‹±)
    - í•œ ë²ˆ ìƒì„±ëœ í´ë¼ì´ì–¸íŠ¸ëŠ” ì„¸ì…˜ ê°„ ì¬ì‚¬ìš©
    - persistent ëª¨ë“œë¡œ ë””ìŠ¤í¬ì— ì €ì¥
    """
    chroma_db_path = os.path.join(_get_cache_dir(), "chroma_db")
    return chromadb.PersistentClient(
        path=chroma_db_path,
        settings=Settings(
            anonymized_telemetry=False
        )
    )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¾ ì„ë² ë”© ë²¡í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _get_cache_dir():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë°˜í™˜"""
    cache_dir = os.path.join(os.path.dirname(__file__), "glossary", ".cache")
    os.makedirs(cache_dir, exist_ok=True)
    return cache_dir


def _get_embeddings_cache_path():
    """ì„ë² ë”© ë²¡í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ"""

    return os.path.join(_get_cache_dir(), "embeddings.pkl")


def _get_metadata_cache_path():
    """ë©”íƒ€ë°ì´í„° ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
    return os.path.join(_get_cache_dir(), "metadata.pkl")


def _get_checksum_cache_path():
    """ì²´í¬ì„¬ ìºì‹œ íŒŒì¼ ê²½ë¡œ"""
    return os.path.join(_get_cache_dir(), "checksum.json")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ’¾ ì„ë² ë”© ë²¡í„° ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_embeddings_cache(documents: List[str], embeddings, metadatas: List[Dict], ids: List[str], checksum: str):
    """ì„ë² ë”© ë²¡í„°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥ (ë¡œì»¬ì€ ì••ì¶• ì—†ìŒ, ë¹ ë¥¸ ë¡œë“œ)"""
    try:
        cache_dir = _get_cache_dir()
        
        # ì„ë² ë”© ë²¡í„° ì €ì¥ (ì••ì¶• ì—†ìŒ - ë¹ ë¥¸ ë¡œë“œ)
        cache_data = {
            'documents': documents,
            'embeddings': embeddings,
            'metadatas': metadatas,
            'ids': ids
        }
        
        with open(_get_embeddings_cache_path(), 'wb') as f:
            pickle.dump({
                'documents': documents,
                'embeddings': embeddings,
                'metadatas': metadatas,
                'ids': ids
            }, f)

        
        # ì²´í¬ì„¬ ì €ì¥
        with open(_get_checksum_cache_path(), 'w', encoding='utf-8') as f:
            json.dump({'checksum': checksum}, f)
        
    except Exception as e:
        st.warning(f"âš ï¸ ì„ë² ë”© ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ“‚ ì„ë² ë”© ë²¡í„° ë¡œë“œ (ë¡œì»¬ ìºì‹œ)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_cache(checksum: str) -> Optional[Dict]:
    """ì €ì¥ëœ ì„ë² ë”© ë²¡í„°ë¥¼ ë¡œì»¬ ìºì‹œ íŒŒì¼ì—ì„œ ë¡œë“œ"""
    try:
        # ì²´í¬ì„¬ í™•ì¸
        checksum_path = _get_checksum_cache_path()
        if not os.path.exists(checksum_path):
            return None
        
        with open(checksum_path, 'r', encoding='utf-8') as f:
            cached_data = json.load(f)
            if cached_data.get('checksum') != checksum:
                return None  # CSV íŒŒì¼ì´ ë³€ê²½ë¨
        
        # ì„ë² ë”© ë²¡í„° ë¡œë“œ

        embeddings_path = _get_embeddings_cache_path()
        if not os.path.exists(embeddings_path):
            return None
        
        with open(embeddings_path, 'rb') as f:
            return pickle.load(f)
    
    except Exception as e:
        st.warning(f"âš ï¸ ë¡œì»¬ ì„ë² ë”© ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜ï¸ Supabase Storageì— ì„ë² ë”© ì €ì¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_embeddings_to_supabase(documents: List[str], embeddings, metadatas: List[Dict], ids: List[str], checksum: str) -> bool:
    """Supabase Storageì— ì„ë² ë”© ë²¡í„° ì €ì¥"""
    if not SUPABASE_ENABLE:
        return False
    
    supabase = get_supabase_client()
    if not supabase:
        return False
    
    try:
        # 1. ì„ë² ë”© ë°ì´í„° ì¤€ë¹„
        cache_data = {
            'documents': documents,
            'embeddings': embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings,
            'metadatas': metadatas,
            'ids': ids
        }
        
        pickled_data = pickle.dumps(cache_data)
        compressed_data = gzip.compress(pickled_data)
        
        # 3. Storage ë²„í‚·ê³¼ ê²½ë¡œ ì„¤ì • (gzip í™•ì¥ì)
        bucket_name = "glossary-cache"
        storage_path = f"embeddings/{checksum}.pkl.gz"
        
        # 4. Storageì— ì—…ë¡œë“œ (ê¸°ì¡´ íŒŒì¼ì´ ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°)
        try:
            # ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì‹œë„ (ìˆìœ¼ë©´)
            supabase.storage.from_(bucket_name).remove([storage_path])
        except:
            pass  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ
        
        # ìƒˆ íŒŒì¼ ì—…ë¡œë“œ (gzip ì••ì¶•ëœ ë°ì´í„°)
        supabase.storage.from_(bucket_name).upload(
            storage_path,
            compressed_data,
            file_options={"content-type": "application/octet-stream", "upsert": "true"}
        )
        
        # 5. ë©”íƒ€ë°ì´í„°ë¥¼ í…Œì´ë¸”ì— ì €ì¥ (glossary_embeddings í…Œì´ë¸”)
        try:
            supabase.table("glossary_embeddings").upsert({
                "checksum": checksum,
                "storage_path": storage_path,
                "term_count": len(documents),
                "updated_at": "now()"
            }).execute()
        except Exception as table_error:
            # í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ê²½ê³ ë§Œ (StorageëŠ” ì„±ê³µí–ˆìœ¼ë¯€ë¡œ)
            st.warning(f"âš ï¸ glossary_embeddings í…Œì´ë¸” ì €ì¥ ì‹¤íŒ¨: {table_error}")
        
        return True
    
    except Exception as e:
        st.warning(f"âš ï¸ Supabase Storage ì €ì¥ ì‹¤íŒ¨: {e}")
        return False


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# â˜ï¸ Supabase Storageì—ì„œ ì„ë² ë”© ë¡œë“œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_from_supabase(checksum: str) -> Optional[Dict]:
    """Supabase Storageì—ì„œ ì„ë² ë”© ë²¡í„° ë¡œë“œ (1ìˆœìœ„)"""
    if not SUPABASE_ENABLE:
        return None
    
    supabase = get_supabase_client()
    if not supabase:
        return None
    
    try:
        bucket_name = "glossary-cache"
        storage_path = None
        
        # 1. ë©”íƒ€ë°ì´í„° í…Œì´ë¸”ì—ì„œ í™•ì¸ (ì„ íƒì , ì—†ì–´ë„ ì§„í–‰)
        try:
            result = supabase.table("glossary_embeddings").select("*").eq("checksum", checksum).execute()
            if result.data and len(result.data) > 0:
                # ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ê²½ë¡œ ì‚¬ìš©
                metadata = result.data[0]
                storage_path = metadata.get("storage_path")
        except:
            # í…Œì´ë¸”ì´ ì—†ì–´ë„ Storageì—ì„œ ì§ì ‘ í™•ì¸
            pass
        
        # 2. Storageì—ì„œ ë‹¤ìš´ë¡œë“œ ì‹œë„ (.pkl.gz ìš°ì„ , .pkl fallback)
        if not storage_path:
            # ë©”íƒ€ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ê²½ë¡œ ì‹œë„
            storage_paths = [
                f"embeddings/{checksum}.pkl.gz",  # ì••ì¶•ëœ íŒŒì¼ ìš°ì„ 
                f"embeddings/{checksum}.pkl"      # ì••ì¶• ì•ˆ ëœ íŒŒì¼ fallback
            ]
        else:
            storage_paths = [storage_path]
        
        response = None
        is_gzipped = False
        
        for path in storage_paths:
            try:
                response = supabase.storage.from_(bucket_name).download(path)
                if response:
                    is_gzipped = path.endswith('.gz')
                    break
            except:
                continue
        
        if not response:
            return None
        
        # 3. gzip ì••ì¶• í•´ì œ (í•„ìš”í•œ ê²½ìš°)
        if is_gzipped:
            decompressed_data = gzip.decompress(response)
            cache_data = pickle.loads(decompressed_data)
        else:
            cache_data = pickle.loads(response)
        
        return cache_data

    
    except Exception as e:
        # íŒŒì¼ì´ ì—†ê±°ë‚˜ ì—ëŸ¬ ë°œìƒ ì‹œ None ë°˜í™˜ (ì¡°ìš©íˆ ì‹¤íŒ¨)
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ë¡œë“œ: Supabase ìš°ì„ , ë¡œì»¬ Fallback
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _load_embeddings_with_fallback(checksum: str) -> Optional[Dict]:
    """
    ì„ë² ë”© ë²¡í„° ë¡œë“œ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹)
    
    âœ… ìµœì í™”: Supabase Storageë¥¼ ìš°ì„  í™•ì¸ (ì´ë¯¸ ì˜¬ë¼ê°€ ìˆìœ¼ë©´ ë¹ ë¥´ê²Œ ë¡œë“œ)
    
    ìš°ì„ ìˆœìœ„:
    1. Supabase Storage (ì›ê²© ì €ì¥ì†Œ, ì´ë¯¸ ì˜¬ë¼ê°€ ìˆìœ¼ë©´ ì¦‰ì‹œ ì‚¬ìš©)
    2. ë¡œì»¬ ìºì‹œ íŒŒì¼ (ë¹ ë¥¸ ë¡œì»¬ ì ‘ê·¼, Supabase ì‹¤íŒ¨ ì‹œ)
    3. None (ìƒˆë¡œ ìƒì„± í•„ìš”)
    """
    # âœ… 1ìˆœìœ„: Supabase Storage (ì´ë¯¸ ì˜¬ë¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©)
    cached_data = _load_embeddings_from_supabase(checksum)
    if cached_data:
        st.session_state["rag_cache_source"] = "supabase"
        # ë¡œì»¬ ìºì‹œì—ë„ ì €ì¥í•˜ì—¬ ë‹¤ìŒì—ëŠ” ë” ë¹ ë¥´ê²Œ ì ‘ê·¼
        try:
            _save_embeddings_cache(
                cached_data['documents'],
                cached_data['embeddings'],
                cached_data['metadatas'],
                cached_data['ids'],
                checksum
            )
            st.session_state["rag_cache_synced"] = True
        except:
            pass
        return cached_data
    
    # âœ… 2ìˆœìœ„: ë¡œì»¬ ìºì‹œ íŒŒì¼ (Supabase ì‹¤íŒ¨ ì‹œ)
    cached_data = _load_embeddings_cache(checksum)
    if cached_data:
        st.session_state["rag_cache_source"] = "local"
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ Supabaseì— ë™ê¸°í™” (ë‹¤ìŒì—ëŠ” Supabaseì—ì„œ ë¹ ë¥´ê²Œ ë¡œë“œ)
        _sync_supabase_async(
            cached_data['documents'],
            cached_data['embeddings'],
            cached_data['metadatas'],
            cached_data['ids'],
            checksum
        )
        return cached_data

    # âœ… 3ìˆœìœ„: ì—†ìŒ (ìƒˆë¡œ ìƒì„± í•„ìš”)
    st.session_state["rag_cache_source"] = "none"
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ§  RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ë²¡í„° DB êµ¬ì¶• (í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™” ë²„ì „)
# - ì„ë² ë”© ëª¨ë¸: ì „ì—­ ìºì‹œë¡œ ì¬ì‚¬ìš© (ì„¸ì…˜ë§ˆë‹¤ ì¬ë¡œë“œ ë°©ì§€)
# - ì„ë² ë”© ë²¡í„°: Supabase Storage ìš°ì„ , ë¡œì»¬ Fallback (í•˜ì´ë¸Œë¦¬ë“œ)
# - ChromaDB: persistent ëª¨ë“œë¡œ ë””ìŠ¤í¬ì— ì €ì¥ (ì„¸ì…˜ ê°„ ìœ ì§€)
# - CSV ì²´í¬ì„¬: íŒŒì¼ ë³€ê²½ ê°ì§€í•˜ì—¬ ìë™ ì¬ì„ë² ë”©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def initialize_rag_system(is_background: bool = False):
    """
    RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”: ë²¡í„° DB ìƒì„± ë° ê¸ˆìœµìš©ì–´ ì„ë² ë”© (í•˜ì´ë¸Œë¦¬ë“œ ìºì‹œ)
    
    Args:
        is_background: ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ ì¤‘ì´ë©´ True (st.spinner ì‚¬ìš© ì•ˆ í•¨)
    """

    # ì„¸ì…˜ì— ì´ë¯¸ ì´ˆê¸°í™”ë˜ì–´ ìˆìœ¼ë©´ ìŠ¤í‚µ
    if "rag_initialized" in st.session_state and st.session_state.rag_initialized:
        return

    # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì²´í¬
    is_background_thread = is_background or (threading.current_thread().name != "MainThread")
    
    # ìŠ¤í”¼ë„ˆ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € (ë°±ê·¸ë¼ìš´ë“œì—ì„œëŠ” no-op)
    class _noop_context:
        def __enter__(self):
            return self
        def __exit__(self, *args):
            pass
    
    spinner_context = _noop_context if is_background_thread else st.spinner

    perf_enabled = _perf_enabled()
    perf_steps: List[Dict] = []
    total_start = time.perf_counter() if perf_enabled else 0.0
    step_start = total_start
    perf_logged = False

    try:
        # 1ï¸âƒ£ CSV ë¡œë“œ ë° ì²´í¬ì„¬ ê³„ì‚°
        with spinner_context("ğŸ“„ ê¸ˆìœµìš©ì–´ íŒŒì¼ ë¡œë“œ ì¤‘..."):
            csv_path = os.path.join(os.path.dirname(__file__), "glossary", "ê¸ˆìœµìš©ì–´.csv")
            if not os.path.exists(csv_path):
                if not is_background_thread:
                    st.warning(f"âš ï¸ ê¸ˆìœµìš©ì–´ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
                st.session_state.rag_initialized = False
                return

            df = pd.read_csv(csv_path, encoding="utf-8")
            df = df.fillna("")
            csv_checksum = _calculate_csv_checksum(csv_path)
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "csv_load", step_start)

        # 2ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (st.cache_resourceë¡œ ìºì‹±)
        # ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë¡œë“œê°€ ë§¤ìš° ëŠë¦¬ë¯€ë¡œ í•­ìƒ ìŠ¤í”¼ë„ˆ í‘œì‹œ
        with spinner_context("ğŸ¤– í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ 10-20ì´ˆ ì†Œìš”)"):
            embedding_model = _get_embedding_model()
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "model_ready", step_start)

        # 3ï¸âƒ£ ChromaDB í´ë¼ì´ì–¸íŠ¸ ìƒì„± (persistent ëª¨ë“œ, st.cache_resourceë¡œ ìºì‹±)
        with spinner_context("ğŸ’¾ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
            chroma_client = _get_chroma_client()
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "chroma_client", step_start)

        # 4ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ì„ë² ë”© ë¡œë“œ ì‹œë„ (Supabase ìš°ì„ , ë¡œì»¬ Fallback)
        with spinner_context("ğŸ”„ ì„ë² ë”© ë²¡í„° ë¡œë“œ ì¤‘..."):
            cached_data = _load_embeddings_with_fallback(csv_checksum)
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "cache_lookup", step_start)

        # 5ï¸âƒ£ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±
        collection_name = "financial_terms"
        with spinner_context("ğŸ” ë²¡í„° ì»¬ë ‰ì…˜ í™•ì¸ ì¤‘..."):
            try:
                collection = chroma_client.get_collection(name=collection_name)
                if collection.count() > 0 and cached_data is not None:
                    documents = cached_data['documents']
                    metadatas = cached_data['metadatas']
                    ids = cached_data['ids']

                    st.session_state.rag_collection = collection
                    st.session_state.rag_embedding_model = embedding_model
                    st.session_state.rag_initialized = True
                    st.session_state.rag_term_count = len(documents)
                    _cache_rag_metadata(metadatas)
                    st.session_state["rag_explanation_cache"] = {}

                    if perf_enabled:
                        step_start = _perf_step(perf_enabled, perf_steps, "cache_ready", step_start)
                        perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
                        _record_perf("initialize", perf_steps)
                        perf_logged = True

                    if not is_background_thread:
                        cache_source = "Supabase" if SUPABASE_ENABLE else "ë¡œì»¬"
                        st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({cache_source} ìºì‹œ ì‚¬ìš©, {len(documents)}ê°œ ìš©ì–´)")
                    return
                elif cached_data is None:
                    try:
                        chroma_client.delete_collection(name=collection_name)
                    except:
                        pass
                    collection = chroma_client.create_collection(
                        name=collection_name,
                        metadata={"description": "ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë²¡í„° DB"}
                    )
            except:
                collection = chroma_client.create_collection(
                    name=collection_name,
                    metadata={"description": "ê¸ˆìœµ ìš©ì–´ ì‚¬ì „ ë²¡í„° DB"}
                )
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "collection_ready", step_start)

        if cached_data is not None:
            with spinner_context("ğŸ“¦ ìºì‹œëœ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                documents = cached_data['documents']
                embeddings = cached_data['embeddings']
                metadatas = cached_data['metadatas']
                ids = cached_data['ids']

                if collection.count() == 0:
                    collection.add(
                        documents=documents,
                        metadatas=metadatas,
                        embeddings=embeddings.tolist() if hasattr(embeddings, 'tolist') else embeddings,
                        ids=ids
                    )
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "cache_materialize", step_start)
            _cache_rag_metadata(metadatas)
        else:
            with spinner_context("ğŸ“ ê¸ˆìœµìš©ì–´ ë°ì´í„° ì¤€ë¹„ ì¤‘..."):
                documents = []
                metadatas = []
                ids = []

                for idx, row in df.iterrows():
                    term = str(row.get("ê¸ˆìœµìš©ì–´", "")).strip()
                    if not term:
                        continue

                    synonym = str(row.get("ìœ ì˜ì–´", "")).strip()
                    definition = str(row.get("ì •ì˜", "")).strip()
                    analogy = str(row.get("ë¹„ìœ ", "")).strip()

                    search_text = f"{term}"
                    if synonym:
                        search_text += f" ({synonym})"
                    search_text += f" - {definition}"
                    if analogy:
                        search_text += f" | ë¹„ìœ : {analogy}"

                    documents.append(search_text)

                    metadatas.append({
                        "term": term,
                        "synonym": synonym,
                        "definition": definition,
                        "analogy": analogy,
                        "importance": str(row.get("ì™œ ì¤‘ìš”?", "")).strip(),
                        "correction": str(row.get("ì˜¤í•´ êµì •", "")).strip(),
                        "example": str(row.get("ì˜ˆì‹œ", "")).strip(),
                        "difficulty": str(row.get("ë‹¨ì–´ ë‚œì´ë„", "")).strip(),
                    })

                    ids.append(f"term_{idx}")
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "documents_prepared", step_start)

            with spinner_context(f"ğŸ”„ {len(documents)}ê°œ ê¸ˆìœµìš©ì–´ ë²¡í„°í™” ì¤‘..."):
                embeddings = embedding_model.encode(documents, show_progress_bar=False)
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "embedding_encode", step_start)

            collection.add(
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings.tolist(),
                ids=ids
            )
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "collection_populate", step_start)

            with spinner_context("ğŸ’¾ ì„ë² ë”© ë²¡í„° ì €ì¥ ì¤‘..."):
                _save_embeddings_cache(documents, embeddings, metadatas, ids, csv_checksum)
                st.session_state["rag_cache_synced"] = False
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "cache_save", step_start)

            _sync_supabase_async(documents, embeddings, metadatas, ids, csv_checksum)

        st.session_state.rag_collection = collection
        st.session_state.rag_embedding_model = embedding_model
        st.session_state.rag_initialized = True
        st.session_state.rag_term_count = len(documents)
        _cache_rag_metadata(metadatas)
        st.session_state["rag_explanation_cache"] = {}

        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "session_update", step_start)
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
            _record_perf("initialize", perf_steps)
            perf_logged = True

        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œì—ì„œëŠ” UI ë©”ì‹œì§€ í‘œì‹œ ì•ˆ í•¨
        if not is_background_thread:
            if cached_data is not None:
                cache_source = "Supabase" if SUPABASE_ENABLE else "ë¡œì»¬"
                st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({cache_source} ìºì‹œ ì‚¬ìš©, {len(documents)}ê°œ ìš©ì–´)")
            else:
                save_source = "Supabase + ë¡œì»¬" if SUPABASE_ENABLE else "ë¡œì»¬"
                st.success(f"âœ… RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ! ({len(documents)}ê°œ ìš©ì–´ ë¡œë“œ, {save_source}ì— ì €ì¥ë¨)")

    except Exception as e:
        if not is_background_thread:
            st.error(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        st.session_state.rag_initialized = False
    finally:
        if perf_enabled and not perf_logged:
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
            _record_perf("initialize", perf_steps)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ” RAG ê¸°ë°˜ ìš©ì–´ ê²€ìƒ‰
# - ì‚¬ìš©ì ì§ˆë¬¸ì„ ë²¡í„°í™”í•˜ì—¬ ìœ ì‚¬í•œ ìš©ì–´ ê²€ìƒ‰
# - ìƒìœ„ kê°œì˜ ê´€ë ¨ ìš©ì–´ ë°˜í™˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def search_terms_by_rag(query: str, top_k: int = 3) -> List[Dict]:
    """RAGë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ê¸ˆìœµ ìš©ì–´ ê²€ìƒ‰"""

    if not st.session_state.get("rag_initialized", False):
        return []

    perf_enabled = _perf_enabled()
    perf_steps: List[Dict] = []
    total_start = time.perf_counter() if perf_enabled else 0.0
    step_start = total_start
    perf_logged = False

    try:
        collection = st.session_state.rag_collection
        embedding_model = st.session_state.rag_embedding_model

        # âœ… ì„±ëŠ¥ ê°œì„ : ì„ë² ë”© ê²°ê³¼ ìºì‹± (ë™ì¼ ì§ˆë¬¸ì— ëŒ€í•œ ì¬ì‚¬ìš©)
        query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()
        embedding_cache_key = f"rag_embedding_cache_{query_hash}"
        
        cached_embedding = st.session_state.get(embedding_cache_key)
        if cached_embedding is not None:
            # ìºì‹œ íˆíŠ¸: ì„ë² ë”© ì¸ì½”ë”© ìƒëµ (ê±°ì˜ 0ms)
            query_embedding = cached_embedding
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "encode_cached", step_start)
        else:
            # ìºì‹œ ë¯¸ìŠ¤: ì„ë² ë”© ì¸ì½”ë”© ìˆ˜í–‰
            query_embedding = embedding_model.encode([query])[0]
            # ìºì‹œì— ì €ì¥ (ë‹¤ìŒ í˜¸ì¶œ ì‹œ ì¦‰ì‹œ ì‚¬ìš©)
            st.session_state[embedding_cache_key] = query_embedding
            if perf_enabled:
                step_start = _perf_step(perf_enabled, perf_steps, "encode", step_start)

        results = collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=top_k
        )
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "query", step_start)

        matched_terms = []
        if results and results['metadatas']:
            for metadata in results['metadatas'][0]:
                matched_terms.append(metadata)
        if perf_enabled:
            step_start = _perf_step(perf_enabled, perf_steps, "format", step_start)
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2), "info": {"top_k": top_k, "returned": len(matched_terms)}})
            _record_perf("query", perf_steps)
            perf_logged = True

        return matched_terms

    except Exception as e:
        st.error(f"âŒ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        return []
    finally:
        if perf_enabled and not perf_logged:
            perf_steps.append({"step": "total", "ms": round((time.perf_counter() - total_start) * 1000, 2)})
            _record_perf("query", perf_steps)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ¦‰ ì±—ë´‡ ì‘ë‹µìš©: RAG ê¸°ë°˜ ìš©ì–´ ì„¤ëª… ìƒì„± (ê¸°ì¡´ í•¨ìˆ˜ ëŒ€ì²´)
# - ë³€ê²½ ì‚¬í•­:
#   1. ê¸°ì¡´: í•˜ë“œì½”ë”©ëœ DEFAULT_TERMS ì‚¬ì „ì—ì„œ ê²€ìƒ‰
#   2. ì‹ ê·œ: RAG ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ ìš©ì–´ ì°¾ê¸°
#   3. Fallback: RAG ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë™ì‘
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def explain_term(term: str, chat_history=None, return_rag_info: bool = False):
    """
    ìš©ì–´ ì„¤ëª… ìƒì„± (RAG ì •í™• ë§¤ì¹­ ìš°ì„ , ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ì‚¬ì „ ì‚¬ìš©)

    Args:
        term: ì„¤ëª…í•  ê¸ˆìœµ ìš©ì–´
        chat_history: ì±„íŒ… ì´ë ¥ (í–¥í›„ ì»¨í…ìŠ¤íŠ¸ ê°•í™”ìš©)
        return_rag_info: Trueì¼ ê²½ìš° (explanation, rag_info) íŠœí”Œ ë°˜í™˜

    Returns:
        return_rag_info=False: ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš©ì–´ ì„¤ëª… ë¬¸ìì—´
        return_rag_info=True: (ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš©ì–´ ì„¤ëª…, RAG ë©”íƒ€ë°ì´í„° ë˜ëŠ” None) íŠœí”Œ
    """

    rag_info: Optional[Dict] = None
    metadata: Optional[Dict] = None
    synonym_matched = False

    if st.session_state.get("rag_initialized", False):
        try:
            metadata_map = st.session_state.get("rag_metadata_by_term")
            if not metadata_map:
                collection = st.session_state.get("rag_collection")
                if collection is None:
                    raise ValueError("RAG ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
                all_data = collection.get()
                if all_data and all_data["metadatas"]:
                    _cache_rag_metadata(all_data["metadatas"])
                    metadata_map = st.session_state.get("rag_metadata_by_term", {})

            if metadata_map:
                metadata = metadata_map.get(term.lower())
                if metadata:
                    base_term = (metadata.get("term") or "").strip()
                    synonym_field = (metadata.get("synonym") or "").strip()
                    if synonym_field:
                        synonyms = [s.strip().lower() for s in re.split(r"[,\n]", synonym_field) if s.strip()]
                        synonym_matched = term.lower() in synonyms and term.lower() != base_term.lower()
                    else:
                        synonym_matched = False

                    definition = metadata.get("definition", "")
                    analogy = metadata.get("analogy", "")
                    importance = metadata.get("importance", "")
                    correction = metadata.get("correction", "")
                    example = metadata.get("example", "")

                    cache = st.session_state.setdefault("rag_explanation_cache", {})
                    cache_key = base_term.lower()
                    response = cache.get(cache_key)

                    if response is None:
                        structured_context = _build_structured_context_from_metadata(
                            base_term=base_term,
                            metadata=metadata,
                            question_term=term,
                            synonym_matched=synonym_matched,
                        )
                        response = _generate_structured_term_response(
                            base_term=base_term,
                            context=structured_context,
                            question_term=term,
                        )
                        cache[cache_key] = response

                    if return_rag_info:
                        rag_info = {
                            "search_method": "exact_match",
                            "matched_term": base_term,
                            "synonym_used": synonym_matched,
                            "source": "rag"
                        }

                    if return_rag_info:
                        return response, rag_info
                    return response
        except Exception as e:
            st.warning(f"âš ï¸ RAG ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜, ê¸°ë³¸ ì‚¬ì „ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {e}")

    terms = st.session_state.get("financial_terms", DEFAULT_TERMS)

    if term not in terms:
        message = f"'{term}'ì— ëŒ€í•œ ì •ë³´ê°€ ì•„ì§ ì—†ì–´. ë‹¤ë¥¸ ìš©ì–´ë¥¼ ì„ íƒí•´ì¤˜."
        if return_rag_info:
            return message, None
        return message

    info = terms[term]
    structured_context = _build_structured_context_from_default(term, info)
    response = _generate_structured_term_response(
        base_term=term,
        context=structured_context,
        question_term=term,
    )

    if return_rag_info:
        return response, None
    return response
