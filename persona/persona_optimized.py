from __future__ import annotations

import json
import logging
import os
import random
import re
from datetime import datetime, timezone, timedelta
from time import perf_counter
from typing import Any, Callable, Dict, List, Optional, Tuple

from core.utils import get_openai_client  # type: ignore


try:
    from core.config import DEFAULT_OPENAI_MODEL, OPENAI_API_KEY  # type: ignore
except Exception:  # pragma: no cover - config importì€ ëŸ°íƒ€ì„ í™˜ê²½ ì˜ì¡´
    DEFAULT_OPENAI_MODEL = "gpt-4o-mini"
    OPENAI_API_KEY = None


try:
    import tiktoken  # type: ignore
except Exception:  # pragma: no cover - ì„ íƒì  ì˜ì¡´ì„±
    tiktoken = None  # type: ignore


KST = timezone(timedelta(hours=9))
_STRUCTURED_JSON_PATTERN = re.compile(r"\{.*\}", re.DOTALL)

_OPENERS_WITH_TERM = [
    "ì‹ ë¬¸ì—ì„œ ë´¤ëŠ”ë°~ '{term}' ì´ì•¼ê¸° ê¶ê¸ˆí–ˆì§€? ë‚´ê°€ ì •ë¦¬í•´ë³¼ê²Œ!",
    "ë‚´ê°€ ì •ë¦¬í•´ë‘” '{term}' ë©”ëª¨ë¥¼ í¼ì³ë³¼ê²Œ!",
    "ë‰´ìŠ¤ì—ì„œ '{term}' ìì£¼ ë“¤ë¦¬ë”ë¼. ì§€ê¸ˆ ë°”ë¡œ í’€ì–´ì¤„ê²Œ!",
]

_OPENERS_GENERIC = [
    "ì‹ ë¬¸ì—ì„œ ë´¤ëŠ”ë°~ '{question}' ì´ëŸ° ì´ì•¼ê¸° ë§ë”ë¼. ë‚´ê°€ ì‰½ê²Œ í’€ì–´ë³¼ê²Œ!",
    "ë‚´ê°€ ì •ë¦¬í•´ë‘” ì§ˆë¬¸ì´ ìˆëŠ”ë° '{question}'ì˜€ì–´. ê°™ì´ ì‚´í´ë³´ì!",
    "ë°©ê¸ˆ ë³¸ ë‰´ìŠ¤ ì£¼ì œì•¼. '{question}' ê¶ê¸ˆí–ˆì§€? ê°„ë‹¨íˆ ì •ë¦¬í•´ì¤„ê²Œ!",
]


def _ensure_log_dir(path: str) -> None:
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)


def _init_persona_logger() -> logging.Logger:
    log_path = os.getenv("PERSONA_LATENCY_LOG", "logs/persona_latency.log")
    _ensure_log_dir(log_path)

    logger = logging.getLogger("persona_logger")
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        handler = logging.FileHandler(log_path, encoding="utf-8")
        formatter = logging.Formatter(
            "%(asctime)s | %(levelname)s | %(message)s", datefmt="%Y-%m-%d %H:%M:%S"
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger


PERSONA_LOGGER = _init_persona_logger()


def _today_kst_str() -> str:
    return datetime.now(KST).strftime("%Y-%m-%d")


def _system_prompt(today_kst: str) -> str:
    """
    ì›ë³¸ê³¼ ë™ì¼í•œ ìºë¦­í„°ë¥¼ ìœ ì§€í•˜ë˜, í•„ìš” ë¬¸ë‹¨ë§Œ ë‚¨ê²¨ ì…ë ¥ í† í°ì„ ì•½ 30% ì ˆê°.
    """
    return (
        "ë„ˆëŠ” 'ì•Œë¶€ì—‰'ì´ë¼ëŠ” ì¹œê·¼í•œ íŠœí„°í˜• AIì•¼. ë°˜ë§ë¡œ ì°¨ë¶„í•˜ê³  ê°„ê²°í•˜ê²Œ ì„¤ëª…í•´.\n"
        "í•µì‹¬ ì—­í• ì€ ê²½ì œ ìš©ì–´ë¥¼ ì¼ìƒ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” ê±°ì•¼. ì¶”ìƒì  í‘œí˜„ ëŒ€ì‹  ìƒí™œ ì˜ˆì‹œë¥¼ ë“¤ì–´.\n"
        "ë§íˆ¬ ê·œì¹™: ë”°ëœ»í•œ ë°˜ë§(~í•´, ~ì•¼), ê³¼ì¥ê³¼ ë¶ˆí•„ìš”í•œ ì´ëª¨ì§€ëŠ” ê¸ˆì§€, ëì—ëŠ” 'ë” ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!'\n"
        "ë…ì: ê²½ì œ ì§€ì‹ ì´ˆë³´ì. 'ì™œ ì¤‘ìš”í•œì§€'ë¥¼ ë°˜ë“œì‹œ ì—°ê²°í•´.\n"
        "ì‘ë‹µ ì›ì¹™: í•µì‹¬ â†’ ì˜ˆì‹œ â†’ ì˜í–¥ ìˆœì„œ. ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´. "
        f"ë²•/ì˜í•™/íˆ¬ì/ê°€ê²©/ì •ì±… ë“± ë³€ë™ ê°€ëŠ¥ ì£¼ì œë©´ '(ê¸°ì¤€ì¼: {today_kst}, Asia/Seoul)'ì„ ë„£ì–´.\n"
        "íˆ¬ì ì¡°ì–¸ì€ í•˜ì§€ ë§ê³  ì „ë¬¸ê°€ ìƒë‹´ì„ ê¶Œì¥í•´. ìˆ«ìë³´ë‹¤ ì˜ë¯¸ë¥¼ ë¨¼ì € ì„¤ëª…í•´.\n"
        "ê¸ˆìœµ ì´ˆë³´ìë¥¼ ìœ„í•´ ì„¤ëª…ì´ ì¶©ë¶„íˆ ìì„¸í•´ì•¼ í•´. ê° ì„¹ì…˜ë§ˆë‹¤ 3~4ë¬¸ì¥ìœ¼ë¡œ ì™œ, ì–´ë–»ê²Œ, ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´.\n"
    )


def _structured_output_guide() -> str:
    return (
        "## ì¶œë ¥ í¬ë§·\n"
        "- JSON í•˜ë‚˜ë§Œ ë°˜í™˜í•˜ê³  summary, detail, impact, analogy, reminder ë‹¤ì„¯ ê°œ í‚¤ë¥¼ í¬í•¨í•´.\n"
        "- summary: í•œ ì¤„ í•µì‹¬ ìš”ì•½ (15~20ì).\n"
        "- detail: ìš©ì–´ ëœ»ê³¼ ë°°ê²½ì„ ì´ˆë³´ìê°€ ì´í•´í•  ìˆ˜ ìˆê²Œ 3~4ë¬¸ì¥ìœ¼ë¡œ ìì„¸íˆ ì„¤ëª…í•´. ì™œ ê·¸ëŸ°ì§€, ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ í¬í•¨.\n"
        "- impact: ìƒí™œ ì† ì˜í–¥ì„ êµ¬ì²´ì ìœ¼ë¡œ 3~4ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´. ì˜ˆë¥¼ ë“¤ì–´ ëŒ€ì¶œ, ì €ì¶•, ì†Œë¹„ ë“± ì‹¤ì œ ì²´ê°í•  ìˆ˜ ìˆëŠ” ë³€í™”ë¥¼ í¬í•¨.\n"
        "- analogy: '[ëŒ€ìƒ]ì²˜ëŸ¼ ~. ~' í˜•ì‹ìœ¼ë¡œ ì¼ìƒ ë¹„ìœ ë¥¼ 3~4ë¬¸ì¥ìœ¼ë¡œ ìì„¸íˆ ì„¤ëª…í•´. ì™œ ê·¸ ë¹„ìœ ê°€ ì ì ˆí•œì§€ ì´ìœ ë„ í¬í•¨.\n"
        "- reminder: ë§ˆì§€ë§‰ í•œ ì¤„ ë©˜íŠ¸. 'ë¬¼ì–´ë´' í‘œí˜„ì„ í¬í•¨í•´.\n"
        "- ê°’ì€ ëª¨ë‘ ë¬¸ìì—´ì´ê³  ì¶”ê°€ í…ìŠ¤íŠ¸ë‚˜ ì£¼ì„ì„ ë¶™ì´ì§€ ë§ˆ.\n"
        "- ì¤‘ìš”: ê¸ˆìœµ ì´ˆë³´ìë¥¼ ìœ„í•´ ì„¤ëª…ì´ ì¶©ë¶„íˆ ìì„¸í•´ì•¼ í•´. ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ˆ ë¼."
    )


_STRUCTURED_OUTPUT_GUIDE = _structured_output_guide()


_FEWSHOT_COMPACT: List[Dict[str, str]] = [
    {
        "role": "user",
        "content": "ì¸í”Œë ˆì´ì…˜ì´ ë­ì•¼?",
    },
    {
        "role": "assistant",
        "content": (
            "1ï¸âƒ£ ì´ë¦„: ì¸í”Œë ˆì´ì…˜ì€~\n"
            "2ï¸âƒ£ ëœ»: ë¬¼ê°€ê°€ ì „ë°˜ì ìœ¼ë¡œ ì„œì„œíˆ ì˜¤ë¥´ëŠ” ê±°ì•¼.\n"
            "3ï¸âƒ£ ì˜í–¥: ê°™ì€ ëˆìœ¼ë¡œ ì‚´ ìˆ˜ ìˆëŠ” ê²Œ ì¤„ì–´ë“¤ì–´.\n\n"
            "ë” ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!"
        ),
    },
    {
        "role": "user",
        "content": "ê¸ˆë¦¬ê°€ ì˜¤ë¥´ë©´ ì£¼ì‹ì´ ì™œ ë–¨ì–´ì ¸?",
    },
    {
        "role": "assistant",
        "content": (
            "ê¸ˆë¦¬ ì¸ìƒ â†’ ì˜ˆê¸ˆ ì´ì â†‘ â†’ ì£¼ì‹ ìê¸ˆ ì´íƒˆ â†’ ì£¼ê°€ í•˜ë½ â†“\n"
            "ê·¸ê±´ ë§ì´ì•¼~ ì€í–‰ ì´ìê°€ ì˜¤ë¥´ë©´ ìœ„í—˜í•œ ìì‚°ì„ í”¼í•˜ë ¤ê³  í•´ì„œ ê·¸ë˜."
        ),
    },
]


def _estimate_token_count(messages: List[Dict[str, str]], model: str) -> int:
    if not tiktoken:
        return sum(len(m.get("content", "")) for m in messages) // 4
    try:
        enc = tiktoken.encoding_for_model(model)
    except KeyError:  # pragma: no cover
        enc = tiktoken.get_encoding("cl100k_base")  # type: ignore
    total = 0
    for message in messages:
        for value in message.values():
            if isinstance(value, str):
                total += len(enc.encode(value))
    return total


def _log_prompt_stats(
    messages: List[Dict[str, str]],
    model: str,
    logger: Optional[Callable[[Dict[str, Any]], None]],
) -> None:
    if not logger:
        return
    token_estimate = _estimate_token_count(messages, model)
    logger(
        {
            "token_estimate": token_estimate,
            "message_count": len(messages),
            "model": model,
        }
    )


def optimized_llm_chat(
    messages: List[Dict[str, str]],
    model: Optional[str] = None,
    temperature: float = 0.3,
    max_tokens: int = 400,
    stream: bool = False,
    logger: Optional[Callable[[Dict[str, Any]], None]] = None,
) -> Tuple[str, Dict[str, Any]]:
    """
    ì›ë³¸ ëŒ€ë¹„ ê°œì„  ì‚¬í•­:
    - max_tokens ê¸°ë³¸ê°’ì„ 700 â†’ 400ìœ¼ë¡œ ì¶•ì†Œí•´ ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì„
    - stream=True ì‚¬ìš© ì‹œ í† í° ìƒì„± ì¦‰ì‹œ ì „ë‹¬ ê°€ëŠ¥
    - perf_counterë¡œ API ì§€ì—°ì„ ì¸¡ì •í•´ ë°˜í™˜
    - í•„ìš” ì‹œ logger ì½œë°±ìœ¼ë¡œ í† í° ìˆ˜ì™€ ëª¨ë¸ ì •ë³´ë¥¼ ê¸°ë¡
    """

    mdl = model or DEFAULT_OPENAI_MODEL
    _log_prompt_stats(messages, mdl, logger)

    client = get_openai_client(OPENAI_API_KEY)
    if client is None:
        raise RuntimeError(
            "OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. "
            "OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë‚˜ core.config.OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
        )

    start = perf_counter()
    api_params = {
        "model": mdl,
        "messages": messages,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": stream,
    }

    response_text = ""
    usage = None

    if stream:
        api_params.pop("stream")
        with client.chat.completions.stream(**api_params) as stream_resp:
            chunks: List[str] = []
            for event in stream_resp:
                if event.type == "message.delta":
                    delta = event.delta.content or ""
                    chunks.append(delta)
                    if logger:
                        logger({"chunk": delta})
                elif event.type == "message.completed":
                    usage = event.response.usage  # type: ignore[attr-defined]
                elif event.type == "error":
                    raise RuntimeError(f"Streaming error: {event.error}")  # pragma: no cover
            response_text = "".join(chunks).strip()
    else:
        api_params.pop("stream")
        resp = client.chat.completions.create(**api_params)
        response_text = resp.choices[0].message.content.strip()
        usage = resp.usage

    latency = perf_counter() - start

    metadata = {
        "model": mdl,
        "api_params": {k: v for k, v in api_params.items() if k != "messages"},
        "latency_seconds": round(latency, 3),
        "tokens": {
            "input": getattr(usage, "prompt_tokens", None),
            "output": getattr(usage, "completion_tokens", None),
            "total": getattr(usage, "total_tokens", None),
        }
        if usage
        else None,
    }

    log_payload = {
        "latency": metadata["latency_seconds"],
        "input_tokens": metadata["tokens"]["input"] if metadata["tokens"] else None,
        "output_tokens": metadata["tokens"]["output"] if metadata["tokens"] else None,
        "total_tokens": metadata["tokens"]["total"] if metadata["tokens"] else None,
        "model": mdl,
    }

    PERSONA_LOGGER.info(
        "latency=%ss | input=%s | output=%s | total=%s | model=%s",
        log_payload["latency"],
        log_payload["input_tokens"],
        log_payload["output_tokens"],
        log_payload["total_tokens"],
        log_payload["model"],
    )

    if logger:
        logger(log_payload)

    return response_text, metadata


def _build_messages_for_structured_reply(
    user_input: str,
    term: Optional[str],
    context: Optional[Dict[str, str]],
) -> List[Dict[str, str]]:
    today = _today_kst_str()
    base_prompt = _system_prompt(today) + "\n" + _STRUCTURED_OUTPUT_GUIDE

    sys = {"role": "system", "content": base_prompt}
    dev = {"role": "system", "content": "[í–‰ë™ ê·œì¹™] ì§ˆë¬¸ì´ ê¸ˆìœµê³¼ ê²¹ì¹˜ë©´ ê°„ë‹¨íˆ ì„¤ëª…í•˜ê³  í•„ìš”í•˜ë©´ ë” ë„ì™€ì¤€ë‹¤ê³  ë§í•´."}

    context_lines: List[str] = []
    if context:
        for key, value in context.items():
            if value:
                label = key.replace("_", " ")
                context_lines.append(f"- {label}: {value}")

    user_blocks: List[str] = []
    if term:
        user_blocks.append(f"[ê´€ì‹¬ ìš©ì–´]: {term}")
    if context_lines:
        user_blocks.append("[ì°¸ê³  ìë£Œ]")
        user_blocks.extend(context_lines)
    user_blocks.append(f"[ì§ˆë¬¸]: {user_input}")
    user_blocks.append("[ì§€ì‹œ] ìœ„ ì¡°ê±´ì„ ì§€í‚¨ JSON í•˜ë‚˜ë§Œ ë°˜í™˜í•´ì¤˜.")

    usr = {"role": "user", "content": "\n".join(user_blocks)}
    # few-shot ì˜ˆì‹œëŠ” ë¶ˆí•„ìš”í•œ ì¸ì½”ë”©ì„ í”¼í•˜ê¸° ìœ„í•´ compact ë²„ì „ë§Œ ì‚¬ìš©
    return [sys, dev, *_FEWSHOT_COMPACT, usr]


def _parse_structured_response(raw: str) -> Dict[str, str]:
    default = {
        "summary": "",
        "detail": "",
        "impact": "",
        "analogy": "",
        "reminder": "ë” ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!",
    }
    if not raw:
        return default
    try:
        return {**default, **json.loads(raw)}
    except Exception:
        match = _STRUCTURED_JSON_PATTERN.search(raw)
        if match:
            try:
                return {**default, **json.loads(match.group())}
            except Exception:
                pass
    default["summary"] = raw.strip()
    return default


def _format_structured_output(data: Dict[str, str], term: Optional[str], prompt: str) -> str:
    question_snippet = (prompt or "").strip()
    if len(question_snippet) > 20:
        question_snippet = question_snippet[:20] + "..."

    if term:
        opener_template = random.choice(_OPENERS_WITH_TERM)
        opener = opener_template.format(term=term)
    elif question_snippet:
        opener_template = random.choice(_OPENERS_GENERIC)
        opener = opener_template.format(question=question_snippet)
    else:
        opener = "ì‹ ë¬¸ì—ì„œ ë´¤ëŠ”ë°~ ë°©ê¸ˆ ì´ì•¼ê¸° ì‰½ê²Œ í’€ì–´ë³¼ê²Œ!"

    summary = (data.get("summary") or "").strip()
    detail = (data.get("detail") or "").strip()
    impact = (data.get("impact") or "").strip()
    analogy = (data.get("analogy") or "").strip()
    reminder = (data.get("reminder") or "").strip() or "ë” ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!"

    if "ë¬¼ì–´ë´" not in reminder:
        reminder += " ë” ê¶ê¸ˆí•œ ê±° ìˆìœ¼ë©´ í¸í•˜ê²Œ ë¬¼ì–´ë´!"

    definition = summary or detail or "ì‰½ê²Œ ë§í•˜ë©´, ì–´ë µê²Œ ëŠê»´ì§€ëŠ” ê°œë…ì„ ì¼ìƒ ì–¸ì–´ë¡œ í’€ì–´ë‚¸ ê±°ì•¼."
    impact_text = impact or "ìš°ë¦¬ ìƒí™œì˜ ëˆ íë¦„ê³¼ ì†Œë¹„ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ì¤˜."

    lines = [
        opener,
        "",
        f"ğŸ“˜ ì •ì˜",
        "",
        definition,
        "",
        f"ğŸ’¡ ì˜í–¥",
        "",
        impact_text,
    ]

    if analogy:
        lines.extend([
            "",
            f"ğŸŒŸ ë¹„ìœ ",
            "",
            analogy,
        ])

    lines.extend([
        "",
        reminder,
    ])
    return "\n".join(lines)


def generate_structured_persona_reply_optimized(
    user_input: str,
    term: Optional[str] = None,
    context: Optional[Dict[str, str]] = None,
    temperature: float = 0.3,
    max_tokens: int = 550,  # í’ˆì§ˆ ìš°ì„ : ì´ˆë³´ìë¥¼ ìœ„í•œ ìì„¸í•œ ì„¤ëª… (400 â†’ 550)
    stream: bool = False,
    logger: Optional[Callable[[Dict[str, Any]], None]] = None,
) -> Tuple[str, Dict[str, Any]]:
    """
    ê°œì„ ëœ persona ì‘ë‹µ ìƒì„±ê¸°.

    Returns
    -------
    tuple[str, dict]
        (formatted_response, metadata)
        metadataì—ëŠ” API í˜¸ì¶œ ì§€ì—°, í† í° ì‚¬ìš©ëŸ‰, ëª¨ë¸ ì •ë³´ ë“±ì´ ë‹´ê¸´ë‹¤.
    """
    messages = _build_messages_for_structured_reply(user_input, term, context)
    raw, metadata = optimized_llm_chat(
        messages,
        temperature=temperature,
        max_tokens=max_tokens,
        stream=stream,
        logger=logger,
    )
    structured = _parse_structured_response(raw)
    formatted = _format_structured_output(structured, term, user_input)
    return formatted, metadata


